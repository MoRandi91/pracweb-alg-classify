{% extends "reports_base.html" %}
{% block author %}Александр Фонарев{% endblock %}
{% block topic %}Настройка суперпозиции алгебраических операторов с помощью корректирующих операций{% endblock %}

{% block report %}
    <h1>Введение</h1>
	
	Одна из наиболее распространенных задач машинного обучения &mdash; задача классификации.
	Она состоит в распределении объектов по классам в результате обучения по прецедентам.
	В данной работе мы рассмотрим алгебраический подход к задаче классификации.
	Идея этого подхода простая и красивая. Трудно построить сразу нужный алгоритм, 
	действующий из пространства начальных информаций в пространство финальных информаций.
	Поэтому мы делаем это в два шага: сначала переходим в удобное нам вспомогательное пространство оценок,
	а из него уже в итоговое пространство.
	Также в этом подходе возникает возможность рассматривать различные суперпозиции 
	алгоритмических операторов. Это расширяет семейство алгоритмов и может привести к повышению качества
	получаемого решения.
	В данном задании мы применяем этот подход на практике,
	рассматриваем несколько моделей операторов и корректирующих семейств,
	их суперпозиции, делаем выводы о качестве работы построенных алгоритмов.
	
	<h1>Задача классификации</h1>
	<h2>Общая постановка</h2>
	На неформальном языке задача классификации заключается в некотором "разумном" отнесении объектов к классам. 
	Будем рассматривать $q$ объектов и $l$ классов. Задача классификации -- задача обучения с учителем.
	Поэтому рассматривается также обучающая выборка: множество $m$ объектов, для которых известны принадлежности к классам.
	
	Опишем общую постановку задачи формально. Пусть:
	<UL>
	<LI> $I_{ob}$ &mdash; пространство описаний объектов. Дескриптивная функция D сопоставляет каждому объекту
	его признаковое описание. Например, $D(s) = \|\rho_t(s, s^k)\|_{mn}$, где s &mdash; рассмтриваемый объект, 
	$s^k$ &mdash; объект обучающей выборки $k = 1, \dots m$, $\rho_t$ &mdash; некоторая метрика, $t = 1, \dots n$. 
	Tогда $I_{ob} = \mathfrak{C_{mn}}(\mathbb{R}_{+})$.
	<LI> $I_{cl}$&mdash; пространство описаний классов. Например, описанием j-го класса может быть вектор 
	$(P_j(s^1), \dots P_j(s^m)$, где $P_j)$ &mdash; предикат, говорящий о принадлежности объектов данному классу.
	Тогда $I_{cl} = E_2^m$.
	<LI>$\mathfrak{I_i}$ &mdash; пространство начальных информаций. Это пространство матриц размерности $q$ на $l$:
	$\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{I})}$,
	где $\mathfrak{I}$ &mdash; пространство совместных описаний объектов и классов, т.е. $\mathfrak{I} = I_{ob} \times I_{cl}$.
	<LI>$\mathfrak{I_f}$ &mdash; пространство финальных информаций. Это пространство матриц той же размрности: $\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{\tilde{I}})}$,
	где $\mathfrak{\tilde{I}} = E_2$, т.е. каждый элемент матрицы говорит о принадлежности соответсвующего объекта соответсвующему классу; 
	<LI>$I^u$ &mdash; универсальные ограничения. Эти ограничения не проверяются констрактивно, а задаются как некоторый "мешок" отображений,
	например, непрерывные отображения, монотонные, или в общем случае &mdash; отображения некоторой категории.
	<LI>$I^l$ &mdash; локальные ограничения. Эти ограничения задаются конструктивно. Например, это требование принимать истинные значения на прецедентах.
	Такое требование называется корректностью алгоритма. Заметим, что в данном задании мы не будем вводить такое требование.
	<LI>$\mathfrak{m}$ &mdash; модель алгоритмов, или семейство алгоритмов, удовлетворяющее определенным локальным и глобальным ограничениям.
	$\mathfrak{m:*}$ &mdash; семейство всех возможных алгоримтов. 
	</UL>
	
	Задача заключается в построении алгоритма (отображения) $A$, действующего из пространства начальных информация в пространство финальных информация
	и удовлетворяющих глобальным и локальным ограничениям:
	<p>
	\[
	A \in \mathfrak{m}^*: \mathfrak{I_i} \rightarrow \mathfrak{I_f}, \quad	A \in \mathfrak{m}^*[I^u], A \in \mathfrak{m}^*[I^l]
	\]
	</p>
	
	<h2>Конкретизация постановки в условиях практикума</h2>
	Перейдем от общей остановки задачи к непостредственно той, которую будем рассматривать в рамках данного задания.
	
	<UL>
	<LI> Объектами являются точки на плоскости. Пространство описания объектов $I_{ob} = \mathbb{R}^2$.
	
	<LI> Универсальные ограничения $I^u$: однородность объектов (порядок объектов неважен для алгоритма) и независимость объектов
	(алгоритм осущеcтвляет классификацию каждого отдельного объекта независимо).
	Следующим универсальным ограничением является принадлежность объекта только к одному классу.
	Из этого огранчения следует, что классы не являются независимыми.
	Однако классы являются в нашем случае однородными: алгоритм не различает классы и относится одинаково ко всем.
	Кроме того, мы будем рассматривать алгоритмы, принадлежащие только определенному семейству.
	Далее в отчете мы подробно опишем оба рассматриваемых семейства.
	
	<LI> Из независимости объектов следует, что можно рассматривать задачу классификации отдельно для каждого объекта.
	Кроме того, договоримся рассматривать информацию о прецедентах и классах как параметры алгоритма.
	Тогда можно сказать, что пространство начальных информаций $\mathfrak{I_i} = \mathbb{R}^2$.
	
	<LI> Из условия о принадлежности только одному классу следует, что можно задать результат классификации объекта
	меткой класса: числом из множества $\{1, \dots l\}$. 
	Так как рассматривается алгоритм для классификации отдельного объекта, получаем пространство финальных информаций 
	$\mathfrak{I_f} = \{1, \dots l\}$.
	</UL>
	

	<h1>Оптимизационный подход &mdash; для алгоритмов</h1>
	В поставленной задаче мы не требуем условия корректности алгоритма,
	т.к. поиск корректного алгоритма часто является неразрешимой задачей, а также может приводить к переобучению.
	Поэтому необходим иной способ учета обучающей информации и выбора алгоритма из семейства.
	При оптимизационном подходе вводится некоторый функционал качества и выбирается алгоритм, на котором достигается минимум:
	\[
	Q(A, \tilde{S}) \longrightarrow \min,
	\]
	где $\tilde{S}$ &mdash; обучающая выборка.
	Конкретный вид фукнционала может быть различным. Например, это может быть число ошибок алгоритма на обучающей выборке.
	Тогда мы будем выбирать корректный алгоритм (в случае, если такой алгоритм существует для данной выборки при остальных ограничениях задачи).
	В рамках этого задания мы не будем пользоваться функционалом вида $ Q(A, \tilde{S}) $. 
	В дальнейшем мы рассмотрим алгоритм $ A $ в виде композиции алгоритмического оператора и решающего правила и 
	будем решать оптимизационную задачу для оператора, а не для алгоритма.
	
	<h1>Алгебраический подход &mdash; идея декомпозиции</h1>
	<p>
	В общем случае работать с пространством начальных и финальных информаций неудобно, 
	поэтому в рамках алгебраического подхода решения задач классификации переходят в некоторое пространство, 
	называемое пространство оценок. Его выбирают произвольно, так, чтобы было удобно. 
	Для выполнения этого перехода в другое пространство алгоритм рассматривают как композицию алгоритмического оператора 
	и решающего правила $A = C \circ B$, 
	где $ B: \mathfrak{I_i} \rightarrow \mathfrak{I_e} $ &mdash; алгоритмический оператор, 
	$ C: \mathfrak{I_e} \rightarrow \mathfrak{I_f} $ &mdash; решающее правило. 
	Здесь $\mathfrak{I_e}$ &mdash; пространство оценок. В данном задании выберем $\mathfrak{I_e} = \mathbb{R}^l$.
	</p>
	
	<img src="diagram0.png"  width="200">
	
	<p>
	В рамках данного задания $ B \in \mathfrak{m_0} $, 
	где $ \mathfrak{m_0} = \mathfrak{m_{0_1}} \cup \mathfrak{m_{0_2}} $ &mdash; некоторое семейство операторов &mdash; объединение двух семейств,
	конкретный вид которых будет рассмотрен далее. А решающее правило зафиксируем как $ C = argmax $.
	</p>
	
	<h1>Оптимизационный подход &mdash; для операторов</h1>
	Будем искать не оптимальный алгоритм, а оптимальный оператор, 
	зафиксировав при этом решающее правило, 
	т.е. введем функционал вида $ Q(B, \tilde{S}) $. Этот функционал можно выбирать произвольным образом.
	
	Заметим, однако, что при данном подходе мы сталкиваемся с проблемой правильных оценок для элементов обучающей выборки, 
	т.к. мы знаем только метки классов для них, а не оценки.
	
	В ходе выполнения данной работы, был выбран функционал качества
	\begin{equation}\label{quality functional for operator}
		Q(B, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel B(S_i) - c_i \parallel^2} \longrightarrow \underset{B}{\min},	
	\end{equation}
	где $ c_i $ &mdash; оценки для объектов: бинарный вектор размерности $l$ с единсвтвенной единицей в позиции,
	соответсвующей классу, которму принадлежит объект.
	
	
<h1>Алгебраический подход &mdash; идея суперпозиции</h1>
	<p>
	Представим теперь наш алгоритм не в виде $ A = C \circ B$, 
	а в виде $ A = C \circ F \circ B $, где $F: \mathfrak{I_e} \rightarrow \mathfrak{I_e}$ &mdash; корректирующая операция.
	</p>
	
	<img src="diagram1.png"  width="220">
	
	<p>
	Основная идея алгебраического подхода состоит в следующем. 
	В общем случае выбранная нами модель операторов может не иметь оптимума, 
	поэтому будем использовать алгебраическое расширение модели. 
	Будем строить не один оператор, а несколько, и использовать их суперпозицию $A = C \circ F(B_1, \dots, B_p)$.
	</p>
	
	<img src="diagram2.png"  width="310">
	
	<p>
	Корректирующие операции $ F \in \mathfrak{f} $, где $ \mathfrak{f} = \mathfrak{f_1} \cup \mathfrak{f_2} $ &mdash; семейство корректирующих операций, 
	состоящее из двух подсемейств, где $ \mathfrak{f_i} = \{G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e} | p \in \mathbb{N} \}, i = 1, 2 $.
	</p>
	
	На самом деле корректирующая операция есть некоторое другое отображение
	\begin{equation}
		F: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e}\}^p \rightarrow \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \}
	\end{equation}
	Но она индицируется операцией
	\begin{equation}
		G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e}
	\end{equation}
	если
	\begin{equation}
		F(B_1, \cdots, B_p)(S) = G(B_1(S), \cdots, B_p(S))
	\end{equation}
	
	<h1>Оптимизационный подход &mdash; для суперпозиций</h1>
	Выше мы ввели функционал качества для настройки одного оператора.
	\begin{equation}
		Q: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} \times (\mathfrak{I_i}, \mathfrak{I_e})^q \rightarrow \mathbb{R}
	\end{equation}
	Т.к. $ F(\cdot) \in \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} $, то мы можем в тот же функционал подставить вместо одного оператора суперпозицию.
	\begin{equation}
		Q(F(B_1, \cdots, B_p), \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel F(B_1(S_i), \cdots, B_p(S_i)) - c_i \parallel^2}
	\end{equation}
	Таким образом, получаем оптимизационную задачу:
	\begin{equation}\label{quality functional for superposition}
		Q(F(B_1, \cdots, B_p), \tilde{S}) \longrightarrow \underset{p, B_1, \cdots, B_p, F}{\min}
	\end{equation}
	
		
	<h1>Итерационный процесс построения суперпозиции</h1>
	Будем последовательно добавлять операторы в суперпозицию.
	
	<UL>
		<LI> $B_1 = \underset{B}{argmin} Q(B, \tilde{S})$
		<LI> $(F, B_1, B_2) = \underset{F, B_1, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		Решать эту задачу слишком сложно, поэтому будем использовать неэквивалентное упрощение. 
		В качестве первого оператора будем использовать оператор полученный на предыдущем шаге. Т.е.
		$(F, B_1, B_2) = \underset{F, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		<LI> $(F, B_1, B_2, B_3) = \underset{F, B_3}{argmin} Q(F(B_1, B_2, B_3), \tilde{S})$
		$\cdots$
	</UL>
	Это внешний цикл настройки суперпозиции, который останавливается, 
	когда $|Q_{new} - Q_{old}| < \varepsilon $, где $Q_{new}$ и $Q_{old}$ &mdash; значение функционала, 
	достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	При каждой итерации внешнего цикла надо решать задачу оптимизации 
	$(F, B_1,\cdots, B_p) = \underset{F, B_p}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$. 
	Для этого входим во внутренний цикл, который будет решать эту задачу методом покоординатного спуска(опять используем неэквивалентное упрощение).
	
	<OL>
		<LI>$F^0$ &mdash; задаем некоторое начальное приближение для корректирующей операции
		<LI>$B_p^i = \underset{B_p}{argmin} Q(F^{i-1}(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$
		<LI>$F^i = \underset{F}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p^i), \tilde{S})$
	</OL>
	
	 Проводим итерации цикла 1 - 2 до того, как $|Q'_{new} - Q'_{old}| < \delta $, где $Q'_{new}$ и $Q'_{old}$ &mdash; значение функционала, 
	 достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	 Для реализации изложенного итерационного подхода необходимо уметь выполнять следующие действия:
	 <UL>
	 	<LI> $B^* = \underset{B}{argmin}Q(B, \tilde{S})$;
	 	<LI> $F^*_p \mapsto F^0_{p+1}$;
	 	<LI> $B^* = \underset{B}{argmin}Q(F(B_1, \cdots, B_{p-1}, B), \tilde{S})$;
	 	<LI> $F^* = \underset{F}{argmin}Q(F(B_1, \cdots, B_p), \tilde{S})$
	 	<LI> критерий останова для внутреннего цикла $|Q'_{new} - Q'_{old}| < \delta $;
	 	<LI> критерий останова для цикла наращивания p $|Q_{new} - Q_{old}| < \varepsilon $;
	 </UL>	
		Для добавления сразу двух операторов:
	<UL>	
	 	<LI> $F^*_p \mapsto F^0_{p+2}$;
	 	<LI> $(B^*_{p+1}, B^*_{p+2}) = \underset{B', B''}{argmin}Q(F(B_1, \cdots, B_p, B', B''), \tilde{S})$;
	</UL>
		Для перенастройки ранее добавленного оператора:
	<UL>
	 	<LI> $B^*_i = \underset{B}{argmin}Q(F(B_1, \cdots, B_{i-1}, B, B_{i+1}, \cdots, B_p), \tilde{S})$.
	 </UL>
	
	<h1>Типы параметров по отношению к оптимизационному подходу</h1>
	Параметры можно разделить на две группы: оптимизируемые и неоптимизируемые.
	Неоптимизируемые параметры либо фиксируются, либо настраиваются по выборке (с помощью какой-либо эвристики).
	Оптимизируемые параметры могут быть либо найдены аналитиески, либо с помощью численной процедуры оптимизации.	
	
    
    
    
    
    
    












    
    
    
    
<h1>Модель операторов K ближайших соседей (kNN)</h1>

Метод $k$ ближайших соседей является
одним из самых простых алгоритмов классификации
и заключается в том,
что классифицируемый объект $S_i \in \tilde S$
относится к тому классу $K_i$,
к которому принадлежит большинство из его $k$ соседей &mdash;
$k$ ближайших к нему объектов.

Этот метод опирается на гипотезу компактности:
предположение о том,
что мера сходства объектов выбрана достаточно удачно,
и схожие объекты скорее лежат в одном классе, чем в разных.
В этом случае граница между классами имеет достаточно простую форму,
а классы образуют компактно локализованные области в пространстве объектов.

<h2>Использование модели</h2>

Пусть на пространстве объектов $\mathfrak{I}_i$
задана метрика $\rho(S_i,S_j), S_i, S_j \in \mathfrak{I}_i$,
определяющая сходство объектов.
Для классифицируемого объекта $S$ упорядочим
обучающую выборку по убыванию значения $\rho(S, S_i)$:
\[
    \rho(S,S_s^{(1)}) \leqslant \rho(S,S_s^{(2)}) \leqslant \dots \leqslant \rho(S,S_s^{(q)})
\],
где через $S_s^{(i)}$ обозначается $i$-й сосед объекта $S$ из выборки $\tilde S$,
$q$ &mdash; мощность обучающей выборки: $|\tilde S| = q$.
В наиболее общем виде алгоритм ближайших соседей есть
\[
    A(S) = \arg\max\limits_{j \in \{1,\dots,l\}} \Gamma_j(S), \;
    \Gamma_j = \sum_{i=1}^q [K_s^{(i)} = K_j] \omega (i,S),
\]
где $K_s^{(i)}$ &mdash; известный ответ на объекте $S_s^{(i)}$,
$l$ &mdash; число классов,
$\omega(i,S)$ &mdash; весовая функция,
равная в случае $k$ ближайших соседей $\omega(i,S) = [i \leqslant k]$.

Мы решаем задачу классификации,
используя алгебраический подход,
в рамках которого алгоритмический оператор $B \in \mathfrak{M}^0$ действует
из пространства начальных информаций $\mathfrak{I}_i$
в пространство оценок $\mathfrak{I}_e$.
В этом случае оператор kNN
возвращает оценки принадлежностей классифицируемого объекта классам
и при классификации на $l$ классов имеет вид
\[
    B(S) = (e_1, \dots, e_l), \; e_j = \frac{\Gamma_j(S)}{\sum_{m=0}^l e_m} =
    \frac{\sum_{i=1}^q [K_s^{(i)} = K_j][i \leqslant k]}{\sum_{m=0}^l e_m} =
    \frac{\sum_{i=1}^k [K_s^{(i)} = K_j]}{\sum_{m=0}^l e_m}.
\]
В рассматриваемой задаче пространство $\mathfrak{I}_i$ есть $\mathbb{R}^2$,
поэтому в качестве $\rho(S_i,S_j)$ выбрана
евклидова метрика $\rho(S_i,S_j) = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$,
где $S_i = (x_i,y_i), S_j = (x_j,y_j)$.


<h2>Параметры модели </h2>

<table border="1">
<caption></caption>
<tr>
<th>Название</th>
<th>Обозначение</th>
<th>Область значений</th>
<th>Тип</th>
</tr>
<tr>
<td>Обучающая выборка</td>
<td>$\widetilde{S} = (x,y)$ </td>
<td>$\mathbb{R}^2$</td>
<td>Настраиваемый (процедурно-получаемый)</td>
</tr>
<tr>
<td>Число соседей</td>
<td>$k$</td>
<td>$\mathbb{N}$  </td>
<td>Настраиваемый (численно-оптимизируемый)</td>
</tr>
<tr>
<td>Метрика</td>
<td>$\rho$ </td>
<td>$\mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}_+$</td>
<td>Фиксированный</td>
</tr>
<tr>
<td>Весовая функция </td>
<td>$\omega$  </td>
<td>$\mathbb{Z}_+ \times \mathbb{R}^2 \to \mathbb{R}_+$</td>
<td>Фиксированный</td>
</tr>
</table>


<h2>Настройка (выбор оператора из модели)</h2>

Настроить модель оператора &mdash;
значит присвоить значение каждому из его параметров.
В рассматриваемой модели настраиваемым параметром
является число соседей $k$.
Поиск оптимального значения параметра $k$ происходит
в результате минимизации функционала потерь
$Q(B,\widetilde S) = \Vert B(\widetilde S) - C^{-1}(r(\widetilde S)) \Vert^2$,
который имеет следующий вид:
\[
    Q(k,\widetilde S) = \sum_{i=1}^q {\Vert e_i - K_i \Vert^2_{\mathbb{R}^l}} =
    \sum_{i=1}^q \sum_{j=1}^l {(e_{ij} - K_{ij})}^2 \to \min_k,
\]
где
$e_{ij} = \frac{\sum_{t\neq i} [r(S_t) = K_j][t \leqslant k]}{\sum_{m=1}^l e_{im}}$ &mdash;
оценка принадлежности $i$-ого объекта $K_j$-ому классу,
$K_i$ &mdash; $l$-мерный вектор из 0 и 1: $K_{ij} = [r(S_i) = K_j]$,
то есть координата равна 1, тогда и только тогда,
когда $i$-ый объект принадлежит $j$-ому классу.
При минимизации данного функционала метод оптимизации
будет стремиться увеличить оценку за правильный класс
и уменьшить оценку за неверный класс,
чтобы минимизировать норму разности.

<h3>Настройка оператора</h3>

Так как минимизация производится по целочисленному параметру $k$,
принимающему значения из отрезка $1,\dots,q$,
где $q$ &mdash; длина выборки,
то используется метод полного перебора.
Начальная инициализация $k = 1$:
подсчитывается функционал потерь $Q(1,\widetilde S)$.
Далее $k$ увеличивается итерационно с шагом $1$ до тех пор,
пока $Q(i,\widetilde S) \leqslant Q(i-1, \widetilde S)$.
Если функционал $Q$ не уменьшается на протяжении 5 итераций,
параметр $k$, найденный ранее, считается оптимальным.
	
<h2>Исследование модели алгоритмических операторов</h2>

<h3>Пример задачи, для которой в модели есть корректный алгоритм</h3>

Если классы делятся на локализованные области, для каждой из которых объекты этой области существенно ближе друг для друга, чем объекты других областей,
 то с помощью метода $k$ ближайших соседей можно получить корректный алгоритм.

Пример:

<img border="0" src="knn_pic1.bmp" alt="kNN works">

<h3>Пример задачи, для которой в модели нет корректного оператора</h3>

Существует множество случаев, когда $kNN$ не работает, например, когда один из классов гораздо более разрежен, чем другой.

Пример:

<img border="0" src="knn_pic2.bmp" alt="kNN fails">




























<h1>Модель алгоритмов: Решающие деревья</h1>
<h2>Вербальное описание модели</h2>
<p>
Модель представляет собой одно одноуровневое решающее дерево, состоящее из корня, в котором находится условие на признаки объектов и двух листов, в которых находятся оценки принадлежности объектов к классам.
</p>
<h2>Формальное описание модели</h2>
<p>
Пусть $S \in \mathbb{R}^2$ &mdash; объект распознавания. В корне дерева располагается предикат вида $[x_i < d]$, где $x_i$ &mdash; $i$-ый признак объекта $S$, $d$ &mdash; некоторая константа. В листах дерева располагаются оценки принадлежности к классам. Так как решается задача классификации на $l$ классов, то в листах находятся $l$-мерные вектора. Будем обозначать вектора оценок, находящиеся в листах, как $v_{left}$, $v_{right}$ и будем считать, что их компоненты принадлежат отрезку $[0,1]$.  
</p>	
<h2>Использование модели</h2>	
<p>
Для объекта S и заданных параметров модели оценки вычисляются по следующей формуле:
\[
B(S) = v_{left}[x_i < d] + v_{right}[x_i \geq d], 
\]
где
<ul> $x_i$ &mdash; $i$-ый признак объекта S; описание объекта распознавания;
<li> $d$ &mdash; порог в предикате; параметр оператора;
<li> $v_{left}, v_{right}$ &mdash; оценки в листах; параметр оператора.
</ul>
Таким образом, если значение предиката для объекта $S$ в узле дерева истинно, то в качетсве значения оператора возвращается вектор в левом листе, если ложно, то значение в правом листе.
</p>	
<h2>Параметры модели</h2>
<table border="1">
<caption></caption>
<tr>
<th>Название</th>
<th>Обозначение</th>
<th>Область допустимых значений</th>
<th>Тип параметра по отношению к оптимизации</th>
</tr>
<tr>
<td>Номер признака в предикате</td>
<td>$i$</td>
<td>$\{1,2\}$</td>
<td>Оптимзируемый</td>
</tr>
<tr>
<td>Порог в предикате</td>
<td>$d$</td>
<td>$\mathbb{R}$</td>
<td>Оптимзируемый</td>
</tr>
<tr>
<td>Оценки в листах</td>
<td>$v_{left}$, $v_{right}$</td>
<td>$[0,1]^l$</td>
<td>Вычисляемый</td>
</tr>
</table>

<h2>Настройка модели</h2>
<h3>Вычисляемые параметры</h3>
<p>
Оценки в листах строятся следующим образом: значение $j$-ой компоненты вектора $v_{left}$ &mdash; это доля объектов $j$-ого класса, для которых значение предиката истинно, значение $j$-ой компоненты вектора $v_{right}$ &mdash; это доля объектов $j$-ого класса, для которых значение предиката ложно.
</p>
<h3>Оптимизируемые параметры</h3>
<p>
Для оптимизируемых параметров ставится следующая оптимизационная задача:
\[
(i^*, d^*) = \arg\min_{i \in \{1,2\}, d \in \mathbb{R}}{Q(B(i,d),S)}
\]

Так как $i \in \{1,2\}$, то для его оптимизации воспользуемся полным перебором - перебираем все возможные номера координат, которые могут быть в узле.

Параметр $d$ при фиксирванном параметре $i$ настраивается с помощью равномерного поиска. В качестве концов начального отрезка берутся минимальное и максимальное значения соответствующей координаты в выборке прецедентов. Такой метод оптимизации выбран, потому что кажется целесообразным покрыть плоскость равномерной сеткой и перебирать всевозможные значения $d$ &mdash; некоторая аналогия с полным перебором для непрерывного аргумента. Данный метод находит оптимум для унимодальных функций. В нашем случае, функционал не обязательно является унимодальным, соотвественно мы не всегда будет находить оптимиум. Однако данный метод применим для любого набора прецедентов и прост в реализации.
</p>

<h2>Исследование модели алгоритмических операторов</h2>
<h3>Пример задачи, для которой в модели есть корректный оператор</h3>
<p>
Рассмотрим следующую задачу: пусть даны шесть прецедентов с координатами $(-0.1,0.5),(0.1,0.4), (0.06,0.4), (-0.06, 0.5), (0.2,0.5), (-0.2, 0.4)$. К первому классу относятся все объекты, у которых первая координата меньше 0, а ко второму, у которых она больше 0. Очевидно, что существует корректный алгоритмический оператор - например, в узле стоит предикат $[x < 0]$, в левом листе находится вектор $(1,0)$, а в правом $(0,1)$. Программная система строит похожий алгоритмический оператор, только в узле располагается предикат $[x < -0.05]$.Значение функционала равно 0. Так как функционал является суммой квадратов, то значение функционала не превышает 0, то построенный оператор действительно оптимальный.
</p>
<h3>Пример задачи, для которой в модели нет корректного оператора</h3>
<p>
Рассмотрим следующую задачу: пусть даны шесть прецедентов $S_1 = (0,0),S_2 = (1,1), S_3 = (1, 0), S_4 = (0.1,0.1),S_5 = (1.1,1.1), S_6 = (1.1, 0.1)$. К первому классу относятся объекты $S_1, S_2, S_4, S_5$, ко второму соответственно $S_3,S_6$.  Легко показать, что данная выборка не может быть корректно классифицирема с помощью исследуемых алгоритмических операторов. Действително, если в предикате использова первая координата, то невозможно разделить объекты $S_5,S_6$, а если по второй координате, то невозможно разделить объекты в контроле $S_4,S_6$.
</p>























<h1>Специальные монотонные афинные корректирующие операции</h1>
	<h2>Вербальное описание идеи</h2>
	<p>
		Рассмотрим в качестве семейства корректирующих операций специальные монотонные афинные корректирующие операции. 
		В основе семейства лежит использование линейных комбинаций оценок. Результатом корректурующей операции также является элемент
		пространства оценок.
	</p>	
	<h2>Формальное описание </h2>
	<p>
		Семейство специальных афинных корректирующих операций задаётся следующим образом:
		\[
	\overrightarrow{\Gamma} =  a_1\overrightarrow{\Gamma^1} + \ldots + a_p\overrightarrow{\Gamma^p}	\]
	Для того, чтобы задать корректирующую операцию из параметрического семейства необходимо задать значения параметров <!-- $\Gamma^0$ и --> $a_1, a_2, \dots, a_p$
	<!-- было \overrightarrow{\Gamma^0} + и в конце =  = (\overrightarrow{\Gamma^0}\widehat{\Gamma})(\frac{1}{\overrightarrow{a}}) , \overrightarrow{a}\geq\overrightarrow{0}
-->
	</p>
	<h2>Перечисление параметров корректирующей операции</h2>
	<p>
		<table>
			<tr>
				<th>Параметр</th>
				<th>Обозначение</th>
				<th>Область значений</th>
				<th>Тип</th>
			</tr>
			<tr>
				<td>Вектора оценок</td>
				<td>$\Gamma^1, \Gamma^2, \dots, \Gamma^p$</td>
				<td>$p\RR^2$</td>
				<td>Фиксирован</td>
			</tr>
		<!--	<tr>
				<td>Вектор сдвига</td>
				<td>$\Gamma^0$</td>
				<td>$\RR^2$</td>
				<td>Оптимизируемый параметр (численно)</td>
			</tr> -->
			<tr>
				<td>Веса</td>
				<td>$a_1, a_2, \dots, a_p$</td>
				<td>$p\RR_+$</td>
				<td>Оптимизируемый параметр (численно)</td>
			</tr>
		</table>
	</p>
	<h2>Настройка корректирующей операции</h2>
	<p>
		Для отыскания оптимальных значений параметров  $a_1, a_2, \dots, a_p$ решается задача 
		$\sum_j|\sum_i a_i\Gamma_{ij} - y_j|^2 \to min_{a_i}$ при условии $a_i >= 0$, 
	</p>
	<p>
		Указанная задача является задачей квадратичного программирования.
	</p>
























<h1>Семейство корректирующих операций. Специальные афинные корректирующие операции.</h1>
<h2>Вербальное описание идеи</h2>
Корректирующие операции используют тот факт, что пространство оценок является линейным. Такая идея приводит к расширениию модели за счет использования линейной комбинации. 
<h2>Формальное описание семейства</h2>
Специальный аффинные корректируюшие операции отличаются наличием добавочного члена ${\Gamma}^0$. 
Они используют $p$ вещественных параметров. Формально используемую корректирующую операцию можно выразить следующей формулой:
$$\overrightarrow{\Gamma} = \overrightarrow{\Gamma}^0 + \alpha_1\overrightarrow{\Gamma}^1 + \ldots + \alpha_p\overrightarrow{\Gamma}^p = (\overrightarrow{\Gamma}^0 \widehat{\Gamma})
\left(\begin{array}{c}
    1 \\
    \overrightarrow{\alpha} \\
\end{array}\right).  $$

<h2>Использование корректирующих операций</h2>

<h3>Точное и полное описание действия корректирующих операций</h3>
Корректирующие операторы получают оценки от $p$ моделей и складывают их с разными весами, затем прибавляют добавочный член $\overrightarrow{\Gamma}^0$. 
Таким образом, специальные афинные корректирущие операции полностью вписываются в модель декомпозиции и суперпозиции алгебраического подхода, то есть являются частью реализации заданного учащемуся метода. 
Результат их работы полностью определён оценками алгоритмических моделей и своими параметрами. 
Аргумент не является элементом пространства исходной информации. Корректирующие операции работают только с оценками алгоритмических моделей и ничего не знают об объекте распознавания. 
Результат является элементом пространства оценок.

<h2>Перечисление параметров семейства корректирующих операций</h2>

В таблице продемонстрированы параметры семейства:

<table border="1">
<caption></caption>
<tr>
<th>Название параметра</th>
<th>Обозначение параметра</th>
<th>Область допустимых значений</th>
<th>Тип параметра по отношению к оптимизации</th>
</tr>
<tr>
<td>Арность оператора</td>
<td>$p$</td>
<td>$\mathbb{N}$</td>
<td>Настраивается</td>
</tr>
<tr>
<td>Добавочный член</td>
<td>$\overrightarrow{\Gamma}^0$</td>
<td>$\mathbb{R}^l$ </td>
<td>Оптимизируется</td>
</tr>
<tr>
<td>Веса оценок</td>
<td>$\overrightarrow{\alpha}$</td>
<td>$\mathbb{R}^p$</td>
<td>Оптимизируются</td>
</tr>
</table>

<h2>Настройка параметров</h2>
<h3>Значения фиксированных параметров</h3>
Фиксируются начальный вектор ${\Gamma}^0$ и число $p$ вещественных параметров. 
Арность корректирующих операторов определяется количеством обращений за алгоритмическими операторами, пока не сработает криторий останова.

<h3>Оптимизационная задача в явном виде</h3>
Необходимо минимизировать следующий функционал:
$$Q(\overrightarrow{\Gamma}^0, \overrightarrow{\alpha}, S) = \frac{1}{ql} \sum_{i = 1}^{q} \|  \overrightarrow{\Gamma}^0 + \alpha_1\overrightarrow{\Gamma}^1 + \ldots + \alpha_p\overrightarrow{\Gamma}^p - C^{-1}(r_i) \| ^2.$$
Для оптимизации остальных параметров применим метод покоординатной оптимизации. 
Если зафиксировать $\overrightarrow{\alpha}$ и продифференцировать по $\overrightarrow{\Gamma}^0$, а затем приравнять $0$, то получим уравнение для нахождения оптимального значения $\overrightarrow{\Gamma}^0$ при фиксированном $\overrightarrow{\alpha}$. 
Аналогично можно зафиксировать $\overrightarrow{\Gamma}^0$ и все $\overrightarrow{\alpha_i}$, кроме одного, и продифференцировать по нему. 
Таким образом, можно получить оптимизированные параметры.

<h3>Формулы для оптимизируемых аналитически параметров</h3>
$$\overrightarrow{\Gamma}^0 =
    \frac{1}{q}\sum_{i = 1}^{q} (C^{-1}(r_i) - (\alpha_1\overrightarrow{\Gamma}^1 + \ldots + \alpha_p\overrightarrow{\Gamma}^p))
$$

$$\alpha_i =
    \frac{1}{q}\sum_{i = 1}^{q} (C^{-1}(r_i) - (\alpha_1\overrightarrow{\Gamma}^1 + \ldots + \alpha_p\overrightarrow{\Gamma}^p -  \alpha_i\overrightarrow{\Gamma}^i) - \overrightarrow{\Gamma}^0) \overrightarrow{\Gamma}^i.
$$
<h3>Метод оптимизации для численно оптимизируемых параметров</h3>
Был реализован метод покоординатной оптимизации. Метод достаточно прост в реализации и для любой задачи применим, а найденный оператор даёт минимум функционала.

<img border="0" src="one.png" alt="picture one">























			
	
	<h1>Расширение модели 1 семейством 1 </h1>
		<h2>Задача, решаемая корректно как в рамках модели, так и в рамках расширенной модели</h2>
		<h2>Задача, не решаемая корректно в рамках модели, но решаемая в рамках расширенной модели</h2>
		<h2>Задача, не решаемая корректно ни в рамках модели, ни в рамках расширенной модели</h2>
		
	<h1>Расширение модели 2 семейством 2 </h1>
		<h2>Задача, решаемая корректно как в рамках модели, так и в рамках расширенной модели</h2>
		<h2>Задача, не решаемая корректно в рамках модели, но решаемая в рамках расширенной модели</h2>
		<h2>Задача, не решаемая корректно ни в рамках модели, ни в рамках расширенной модели</h2>
		
	<h1>Расширение моделей 1 и 2 семействами 1 и 2 </h1>
		<h2>Задача, решаемая корректно как в рамках моделей, так и в рамках расширенных моделей</h2>
		<h2>Задача, не решаемая корректно в рамках моделей, но решаемая в рамках расширенных моделей</h2>
		<h2>Задача, не решаемая корректно ни в рамках моделей, ни в рамках расширенных моделей</h2>
	
	<h1>Заключение</h1>
		В ходе выполнения данной практической работы был применён алгебраи-
		ческий подход к решению задачи классификации. Была построена суперпо-
		зиция алгоритмических операторов, корректирующей операции над ними и
		решающего правила. Поиск параметров алгоритма производился в ходе реше-
		ния задачи минимизации функцинала качества на обучающей выборке. При
		решении задача минимизации по нескольким параметрам была разделена на
		несколько подзадач минимизации более простых функционалов. Это суще-
		ственно упростило поиск оптимальных параметров суперпозиции. В резуль-
		тате экспериментов было подтверждено, что композиция алгоритмических
		операторов может давать лучшие результаты, чем каждый из операторов по
		отдельности.
		
	<h1>Список литературы</h1>
	<OL>
	<LI> Рудаков К. В. -- Алгебраическая теория универсальных и локальных ограничений для алгоритмов распознавания.
	<LI> Сайт про html: http://htmlbook.ru/
	</OL>
{% endblock report %}

<!-- vim: set ft=htmldjango si sw=2 : -->
