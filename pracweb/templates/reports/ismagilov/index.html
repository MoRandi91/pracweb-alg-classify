{% extends "reports_base.html" %}
{% block author %}Тимур Исмагилов{% endblock %}
{% block topic %}Настройка суперпозиции алгебраических операторов с помощью корректирующих операций{% endblock %}

{% block report %}
	<h1>Введение</h1>
	
	<p>Одна из наиболее распространенных задач машинного обучения &mdash; задача классификации.
	Она состоит в распределении объектов по классам в результате обучения по прецедентам.
	В данной работе мы рассмотрим алгебраический подход к задаче классификации.
	Идея этого подхода простая и красивая. Трудно построить сразу нужный алгоритм,
	действующий из пространства начальных информаций в пространство финальных информаций.
	Поэтому мы делаем это в два шага: сначала переходим в удобное нам вспомогательное пространство оценок,
	а из него уже в итоговое пространство.</p>

	<p>Также в этом подходе возникает возможность рассматривать различные суперпозиции
	алгоритмических операторов. Это расширяет семейство алгоритмов и может привести к повышению качества
	получаемого решения.</p>

	<p>В данном задании мы применяем этот подход на практике,
	рассматриваем несколько моделей операторов и корректирующих семейств,
	их суперпозиции, делаем выводы о качестве работы построенных алгоритмов.</p>
	
	<h1>Задача классификации</h1>

	<h2>Неформальная постановка (общая постановка)</h2>

	<p>На неформальном языке задача классификации заключается в некотором "разумном" отнесении объектов к классам.
	Будем рассматривать $q$ объектов и $l$ классов. Задача классификации &mdash; задача обучения с учителем.
	Поэтому рассматривается также обучающая выборка: множество $m$ объектов, для которых известны принадлежности к классам.</p>
	
	<h2>Формальная постановка (дано)</h2>

	<p>Опишем общую постановку задачи формально</p>

	<h3>Пространство описаний объектов, дескриптивная функция</h3>
	<p> $I_{ob}$ &mdash; пространство описаний объектов. Дескриптивная функция $D$ сопоставляет каждому объекту
	его признаковое описание. Например, $D(s) = \|\rho_t(s, s^k)\|_{mn}$, где s &mdash; рассмтриваемый объект,
	$s^k$ &mdash; объект обучающей выборки $k = 1, \dots m$, $\rho_t$ &mdash; некоторая метрика, $t = 1, \dots n$.
	Tогда $I_{ob} = \mathfrak{C_{mn}}(\mathbb{R}_{+})$.</p>

	<h3>Пространство описаний классов</h3>
	<p>$I_{cl}$&mdash; пространство описаний классов. Например, описанием j-го класса может быть вектор
	$(P_j(s^1), \dots P_j(s^m))$, где $P_j$ &mdash; предикат, говорящий о принадлежности объектов данному классу.
	Тогда $I_{cl} = E_2^m$.</p>

	<h3>Пространство начальных информаций</h3>
	<p>$\mathfrak{I_i}$ &mdash; пространство начальных информаций. Это пространство матриц размерности $q$ на $l$:
	$\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{I})}$,
	где $\mathfrak{I}$ &mdash; пространство совместных описаний объектов и классов, т.е. $\mathfrak{I} = I_{ob} \times I_{cl}$.</p>

	<h3>Пространство финальных информаций</h3>
	<p>$\mathfrak{I_f}$ &mdash; пространство финальных информаций. Это пространство матриц той же размрности: $\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{\tilde{I}})}$,
	где $\mathfrak{\tilde{I}} = E_2$, т.е. каждый элемент матрицы говорит о принадлежности соответсвующего объекта соответсвующему классу; </p>

	<h3>Универсальные ограничения</h3>
	<p>$I^u$ &mdash; универсальные ограничения. Эти ограничения не проверяются констрактивно, а задаются как некоторый "мешок" отображений,
	например, непрерывные отображения, монотонные, или в общем случае &mdash; отображения некоторой категории.</p>

	<h3>Локальные ограничения</h3>
	<p>$I^l$ &mdash; локальные ограничения. Эти ограничения задаются конструктивно. Например, это требование принимать истинные значения на прецедентах.
	Такое требование называется корректностью алгоритма. Заметим, что в данном задании мы не будем вводить такое требование.</p>

	<h3>Модель алгоритмов</h3>
	<p>$\mathfrak{m}$ &mdash; модель алгоритмов, или семейство алгоритмов, удовлетворяющее определенным локальным и глобальным ограничениям.
	$\mathfrak{m:*}$ &mdash; семейство всех возможных алгоримтов.</p>
	
	<h2>Требуется найти алгоритм...</h2>

	<p>Задача заключается в построении алгоритма (отображения) $A$, действующего из пространства начальных информация в пространство финальных информация
	и удовлетворяющих глобальным и локальным ограничениям:
	\[
	A \in \mathfrak{m}^*: \mathfrak{I_i} \rightarrow \mathfrak{I_f}, \quad	A \in \mathfrak{m}^*[I^u], A \in \mathfrak{m}^*[I^l]
	\]
	</p>

	<h2>Корректность алгоритма как способ использовать прецеденты</h2>
	
	<p>В поставленной задаче мы не требуем условия корректности алгоритма,
	т.к. поиск корректного алгоритма часто является неразрешимой задачей, а также может приводить к переобучению.</p>

	<h2>Конкретизация постановки в условиях практикума</h2>

	<p>Перейдем от общей остановки задачи к непостредственно той, которую будем рассматривать в рамках данного задания.</p>

	<h3>Конкретное пространство описаний объектов</h3>
	<p> Объектами являются точки на плоскости. Пространство описания объектов $I_{ob} = \mathbb{R}^2$.</p>
	
	<h3>Конкретные универсальные ограничения</h3>
	<p> Универсальные ограничения $I^u$: однородность объектов (порядок объектов неважен для алгоритма) и независимость объектов
	(алгоритм осущеcтвляет классификацию каждого отдельного объекта независимо).
	Следующим универсальным ограничением является принадлежность объекта только к одному классу.
	Из этого огранчения следует, что классы не являются независимыми.
	Однако классы являются в нашем случае однородными: алгоритм не различает классы и относится одинаково ко всем.
	Кроме того, мы будем рассматривать алгоритмы, принадлежащие только определенному семейству.
	Далее в отчете мы подробно опишем оба рассматриваемых семейства.</p>
	
	<h3>Конкретное пространство начальных информаций</h3>
	<p> Из независимости объектов следует, что можно рассматривать задачу классификации отдельно для каждого объекта.
	Кроме того, договоримся рассматривать информацию о прецедентах и классах как параметры алгоритма.
	Тогда можно сказать, что пространство начальных информаций $\mathfrak{I_i} = \mathbb{R}^2$.</p>
	
	<h3>Конкретное пространство финальных информаций</h3>
	<p> Из условия о принадлежности только одному классу следует, что можно задать результат классификации объекта
	меткой класса: числом из множества $\{1, \dots l\}$.
	Так как рассматривается алгоритм для классификации отдельного объекта, получаем пространство финальных информаций
	$\mathfrak{I_f} = \{1, \dots l\}$.</p>
	

	<h1>Оптимизационный подход &mdash; для алгоритмов</h1>

	<h2>Оптимизационная задача для алгоритма и набора прецедентов $ Q(A, \tilde{S}) $</h2>
	<p>В поставленной задаче мы не требуем условия корректности алгоритма,
	т.к. поиск корректного алгоритма часто является неразрешимой задачей, а также может приводить к переобучению.
	Поэтому необходим иной способ учета обучающей информации и выбора алгоритма из семейства.
	При оптимизационном подходе вводится некоторый функционал качества и выбирается алгоритм, на котором достигается минимум:
	\[
	Q(A, \tilde{S}) \longrightarrow \min,
	\]
	где $\tilde{S}$ &mdash; обучающая выборка.</p>

	<h2>Функционал качества для алгоритма и набора прецедентов $ Q(A, \tilde{S}) $</h2>
	<p>Конкретный вид фукнционала может быть различным. Например, это может быть число ошибок алгоритма на обучающей выборке.
	Тогда мы будем выбирать корректный алгоритм (в случае, если такой алгоритм существует для данной выборки при остальных ограничениях задачи).
	В рамках этого задания мы не будем пользоваться функционалом вида $ Q(A, \tilde{S}) $.
	В дальнейшем мы рассмотрим алгоритм $ A $ в виде композиции алгоритмического оператора и решающего правила и
	будем решать оптимизационную задачу для оператора, а не для алгоритма.</p>
	\\subsubsection{
	<h1>Алгебраический подход &mdash; идея декомпозиции</h1>
	
	<h2>Декомпозиция алгоритма на оператор и решающее правило</h2>

	<p>
	В общем случае работать с пространством начальных и финальных информаций неудобно,
	поэтому в рамках алгебраического подхода решения задач классификации переходят в некоторое пространство,
	называемое пространство оценок. Его выбирают произвольно, так, чтобы было удобно.
	Для выполнения этого перехода в другое пространство алгоритм рассматривают как композицию алгоритмического оператора
	и решающего правила $A = C \circ B$,
	где $ B: \mathfrak{I_i} \rightarrow \mathfrak{I_e} $ &mdash; алгоритмический оператор,
	$ C: \mathfrak{I_e} \rightarrow \mathfrak{I_f} $ &mdash; решающее правило.
	Здесь $\mathfrak{I_e}$ &mdash; пространство оценок. В данном задании выберем $\mathfrak{I_e} = \mathbb{R}^l$.
	</p>
	
	<h2>Картинка с треугольной диаграммой $А=СВ$</h2>
	<img src="diagram0.png"  width="200">
	
	<h2>Конкретизация в условиях практикума</h2>
	<p>
	В рамках данного задания $ B \in \mathfrak{m_0} $,
	где $ \mathfrak{m_0} = \mathfrak{m_{0_1}} \cup \mathfrak{m_{0_2}} $ &mdash; некоторое семейство операторов &mdash; объединение двух семейств,
	конкретный вид которых будет рассмотрен далее. А решающее правило зафиксируем как $ C = argmax $.
	</p>
	
	<h1>Оптимизационный подход &mdash; для операторов</h1>

	<h2>Идея функционала качества для оператора и набора прецедентов $Q(B,S)$</h2>
	<p>Будем искать не оптимальный алгоритм, а оптимальный оператор, зафиксировав при этом решающее правило,
	т.е. введем функционал вида $ Q(B, \tilde{S}) $. Этот функционал можно выбирать произвольным образом.</p>
	
	<h2>Оптимизационная постановка задача классификации через $Q(B,S)$</h2>

	<h2>Канонический способ задания $Q(B,S)$</h2>

	<h2>Проблема определения "правильных" оценок, псевдообращение решающего правила</h2>

	<p>Заметим, однако, что при данном подходе мы сталкиваемся с проблемой правильных оценок для элементов обучающей выборки,
	т.к. мы знаем только метки классов для них, а не оценки.</p>
	
	<h2>Единый для всей работы  функционал $Q(B,S)$ - конкретный в условиях практикума</h2>
	<p>В ходе выполнения данной работы, был выбран функционал качества
	\begin{equation}\label{quality functional for operator}
		Q(B, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel B(S_i) - c_i \parallel^2} \longrightarrow \underset{B}{\min},	
	\end{equation}
	где $ c_i $ &mdash; оценки для объектов: бинарный вектор размерности $l$ с единсвтвенной единицей в позиции,
	соответсвующей классу, которму принадлежит объект.</p>
	
	<h2>Конкретный способ выбора "правильных" оценок в условиях практикума</h2>

	<h2>Сравнение оптимальных и корректных операторов</h2>


	<h1>Алгебраический подход &mdash; идея суперпозиции</h1>

	<h2>Основная идея. Картинка с квадратной диаграммой $А=СFВ$</h2>

	<p>
	Представим теперь наш алгоритм не в виде $ A = C \circ B$,
	а в виде $ A = C \circ F \circ B $, где $F: \mathfrak{I_e} \rightarrow \mathfrak{I_e}$ &mdash; корректирующая операция.
	</p>
	
	<img src="diagram1.png"  width="220">
	
	<p>
	Основная идея алгебраического подхода состоит в следующем.
	В общем случае выбранная нами модель операторов может не иметь оптимума,
	поэтому будем использовать алгебраическое расширение модели.
	Будем строить не один оператор, а несколько, и использовать их суперпозицию $A = C \circ F(B_1, \dots, B_p)$.
	</p>
	
	<img src="diagram2.png"  width="310">
	
	<h2>Операции над операторами, которые снова дают отображения с сигнатурой операторов</h2>
	<p>
	Корректирующие операции $ F \in \mathfrak{f} $, где $ \mathfrak{f} = \mathfrak{f_1} \cup \mathfrak{f_2} $ &mdash; семейство корректирующих операций,
	состоящее из двух подсемейств, где $ \mathfrak{f_i} = \{G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e} | p \in \mathbb{N} \}, i = 1, 2 $.
	</p>
	
	<h2>Операции над оценками индуцируют операции над операторами</h2>

	<p>На самом деле корректирующая операция есть некоторое другое отображение
	\begin{equation}
		F: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e}\}^p \rightarrow \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \}
	\end{equation}
	Но она индицируется операцией
	\begin{equation}
		G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e}
	\end{equation}
	если
	\begin{equation}
		F(B_1, \cdots, B_p)(S) = G(B_1(S), \cdots, B_p(S))
	\end{equation}</p>
	
	<h1>Оптимизационный подход &mdash; для суперпозиций</h1>

	<h2>Функционал качества оператора можно применить к результату коррекции</h2>

	<p>Выше мы ввели функционал качества для настройки одного оператора.
	\begin{equation}
		Q: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} \times (\mathfrak{I_i}, \mathfrak{I_e})^q \rightarrow \mathbb{R}
	\end{equation}</p>

	<p>Так как $ F(\cdot) \in \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} $, то мы можем в тот же функционал подставить вместо одного оператора суперпозицию.
	\begin{equation}
		Q(F(B_1, \cdots, B_p), \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel F(B_1(S_i), \cdots, B_p(S_i)) - c_i \parallel^2}
	\end{equation}</p>

	<h2>Большая оптимизационная задача настройки суперпозиции</h2>
	<p>
	Таким образом, получаем оптимизационную задачу:
	\begin{equation}\label{quality functional for superposition}
		Q(F(B_1, \cdots, B_p), \tilde{S}) \longrightarrow \underset{p, B_1, \cdots, B_p, F}{\min}
	\end{equation}
	</p>
		
	<h1>Итерационный процесс построения суперпозиции</h1>

	<h2>Описание итерационной процедуры по добавлению операторов</h2>

	<p>Будем последовательно добавлять операторы в суперпозицию.</p>	
	<ul>
		<li> $B_1 = \underset{B}{argmin} Q(B, \tilde{S})$
		<li> $(F, B_1, B_2) = \underset{F, B_1, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		Решать эту задачу слишком сложно, поэтому будем использовать неэквивалентное упрощение.
		В качестве первого оператора будем использовать оператор полученный на предыдущем шаге. Т.е.
		$(F, B_1, B_2) = \underset{F, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		<li> $(F, B_1, B_2, B_3) = \underset{F, B_3}{argmin} Q(F(B_1, B_2, B_3), \tilde{S})$
		$\cdots$
	</ul>
	<p>Это внешний цикл настройки суперпозиции, который останавливается,
	когда $|Q_{new} - Q_{old}| < \varepsilon $, где $Q_{new}$ и $Q_{old}$ &mdash; значение функционала,
	достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.</p>
	
	<h2>Описание итерационной процедуры по настройке пары корректируещего оператора</h2>

	<p>При каждой итерации внешнего цикла надо решать задачу оптимизации
	$(F, B_1,\cdots, B_p) = \underset{F, B_p}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$.
	Для этого входим во внутренний цикл, который будет решать эту задачу методом покоординатного спуска(опять используем неэквивалентное упрощение).</p>	
	<ul>
		<li>$F^0$ &mdash; задаем некоторое начальное приближение для корректирующей операции
		<li>$B_p^i = \underset{B_p}{argmin} Q(F^{i-1}(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$
		<li>$F^i = \underset{F}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p^i), \tilde{S})$
	</ul>
	
	<p>Проводим итерации цикла 1 - 2 до того, как $|Q'_{new} - Q'_{old}| < \delta $, где $Q'_{new}$ и $Q'_{old}$ &mdash; значение функционала,
	достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.</p>
	
	<h2>Список "элементарных" действий для настройки (в задании 2 их 6 штук)</h2>

	<p>Для реализации изложенного итерационного подхода необходимо уметь выполнять следующие действия:</p>
	<ul>
	 	<li> $B^* = \underset{B}{argmin}Q(B, \tilde{S})$;
	 	<li> $F^*_p \mapsto F^0_{p+1}$;
	 	<li> $B^* = \underset{B}{argmin}Q(F(B_1, \cdots, B_{p-1}, B), \tilde{S})$;
	 	<li> $F^* = \underset{F}{argmin}Q(F(B_1, \cdots, B_p), \tilde{S})$
	 	<li> критерий останова для внутреннего цикла $|Q'_{new} - Q'_{old}| < \delta $;
	 	<li> критерий останова для цикла наращивания p $|Q_{new} - Q_{old}| < \varepsilon $;
	</ul>	

	<p>Для добавления сразу двух операторов:</p>
	<ul>	
	 	<li> $F^*_p \mapsto F^0_{p+2}$;
	 	<li> $(B^*_{p+1}, B^*_{p+2}) = \underset{B', B''}{argmin}Q(F(B_1, \cdots, B_p, B', B''), \tilde{S})$;
	</ul>

	<p>Для перенастройки ранее добавленного оператора:</p>
	<ul>
	 	<li> $B^*_i = \underset{B}{argmin}Q(F(B_1, \cdots, B_{i-1}, B, B_{i+1}, \cdots, B_p), \tilde{S})$.
	</ul>
	
	<h1>Типы параметров по отношению к оптимизационному подходу</h1>

	<p>Параметры можно разделить на две группы: оптимизируемые и неоптимизируемые.</p>

	<h2>Неоптимизируемые</h2>
	<p>Неоптимизируемые параметры либо фиксируются, либо настраиваются по выборке (с помощью какой-либо эвристики).</p>

	<h2>Оптимизируемые</h2>
	<p>Оптимизируемые параметры могут быть либо найдены аналитически, либо с помощью численной процедуры оптимизации.</p>





        <h1>Модель алгоритмов 1: Парзеновское окно с простейшим финитным ядром.</h1>
        <h2>Вербальное описание модели</h2>
        
        Метод Парзеновского окна представляет собой одну из возможных реализаций байесовского подхода к решению задачи классификации. Он естественным образом возникает при предположении о байесовости вероятностных распределений объектов. В методе производится классификация объекта по находящимся на некотором расстоянии от него объектах с весом, зависящим от расстояния.
        
        <h2>Формальное описание модели</h2>
        Дан набор из $m$ объектов. Объект $S^i$ приписан классу $y^i \in Y$, то есть класс j описывается $m_j$ объектами, которые называют эталонами класса.
        
        Близость объектов определяется с помощью некоторой заданной в пространстве начальной информации $\mathfrak{I}_i = \mathbb{R}^2$ метрики $\rho(S,S^i)$.

        Оценка принадлежности классу $j$:

        $$\Gamma_j(x)~=~\sum_{\substack{i: (S^i, r_i) \in \mathfrak{S}\\r_i = j}} K\left(\frac{\rho(x, S^i)}{h}\right),$$

        где $K:~\mathbb{R}_+~\longrightarrow~\mathbb{R}_+$ - сглаживающее ядро. В данном методе исследуется простейшее финитное ядро:
        \[
        K(x, S^i) =
        \begin{cases}
        1, & \text{если $\rho(x, S^i) \leq r$;} \\
        0, & \text{если $\rho(x, S^i) > r$.}
        \end{cases}
        \]

        <h2>Использование модели</h2>
        
        <h3>Точное описание действия алгоритмического оператора</h3>
        Формула оператора выглядит следующим образом:

        $$\Gamma_j(x)~=~\frac{\sum_{i = 1}^{q}{[r_i = j][\rho(x, S^i) \leq r]}}{\sum_{i = 1}^{q}{[\rho(x, S^i) \leq r]}},$$

        где $r$ - радиус ядра, параметр модели, который нужно будет настраивать.


        Метод парзеновского окна с финитным ядром можно использовать как модель алгоритмических операторов с пространством начальных информаций, таких, которые нужны метрике для сравнения данного ответа с эталонами, и пространством оценок,
        представленным в виде отрезка от $0$ до $1$. Аргумент является элементом пространства исходной информации. Результат является элементом пространства оценок. Легко заметить, что алгоритм полностью детерминирован.

        <h2>Параметры модели операторов</h2>


        <table border="1">
        <caption></caption>
        <tr>
        <th>Название </th>
        <th>Обозначение </th>
        <th>Область  значений</th>
        <th>Тип параметра</th>
        </tr>
        <tr>
        <td>Набор эталонов</td>
        <td>$\mathfrak{S}$</td>
        <td>$(S, r) \in (\mathfrak{J}_i, \mathfrak{J}_f)$</td>
        <td>Оптимизируемый</td>
        </tr>
        <tr>
        <td>Метрика</td>
        <td>$\rho(S,S^i)$</td>
        <td>$\mathbb{R}_{+}$ </td>
        <td>Фиксированный</td>
        </tr>
        <tr>
        <td>Ширина окна</td>
        <td>$r$</td>
        <td>$(0, +\infty)$</td>
        <td>Оптимизируемый</td>
        </tr>
        </table>

        <h2>Настройка параметров</h2>
        <h3>Значения фиксированных параметров</h3>
        Для начала нужно определиться с метрикой. Пусть будет Евклидова метрика.
        Используемое финитное ядро не зависит от выборки, оно фиксировано.

        <h3>Формулы для вычисляемых параметров</h3>
        В Евклидовой метрике в двухмерном варианте расстояние между двумя объектами $S$ и $S^i$ будет определяться следующим образом:
        $$\rho(S, S^i) = ((S_x - S^i_x)^2 + (S_y - S^i_y)^2)^{\frac{1}{2}}$$.

        <h3>Оптимизационная задача в явном виде</h3>
        Для того, чтобы вычислить наилучшее значение финитного ядра, нужно решить задачу оптимизации.
        Оптимизация проходит по двум параметрам: по набору эталонов $\widetilde{S}$ и по радиусу финитного ядра $r$ и состоит из двух шагов.
        Сначала оптимизируется выборка эталонов из контрольной выборки, а уже потом для каждого выбора эталонов ищется радиус ядра.
        Метод Монте-Карло, рассмотренный нами на практикуме на ЭВМ в прошлом семестре, прекрасно справится с двумя шагами.
        Причём на первом шаге, критерием останова будет являться тот факт, что за последние 2 итерации значение функционала не изменилось, а на втором шаге - за последние 5.
        Может возникнуть ситуация, когда в ядро не попадёт ни одной точки, тогда ответом классификатора следует принять тот класс, точек которого больше всего.

        <h3>Формулы для аналитически оптимизируемых параметров</h3>
        Минимизируется следующий функционал качества:
        $$ Q(B(r, \widetilde{S}), S) \rightarrow \min_{r \in (0, +\infty), \widetilde{S} \subset S}$$

        $$ Q(B(r, \widetilde{S}), S) = \frac{1}{ql} \sum_{(S^i,r_i) \in S} \| B(r, \widetilde{S})[S^i] - C^{-1}(r_i) \| ^2 = \frac{1}{ql} \sum_{(S^i,r_i) \in S} \| \frac{\sum_{(S^j,r_j) \in \widetilde{S}}{[r_i = r_j][\rho(S^i, S^j) \leq r]}}{\sum_{(S^j,r_j) \in \widetilde{S}}{[\rho(S^i, S^j) \leq r]}} - C^{-1}(r_i) \| ^2$$

        <h3>Метод оптимизации для численно оптимизируемых параметров</h3>
        Метод Монте-Карло был выбран для полученного типа задачи, основываясь на том, что оптимизация по радиусу дискретна на небольшом количестве точек, для любой задачи метод применим, и найденный оператор даёт минимум функционала.

        <h2>Исследование модели</h2>
        <h3>Пример задачи, для которой в модели есть корректный оператор</h3>
        
        Метод парзеновского окна основан на том, что объект можно классифицировать на основе его некоторой окрестности, он будет корректно работать, если для каждого элемента в ближайшей окрестности лежат элементы того же класса.
        
        <br>
       
        <embed src="parzen_correct.svg" type="image/svg+xml" width="500" height="410">


        <h3>Пример задачи, для которой в модели нет корректного оператора</h3>
        
        Если описанное предположение не выполняется, метод не будет работать.

        <br>
        
        <embed src="parzen_incorrect.svg" type="image/svg+xml" width="500" height="410">





        <h1>Модель алгоритмов 2: Наивный байесовский классификатор</h1>
        <h2>Вербальное описание модели</h2>
        
        Байесовские классификаторы &mdash; широкий класс алгоритмов классификации, основанный на принципе максимума апостериорной вероятности. Для классифицируемого объекта вычисляются функции правдоподобия каждого из классов, по ним вычисляются апостериорные вероятности классов. Объект относится к тому классу, для которого апостериорная вероятность максимальна.
        
        Наивный байесовский классификатор &mdash; самый простой частный случай байесовского классификатора, основанный на дополнительном предположении, что признаки объектов статистически независимы (классификатор называется наивным, поскольку очевидно, что в большинстве реальных задач это не так).
        

        <h2>Формальное описание модели</h2>

        С учетом независимости признаков совместное распределение вероятностей в пространстве описаний объектов и их классов задается следующим образом:
        
        \[p(S,r) = P(r)p(S(x)|r)p(S(y)|r)\],
        где S &mdash; признаковое описание объекта, r &mdash; метка класса.

        $P(r)$ представляет собой дискретное распределение, а распределения $p(S(x)|r), p(S(y)|r)$ выбираются из некоторого параметрического семейства распределений, в нашем случае они представляют собой одномерные нормальные распределения с параметрами мат. ожидание и дисперсия.

        Соответственно задача восстановления совместного распределения по обучающей выборке состоит в максимизации правдоподобия следующего вида:

        \[P(\tilde{S}) = \prod_i {p(S_i,r_i)} = \prod_i{P(r_i)p(S_i(x)|r_i)p(S_i(y)|r_i)} \rightarrow \max_{\{\mu,\sigma,P\}}\].

        Штрафы за неправильный ответ алгоритма вводятся следующим образом: $\lambda_{r, q}$ &mdash; величина штрафа за ответ алгоритма $q$, если "правильный" ответ $r$. Далее предполагается, что $\lambda_{r, r} = 0$, а $\lambda_{r, q} \equiv \lambda_r \forall q \neq r$. Значения $\lambda_r $ оптимизируются.

        В итоге, оценки принадлежности к классам для нового объекта $S$ выглядят следующим образом:
        	\begin{equation}
        	\begin{split}
        		&B(S) = ( \Gamma_1(S), \cdots, \Gamma_l(S))^T; \\
        		&\Gamma_j(S) = \ln {\lambda_j \cdot \tilde{P_j} } + \ln {\tilde{p}_{j_x}( S( x ) )} + \ln { \tilde{p}_{j_y} ( S(y) )},
        	\end{split}
        	\end{equation}
        	
        <h2>Использование модели</h2>
        Для данной модели пространство начальных информаций совпадает с пространством описания объектов, а пространством оценок является весь интервал $(-\infty,+\infty)$. Для объекта S и заданных параметров модели оценки вычисляются по следующим формулам:

        \begin{equation}
        	\begin{split}
        		&B(S) = ( \Gamma_1(S), \cdots, \Gamma_l(S))^T; \\
        		&\Gamma_j(S) = \ln {\lambda_j \cdot \tilde{P_j} } + \ln {\tilde{p}_{j_x}( S( x ) )} + \ln { \tilde{p}_{j_y} ( S(y) )},
        	\end{split}
        	\end{equation}
        	
        где
        <ul>
        <li>$\Gamma_j$ &mdash; $j$-ая компонента вектора ответа оператора $B$. Вспомогательная переменная.
        <li>$\lambda_j$ &mdash; величина штрафа при "неправильной классификации" объекта из класса $j$. Параметр оператора.
        <li>$\tilde{P}_j$ &mdash; оценка априорной вероятности Класса $j$. Параметр оператора.
        <li>$\tilde{p}_{j_x}$ &mdash; оценка функции плотности координаты $x$ объектов в классе $j$ $p(x | r=j)$. Параметр оператора.
        <li>$S( x )$ &mdash; координата $x$ объекта $S$. Вспомогательная переменная.
        <li>$\tilde{p}_{j_y}$ &mdash; оценка функции плотности координаты $y$ объектов в классе $j$ $p(y | r=j)$. Параметр оператора.
        <li>$S( y )$ &mdash; координата $y$ объекта $S$. Вспомогательная переменная.
        </ul>

        <h2>Параметры модели</h2>
        <table border="1">
        <caption></caption>
        <tr>
        <th>Название</th>
        <th>Обозначение</th>
        <th>Область  значений</th>
        <th>Тип параметра</th>
        </tr>
        <tr>
        <td>Величина штрафа</td>
        <td>$\lambda_j$</td>
        <td>$\mathbb{R}$</td>
        <td>Оптимизируемый</td>
        </tr>
        <tr>
        <td>Оценка априорной вероятности</td>
        <td>$\tilde{P}_j$</td>
        <td>$\mathfrak{I_f} \rightarrow [0, 1]$</td>
        <td>Вычисляемый</td>
        </tr>
        <tr>
        <td>Оценка функции плотности $x$</td>
        <td>$\tilde{p}_{j_x}$</td>
        <td>$ \mathbb{R} \rightarrow \mathbb{R}_{+} $</td>
        <td>Вычисляемый</td>
        </tr>
        <tr>
        <td>Оценка функции плотности $y$</td>
        <td>$\tilde{p}_{j_y}$</td>
        <td>$ \mathbb{R} \rightarrow \mathbb{R}_{+} $</td>
        <td>Вычисляемый</td>
        </tr>
        <tr>
        <td>Оценка мат.ожидания $x$</td>
        <td>$\mu_{j_x}$</td>
        <td>$\mathbb{R}$</td>
        <td>Вычисляемый</td>
        </tr>
        <tr>
        <td>Оценка мат.ожидания $y$</td>
        <td>$\mu_{j_y}$</td>
        <td>$\mathbb{R}$</td>
        <td>Вычисляемый</td>
        </tr>
        <tr>
        <td>Оценка дисперсии $x$</td>
        <td>$\sigma_{j_x}$</td>
        <td>$\mathbb{R}$</td>
        <td>Вычисляемый</td>
        </tr>
        <tr>
        <td>Оценка дисперсии $y$</td>
        <td>$\sigma_{j_y}$</td>
        <td>$\mathbb{R}$</td>
        <td>Вычисляемый</td>
        </tr>
        </table>

        <h2>Настройка модели</h2>
        <h3>Вычисляемые параметры</h3>
        Все вычисляемые параметры находятся путем максимизации правдоподобия обучающей выборки. Получаемые формулы:
        \begin{equation}
        	\begin{split}
        		&\tilde{p}_{j_x}(S(x)) = \frac{1}{\sqrt{2\pi\sigma_{j_x}^2}}\exp\left(  {-\frac{(S(x) - \mu_{j_x})^2}{2\sigma_{j_x}^2}}\right)  ; \\
        		&\tilde{p}_{j_y}(S(x)) = \frac{1}{\sqrt{2\pi\sigma_{j_y}^2}}\exp\left( {-\frac{(S(y) - \mu_{j_y})^2}{2\sigma_{j_y}^2}}\right) ; \\
        		&\tilde{P}_j = \frac{\sum_{i=1,\dots, q}[r_i=j]}{\sum_{i=1,\dots, q} 1} ;
        	\end{split}
        	\end{equation}
        где
        <ul>
        <li>$\mu_{j_x}$ &mdash; оценка мат.ожидания для координаты $x$(выборочное мат.ожидание) для Класса $ j $.
        <li>$\sigma_{j_x}$ &mdash; оценка дисперсии для координаты $x$(выборочная дисперсия) для Класса $ j $.
        <li>$\mu_{j_y}$ &mdash; оценка мат.ожидания для координаты $y$(выборочное мат.ожидание) для Класса $ j $.
        <li>$\sigma_{j_y}$ &mdash; оценка дисперсии для координаты $y$(выборочная дисперсия) для Класса $ j $.
        </ul>

        <h3>Оптимизируемые параметры</h3>
        Как было сказано выше, единственным оптимизируемым параметром является величина штрафа каждого класса $\lambda_j$. Поиск оптимального значения параметра $\lambda_j$ происходит в результате минимизации функционала $ Q $. Выпишем явный вид $ Q $ как функции от $(\lambda_1, \cdots, \lambda_l)$.
        	\begin{equation}
        	\begin{split}
        		Q(\lambda_1, \cdots, \lambda_l, \tilde{S}) &= \frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\Gamma_j(S_i) - c_i(j))^2} } = \\
        		&= \frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\ln {\lambda_j \cdot \tilde{P_j} } + \ln {\tilde{p}_{j_x}( S_i( x ) )} + \ln { \tilde{p}_{j_y} ( S_i(y) ))} - c_i(j))^2} } = \\
        		&= \frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\ln {\lambda_j} - M_i)^2} } \longrightarrow \underset{\lambda_1, \cdots, \lambda_l}{\min},
        	\end{split}
        	\end{equation}
        	
        	где $ M_i $ &mdash; некоторые константы, не зависящие от $ \lambda_j $. Заметим, что каждую $ \lambda_j $ можно рассматривать отдельно, таким образом получаем $ l $ задач оптимизации вида:
        	\begin{equation}
        		Q(\lambda_j, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {(\ln {\lambda_j} - M_i)^2} \longrightarrow \underset{\lambda_j}{\min}
        	\end{equation}
        	Можно сделать еще одно переобозначение: $ \Lambda_j = \ln {\lambda_j} $, и получить следующую оптимизационную задачу:
        	\begin{equation}
        		Q(\Lambda_j, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {(\Lambda_j - M_i)^2} \longrightarrow \underset{\Lambda_j}{\min}
        	\end{equation}
        	Полученная оптимизационная задача относится к классу квадратичных задач, ее можно решать методом наименьших квадратов. Тогда решение выписывается следующим образом:
        	\begin{equation}
        	\begin{split}
        		&\lambda_j = \exp{\frac{\sum_{i = 1}^q {M_i}}{q}}, \\
        		&M_i = c_i(j) - \ln{\tilde{P}_j} - \ln{\tilde{p}_{j_x}(S_i(x))} - \ln{\tilde{p}_{j_y}(S_i(y))}
        	\end{split}
        	\end{equation}
        	
        
        <h2>Исследование модели</h2>
        <h3>Пример задачи, для которой в модели есть корректный оператор</h3>

        В первом случае данные можно описать с помощью простейшей вероятностной модели: 

        <br>

        <embed src="bayes_correct.svg" type="image/svg+xml" width="500" height="410">


        <h3>Пример задачи, для которой в модели нет корректного оператора</h3>

        Во втором случае простейшая вероятностная модель уже не подходит: 

        <br>

        <embed src="bayes_incorrect.svg" type="image/svg+xml" width="500" height="410">




        <h1>Семейство корректирующих операций 1: Комитет большинства</h1>
	<h2>Вербальное описание корректирующей операции</h2>
	<p>


	Комитет большинства, или простое голосование &mdash; частный случай алгоритма кластеризации. К каждому объекту применяется несколько классификаторов, каждый из которых дает некоторую оценку принадлежности объекта каждому классу. Затем выбирается класс с наибольшей оценкой.

	</p>	
	<h2>Формальное описание корректирующей операции</h2>
	<p>
	Пусть имеется объект $\tilde{S} \in \mathfrak{S}$ и набор операторов $(B_1, \dots, B_p) \in \left(\mathfrak{M}[\pi]\right)^p$. Тогда скорректированная оценка объекта $\tilde{S}$ вычисляется таким образом: $e^{\tilde{S}} = F(B_1(\tilde{S}), \dots, B_p(\tilde{S})) = (e^{\tilde{S}}_1, \dots, e^{\tilde{S}}_l) \in \mathfrak{I}_e$. Причем $e^{\tilde{S}}_i = \mathbb{I}\left[\frac{1}{p}(B_1^i(\tilde{S}) + \dots +~B_p^i(\tilde{S})) > \alpha_i\right], i = 1 \dots l$. Для определения корректирующей операции надо задать набор порогов $\alpha_i$ для каждого класса.
	</p><p>
	Заметим, что применением корректирующих операций из данного семейства к модели алгоритмов мы действительно его расширяем(по крайней мере не сужаем), т.к. данное семейство $\left(\mathfrak{M}[\pi]\right)^p \to \mathfrak{I}$ содержит тождественную корректирующую операцию.
	</p>
	<h2>Параметры корректирующей операции</h2>

	<p>
		<table border="1">
			<tr>
				<th>Название</th>
				<th>Обозначение</th>
				<th>Область значений</th>
				<th>Тип параметра</th>
			</tr>
			<tr>
				<td>Число операторов</td>
				<td>$p$</td>
				<td>$\mathbb{Z}_{+}$</td>
				<td>Настраиваемый</td>
			</tr>
			<tr>
				<td>Операторы</td>
				<td>$(B_1, \dots, B_p)$</td>
				<td>$(\mathfrak{M}[\pi])^p$</td>
				<td>Настраиваемый</td>
			</tr>
			<tr>
				<td>Пороги</td>
				<td>$\alpha_i, i=1,\dots,l$</td>
				<td>[-1, 1]</td>
				<td>Оптимизируемый</td>
			</tr>
		</table>
	</p>
	<p>
	Пороги $\alpha_i$ лежат на отрезке $[-1, 1]$. Такое ограничение связано с тем, что функция активации принимает значения от $-1$ до $1$.
	</p>
	<h2>Настройка корректирующей операции</h2>
	Единственные оптимизируемые параметрами &mdash; коэффициенты $ \alpha_i, i=1,\dots,l$. Оптимальные значения $ \alpha_i, i=1,\dots,l$ находятся при минимизации следующего функционала:
	\begin{equation}
		Q(\alpha_1, \cdots, \alpha_l, \tilde{S}) = \sum_{j = 1}^q {\sum_{i = 1}^l { \mathbb{I}\left[\sum_{k = 1}^p B_i^k (S_j) > \alpha_i\right] - c_j(i) } } \longrightarrow \underset{\alpha_i}{\min}
	\end{equation}
        Результирующая задача оптимизации решается методом Монте-Карло.
	






        <h1>Семейство корректирующих операций 2: Полиномиальное семейство</h1>

        Полиномиальное семейство корректирующих операций задается следующим образом:

        <br>

        Пусть $B_1, \dots, B_p$ &mdash; алгоритмические операторы.
        Полиномиальной корректирующей операций называется отображение
        \[
            F(B_1, \dots, B_p) = \alpha_1 B_1 + \dots + \alpha_p B_p +
                                 \alpha_{11} B_1^{\ast2} + \dots + \alpha_{1p} B_1\ast B_p +
                                 \alpha_{22} B_2^{\ast2} + \dots + \alpha_{2p} B_2\ast B_p +
                                 \dots + \alpha_{pp} B_p^{\ast2},
        \]
        где $\alpha_1, \dots, \alpha_p, \alpha_{11}, \dots, \alpha_{pp}$ &mdash;
        некоторые коэффициенты
        (всего их $\frac{p(p+1)}{2} + p = \frac{p(p+3)}{2}$),
        а операции $^{\ast2}$ и $\ast$ означают соответственно
        покомпонентное возведение в квадрат и умножение двух векторов.

        <br>

        Заметим, что полиномиальное семейство содержит
        тождественное отображение $F(B) = B$,
        поэтому данная корректирующая операция
        не сужает модель алгоритмических операторов.

        <br>


        <h2>Использование полиномиального семейства корректирующих операций</h2>

        При решении задачи классификации на $l$ классов объектов
        из пространства начальных информаций $\mathfrak{I}_i = \mathbb{R}^2$
        было введено пространство оценок $\mathfrak{I}_e = [0,1]^l$.
        В качестве семейства корректирующих операций взяты
        полиномиальные корректирующие операции,
        действующие из $\mathfrak{I}_e^p = ([0,1]^l)^p$ в пространство $\mathfrak{I}_e = [0,1]^l$.

        <br>

        Таким образом, полиномиальная корректирующая операция принимает в качестве аргумента вектор элементов пространства оценок, и выдает элементом пространства оценок, что и требуется.	 

        <br>

        В общем случае она имеет вид (видно, что алгоритм полностью детерминирован):
        \[
            F(B_1,\dots,B_p)(S) = \alpha_1 B_1(S) + \dots + \alpha_p B_p(S) +
        \]
        \[
                                  + \alpha_{11} B_1(S)^{\ast2} + \dots + \alpha_{1p} B_1(S)\ast B_p(S) +
                                    \alpha_{22} B_2(S)^{\ast2} + \dots + \alpha_{2p} B_2(S)\ast B_p(S) +
                                    \dots + \alpha_{pp} B_p(S)^{\ast2} =
        \]
        \[
                                  = \sum\limits_{i=1}^p \alpha_i B_i(S) +
                                    \sum\limits_{1\leqslant i \leqslant j \leqslant p} \alpha_{ij}B_i(S) \ast B_j(S),
        \]
        \[
            B_i \in \mathfrak{M}_0 : \mathfrak{I}_i \to \mathfrak{I}_e,
            F \in \mathfrak{F} : \mathfrak{I}_e^p \to \mathfrak{I}_e.
        \]

        <br>

        <h2>Параметры полиномиального семейства корректирующих операций</h2>

        <br>


        <table border="1">
        <caption></caption>
        <tr>
        <th>Название</th>
        <th>Обозначение</th>
        <th>Область значений</th>
        <th>Тип параметра</th>
        </tr>
        <tr>
        <td>Число операторов</td>
        <td>$p$ </td>
        <td>$\{1, \dots, 10\}$</td>
        <td>Оптимизируемый</td>
        </tr>
        <tr>
        <td>Степень многочлена</td>
        <td></td>
        <td>$\mathbb{Z}^+$  </td>
        <td>Фиксированный</td>
        </tr>
        <tr>
        <td>Коэффициенты</td>
        <td>$\alpha_i, \alpha{ij} $ </td>
        <td>$[0, 1]$</td>
        <td>Вычисляемый</td>
        </tr>
        <tr>
        <td>Операторы</td>
        <td>$B_i$ </td>
        <td>$\mathbb{R}^2 \to [0, 1]^l$</td>
        <td>Оптимизируемый</td>
        </tr>
        </table>

        <br>

        <h2>Настройка (выбор корректирующей операции из семейства)</h2>

        <br>

        Настроить корректирующую операцию &mdash;
        значит указать значения её параметров.
        Настройку параметров корректирующей операции
        будем проводить в ходе минимизации функционала потерь
        $Q(F(B_1, \dots, B_p), \widetilde{S})$.
        Как было отмечено ранее,
        минимизация производится по параметрам
        $\{\alpha_1, \dots, \alpha_p, \alpha_{11}, \dots, \alpha_{pp}\}$.

        <br>

        <h3>Настройка $p$ операторов</h3>

        <br>

        Пусть корректирующая операция имеет вид:
        \[
            F(B_1, \dots, B_{p-1})(S) = \sum\limits_{i=1}^{p-1} \alpha_i B_i(S) +
                                        \sum\limits_{1\leqslant i \leqslant j \leqslant p-1} \alpha_{ij}B_i(S) \ast B_j(S),
        \]
         При добавлении нового оператора в композицию рассмотрим задачу минимизации
        \[
            Q(F(B_1, \dots, B_p),\widetilde S) = \Vert \sum\limits_{i=1}^{p-1} \alpha_i B_i(\widetilde S) +
                                                 \sum\limits_{1\leqslant i \leqslant j \leqslant p-1}
                                                 \alpha_{ij}B_i(\widetilde S) \ast B_j(\widetilde S) -
                                                 C^{-1}(r(\widetilde{S})) \Vert^2 \to \min\limits_{B_1, \dots, B_p, F}.
        \]
        Мы умеем решать задачу минимизации
        для одного алгебраического оператора
        \[
            Q(B, \widetilde{S}) = \Vert B(\widetilde{S}) - C^{-1}(r(\widetilde{S})) \Vert^2 \to \min_B,
        \]
        где $r(\widetilde{S})$ &mdash; вектор классификации выборки.
        Задачу для одного оператора и известной корректирующей операции поставим так:
        \[
            Q(B, \widetilde{S}) = \Vert \alpha_1^*B_1(\widetilde{S}) + \alpha_{11}^* B_1(\widetilde S)^{\ast 2} -
                                  C^{-1}(r(\widetilde{S})) \Vert^2 \to \min_B.
        \]
        Пусть теперь в суперпозиции есть $p-1$ оператор
        и нужно добавить и настроить еще один.
        Представим минимизируемый функционал в виде
        \[
            Q(F^*(B_1^*, \dots, B_{p-1}^*, B_p),\widetilde{S}) =
            \Vert B_p(\widetilde{S})(\alpha_p^* + \alpha_{1p}^*B_1^*(\widetilde{S}) + \dots +
            \alpha_{p-1,p}^*B_{p-1}^*(\widetilde{S})) + \alpha_{pp}^*(B_p(\widetilde{S}))^{\ast 2} -
        \]
        \[
            \underbrace{ (C^{-1}(r(\widetilde{S})) -
            \alpha_1^*B_1^*(\widetilde{S}) - \dots - \alpha_{p-1}^*B_{p-1}^*(\widetilde{S})-
            \alpha_{11}^*(B_1^*(\widetilde{S}))^{\ast 2} - \dots -
            \alpha_{p-1, p-1}^*(B_{p-1}^*(\widetilde{S}))^{\ast 2})}_{\hat{C^{-1}}} \Vert^2 \to \min\limits_{B_p},
        \]
        и подставляя $\hat{C^{-1}}$ вместо $C^{-1}(r(\widetilde S))$
        в функционал $Q(B,\widetilde S)$,
        можем настроить оператор $B_p$
        при фиксированных $\{B_1^*,\dots,B_{p-1}^*,F^*\}$.


        <h3>Настройка коэффициентов корректирующей операции</h3>


        После добавления нового оператора в композицию
        перенастроим веса линейной комбинации в $F$
        при фиксированных алгебраических операторах
        $\{B_1^*, \dots, B_p^*\}$.
        Минимизация производится по параметрам
        $\{\alpha_1, \dots, \alpha_p, \alpha_{11}, \dots, \alpha_{pp}\}$.
        Отметим, что при фиксированных значениях операторов
        корректирующая операция линейным образом
        зависит от своих коэффициентов.
        Поэтому данная оптимизационная задача решалась так же,
        как и для линейной корректирующей операции,
        с той лишь разницей,
        что оптимизация проводится не в $p$-мерном пространстве,
        как в линейном случае, а в $\frac{p(p+3)}{2}$-мерном.

        <br>

        Итерационный процесс перенастройки последнего добавленного оператора
        и коэффициентов корректирующей операции (внутренний цикл) прекращается,
        когда разница значений оптимизируемого функционала
        на двух соседних итерациях становится меньше некоторого числа.

        <br>

        Добавление новых операторов (итерации внешнего цикла) прекращается,
        если композиция уже состоит из 10 операторов,
        либо если значение оптимизируемого функционала
        на текущей и предыдущей итерациях внешнего цикла
        меньше некоторого числа.
{% endblock report %}

<!-- vim: set ft=htmldjango si sw=2 : -->
