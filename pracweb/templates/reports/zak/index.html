{% extends "reports_base.html" %}
{% block author %}Евгений Зак{% endblock %}
{% block topic %}Настройка суперпозиции алгебраических операторов с помощью корректирующих операций{% endblock %}

{% block report %}
    <h1>Введение</h1>
	
	21 век - век информационных технологий. Век, в котором информация играет ключевую роль.
	Человечество, понимая эту роль, ищет способы и инструменты для обработки и анализа этой информации.
	Одна из наиболее распространенных задач обработки информации является задача классификации.
	Она состоит в распределении объектов информации по классам в результате обучения по прецедентам. 
	Существует множество подходов к решению задачи классификации, один из которых мы рассмотрим в данной работе,
	а именно алгебраический подход к задаче классификации.
	Идея этого подхода достаточно проста и красива. Трудно построить сразу нужный алгоритм, 
	действующий из пространства начальных информаций в пространство финальных информаций.
	Поэтому мы делаем это в два шага: сначала переходим в удобное нам вспомогательное пространство оценок,
	а из него уже в итоговое пространство.
	Также при применении данного подхода возникает возможность рассматривать различные суперпозиции 
	алгоритмических операторов. Это расширяет семейство алгоритмов и может привести к повышению качества
	получаемого решения.
	В данном отчете мы применяем этот подход на практике,
	рассматриваем несколько моделей операторов и корректирующих семейств,
	их суперпозиции, делаем выводы о качестве работы построенных алгоритмов.
	
	<h1>Задача классификации</h1>
	<h2>Общая постановка</h2>
	На неформальном языке задача классификации заключается в некотором "разумном" отнесении объектов к классам. 
	Будем рассматривать $q$ объектов и $l$ классов. Задача классификации -- задача обучения с учителем.
	Поэтому рассматривается также обучающая выборка: множество $m$ объектов, для которых известны принадлежности к классам.
	
	Опишем общую постановку задачи формально. Пусть:
	<UL>
	<LI> $I_{ob}$ &mdash; пространство описаний объектов. Дескриптивная функция D сопоставляет каждому объекту
	его признаковое описание. Например, $D(s) = \|\rho_t(s, s^k)\|_{mn}$, где s &mdash; рассмтриваемый объект, 
	$s^k$ &mdash; объект обучающей выборки $k = 1, \dots m$, $\rho_t$ &mdash; некоторая метрика, $t = 1, \dots n$. 
	Tогда $I_{ob} = \mathfrak{C_{mn}}(\mathbb{R}_{+})$.
	<LI> $I_{cl}$&mdash; пространство описаний классов. Например, описанием j-го класса может быть вектор 
	$(P_j(s^1), \dots P_j(s^m)$, где $P_j)$ &mdash; предикат, говорящий о принадлежности объектов данному классу.
	Тогда $I_{cl} = E_2^m$.
	<LI>$\mathfrak{I_i}$ &mdash; пространство начальных информаций. Это пространство матриц размерности $q$ на $l$:
	$\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{I})}$,
	где $\mathfrak{I}$ &mdash; пространство совместных описаний объектов и классов, т.е. $\mathfrak{I} = I_{ob} \times I_{cl}$.
	<LI>$\mathfrak{I_f}$ &mdash; пространство финальных информаций. Это пространство матриц той же размрности: $\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{\tilde{I}})}$,
	где $\mathfrak{\tilde{I}} = E_2$, т.е. каждый элемент матрицы говорит о принадлежности соответсвующего объекта соответсвующему классу; 
	<LI>$I^u$ &mdash; универсальные ограничения. Эти ограничения не проверяются констрактивно, а задаются как некоторый "мешок" отображений,
	например, непрерывные отображения, монотонные, или в общем случае &mdash; отображения некоторой категории.
	<LI>$I^l$ &mdash; локальные ограничения. Эти ограничения задаются конструктивно. Например, это требование принимать истинные значения на прецедентах.
	Такое требование называется корректностью алгоритма. Заметим, что в данном задании мы не будем вводить такое требование.
	<LI>$\mathfrak{m}$ &mdash; модель алгоритмов, или семейство алгоритмов, удовлетворяющее определенным локальным и глобальным ограничениям.
	$\mathfrak{m:*}$ &mdash; семейство всех возможных алгоримтов. 
	</UL>
	
	Задача заключается в построении алгоритма (отображения) $A$, действующего из пространства начальных информация в пространство финальных информация
	и удовлетворяющих глобальным и локальным ограничениям:
	<p>
	\[
	A \in \mathfrak{m}^*: \mathfrak{I_i} \rightarrow \mathfrak{I_f}, \quad	A \in \mathfrak{m}^*[I^u], A \in \mathfrak{m}^*[I^l]
	\]
	</p>
	
	<h2>Конкретизация постановки в условиях практикума</h2>
	Перейдем от общей остановки задачи к непостредственно той, которую будем рассматривать в рамках данного задания.
	
	<UL>
	<LI> Объектами являются точки на плоскости. Пространство описания объектов $I_{ob} = \mathbb{R}^2$.
	
	<LI> Универсальные ограничения $I^u$: однородность объектов (порядок объектов неважен для алгоритма) и независимость объектов
	(алгоритм осущеcтвляет классификацию каждого отдельного объекта независимо).
	Следующим универсальным ограничением является принадлежность объекта только к одному классу.
	Из этого огранчения следует, что классы не являются независимыми.
	Однако классы являются в нашем случае однородными: алгоритм не различает классы и относится одинаково ко всем.
	Кроме того, мы будем рассматривать алгоритмы, принадлежащие только определенному семейству.
	Далее в отчете мы подробно опишем оба рассматриваемых семейства.
	
	<LI> Из независимости объектов следует, что можно рассматривать задачу классификации отдельно для каждого объекта.
	Кроме того, договоримся рассматривать информацию о прецедентах и классах как параметры алгоритма.
	Тогда можно сказать, что пространство начальных информаций $\mathfrak{I_i} = \mathbb{R}^2$.
	
	<LI> Из условия о принадлежности только одному классу следует, что можно задать результат классификации объекта
	меткой класса: числом из множества $\{1, \dots l\}$. 
	Так как рассматривается алгоритм для классификации отдельного объекта, получаем пространство финальных информаций 
	$\mathfrak{I_f} = \{1, \dots l\}$.
	</UL>
	

	<h1>Оптимизационный подход &mdash; для алгоритмов</h1>
	В поставленной задаче мы не требуем условия корректности алгоритма,
	т.к. поиск корректного алгоритма часто является неразрешимой задачей, а также может приводить к переобучению.
	Поэтому необходим иной способ учета обучающей информации и выбора алгоритма из семейства.
	При оптимизационном подходе вводится некоторый функционал качества и выбирается алгоритм, на котором достигается минимум:
	\[
	Q(A, \tilde{S}) \longrightarrow \min,
	\]
	где $\tilde{S}$ &mdash; обучающая выборка.
	Конкретный вид фукнционала может быть различным. Например, это может быть число ошибок алгоритма на обучающей выборке.
	Тогда мы будем выбирать корректный алгоритм (в случае, если такой алгоритм существует для данной выборки при остальных ограничениях задачи).
	В рамках этого задания мы не будем пользоваться функционалом вида $ Q(A, \tilde{S}) $. 
	В дальнейшем мы рассмотрим алгоритм $ A $ в виде композиции алгоритмического оператора и решающего правила и 
	будем решать оптимизационную задачу для оператора, а не для алгоритма.
	
	<h1>Алгебраический подход &mdash; идея декомпозиции</h1>
	<p>
	В общем случае работать с пространством начальных и финальных информаций неудобно, 
	поэтому в рамках алгебраического подхода решения задач классификации переходят в некоторое пространство, 
	называемое пространство оценок. Его выбирают произвольно, так, чтобы было удобно. 
	Для выполнения этого перехода в другое пространство алгоритм рассматривают как композицию алгоритмического оператора 
	и решающего правила $A = C \circ B$, 
	где $ B: \mathfrak{I_i} \rightarrow \mathfrak{I_e} $ &mdash; алгоритмический оператор, 
	$ C: \mathfrak{I_e} \rightarrow \mathfrak{I_f} $ &mdash; решающее правило. 
	Здесь $\mathfrak{I_e}$ &mdash; пространство оценок. В данном задании выберем $\mathfrak{I_e} = \mathbb{R}^l$.
	</p>
	
	<img src="diagram0.png"  width="200">
	
	<p>
	В рамках данного задания $ B \in \mathfrak{m_0} $, 
	где $ \mathfrak{m_0} = \mathfrak{m_{0_1}} \cup \mathfrak{m_{0_2}} $ &mdash; некоторое семейство операторов &mdash; объединение двух семейств,
	конкретный вид которых будет рассмотрен далее. А решающее правило зафиксируем как $ C = argmax $.
	</p>
	
	<h1>Оптимизационный подход &mdash; для операторов</h1>
	Будем искать не оптимальный алгоритм, а оптимальный оператор, 
	зафиксировав при этом решающее правило, 
	т.е. введем функционал вида $ Q(B, \tilde{S}) $. Этот функционал можно выбирать произвольным образом.
	
	Заметим, однако, что при данном подходе мы сталкиваемся с проблемой правильных оценок для элементов обучающей выборки, 
	т.к. мы знаем только метки классов для них, а не оценки.
	
	В ходе выполнения данной работы, был выбран функционал качества
	\begin{equation}\label{quality functional for operator}
		Q(B, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel B(S_i) - c_i \parallel^2} \longrightarrow \underset{B}{\min},	
	\end{equation}
	где $ c_i $ &mdash; оценки для объектов: бинарный вектор размерности $l$ с единсвтвенной единицей в позиции,
	соответсвующей классу, которму принадлежит объект.
	
	
<h1>Алгебраический подход &mdash; идея суперпозиции</h1>
	<p>
	Представим теперь наш алгоритм не в виде $ A = C \circ B$, 
	а в виде $ A = C \circ F \circ B $, где $F: \mathfrak{I_e} \rightarrow \mathfrak{I_e}$ &mdash; корректирующая операция.
	</p>
	
	<img src="diagram1.png"  width="220">
	
	<p>
	Основная идея алгебраического подхода состоит в следующем. 
	В общем случае выбранная нами модель операторов может не иметь оптимума, 
	поэтому будем использовать алгебраическое расширение модели. 
	Будем строить не один оператор, а несколько, и использовать их суперпозицию $A = C \circ F(B_1, \dots, B_p)$.
	</p>
	
	<img src="diagram2.png"  width="310">
	
	<p>
	Корректирующие операции $ F \in \mathfrak{f} $, где $ \mathfrak{f} = \mathfrak{f_1} \cup \mathfrak{f_2} $ &mdash; семейство корректирующих операций, 
	состоящее из двух подсемейств, где $ \mathfrak{f_i} = \{G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e} | p \in \mathbb{N} \}, i = 1, 2 $.
	</p>
	
	На самом деле корректирующая операция есть некоторое другое отображение
	\begin{equation}
		F: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e}\}^p \rightarrow \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \}
	\end{equation}
	Но она индицируется операцией
	\begin{equation}
		G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e}
	\end{equation}
	если
	\begin{equation}
		F(B_1, \cdots, B_p)(S) = G(B_1(S), \cdots, B_p(S))
	\end{equation}
	
	<h1>Оптимизационный подход &mdash; для суперпозиций</h1>
	Выше мы ввели функционал качества для настройки одного оператора.
	\begin{equation}
		Q: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} \times (\mathfrak{I_i}, \mathfrak{I_e})^q \rightarrow \mathbb{R}
	\end{equation}
	Т.к. $ F(\cdot) \in \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} $, то мы можем в тот же функционал подставить вместо одного оператора суперпозицию.
	\begin{equation}
		Q(F(B_1, \cdots, B_p), \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel F(B_1(S_i), \cdots, B_p(S_i)) - c_i \parallel^2}
	\end{equation}
	Таким образом, получаем оптимизационную задачу:
	\begin{equation}\label{quality functional for superposition}
		Q(F(B_1, \cdots, B_p), \tilde{S}) \longrightarrow \underset{p, B_1, \cdots, B_p, F}{\min}
	\end{equation}
	
		
	<h1>Итерационный процесс построения суперпозиции</h1>
	Будем последовательно добавлять операторы в суперпозицию.
	
	<UL>
		<LI> $B_1 = \underset{B}{argmin} Q(B, \tilde{S})$
		<LI> $(F, B_1, B_2) = \underset{F, B_1, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		Решать эту задачу слишком сложно, поэтому будем использовать неэквивалентное упрощение. 
		В качестве первого оператора будем использовать оператор полученный на предыдущем шаге. Т.е.
		$(F, B_1, B_2) = \underset{F, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		<LI> $(F, B_1, B_2, B_3) = \underset{F, B_3}{argmin} Q(F(B_1, B_2, B_3), \tilde{S})$
		$\cdots$
	</UL>
	Это внешний цикл настройки суперпозиции, который останавливается, 
	когда $|Q_{new} - Q_{old}| < \varepsilon $, где $Q_{new}$ и $Q_{old}$ &mdash; значение функционала, 
	достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	При каждой итерации внешнего цикла надо решать задачу оптимизации 
	$(F, B_1,\cdots, B_p) = \underset{F, B_p}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$. 
	Для этого входим во внутренний цикл, который будет решать эту задачу методом покоординатного спуска(опять используем неэквивалентное упрощение).
	
	<OL>
		<LI>$F^0$ &mdash; задаем некоторое начальное приближение для корректирующей операции
		<LI>$B_p^i = \underset{B_p}{argmin} Q(F^{i-1}(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$
		<LI>$F^i = \underset{F}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p^i), \tilde{S})$
	</OL>
	
	 Проводим итерации цикла 1 - 2 до того, как $|Q'_{new} - Q'_{old}| < \delta $, где $Q'_{new}$ и $Q'_{old}$ &mdash; значение функционала, 
	 достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	 Для реализации изложенного итерационного подхода необходимо уметь выполнять следующие действия:
	 <UL>
	 	<LI> $B^* = \underset{B}{argmin}Q(B, \tilde{S})$;
	 	<LI> $F^*_p \mapsto F^0_{p+1}$;
	 	<LI> $B^* = \underset{B}{argmin}Q(F(B_1, \cdots, B_{p-1}, B), \tilde{S})$;
	 	<LI> $F^* = \underset{F}{argmin}Q(F(B_1, \cdots, B_p), \tilde{S})$
	 	<LI> критерий останова для внутреннего цикла $|Q'_{new} - Q'_{old}| < \delta $;
	 	<LI> критерий останова для цикла наращивания p $|Q_{new} - Q_{old}| < \varepsilon $;
	 </UL>	
		Для добавления сразу двух операторов:
	<UL>	
	 	<LI> $F^*_p \mapsto F^0_{p+2}$;
	 	<LI> $(B^*_{p+1}, B^*_{p+2}) = \underset{B', B''}{argmin}Q(F(B_1, \cdots, B_p, B', B''), \tilde{S})$;
	</UL>
		Для перенастройки ранее добавленного оператора:
	<UL>
	 	<LI> $B^*_i = \underset{B}{argmin}Q(F(B_1, \cdots, B_{i-1}, B, B_{i+1}, \cdots, B_p), \tilde{S})$.
	 </UL>
	
	<h1>Типы параметров по отношению к оптимизационному подходу</h1>
	Параметры можно разделить на две группы: оптимизируемые и неоптимизируемые.
	Неоптимизируемые параметры либо фиксируются, либо настраиваются по выборке (с помощью какой-либо эвристики).
	Оптимизируемые параметры могут быть либо найдены аналитиески, либо с помощью численной процедуры оптимизации.	
	
	Как говорилось в введении, для иллюстрации действенности подхода, в данном отчете рассматриваются несколько 
	моделей алгоритмических операторов и корректирующих семейств, к описанию которых мы сейчас переходим.
	
	<h1>Модель алгоритмов 1. </h1> 
	
	<h1>Метод парзеновского окна с простейшим финитным ядром.</h1>
<h2>Вербальное описание идеи</h2>
Идея метода парзеновского окна достаточно проста и интуитивна понятна. 
Она заключается в предположении, что объекты одного класса в пространстве
лежат близко друг к другу, при этом межклассовые расстояния существенно больше внутриклассовых. 
Тогда классифицировать поступивший объект можно основываясь на том, к каким классам принадлежат объекты в некоторой его окрестности, определенной заранее. 

Таким образом, метод утверждает, что наиболее вероятным классом для поступившего объекта будет тот, к которому относиться наибольшее число 
объектов из окрестности объекта.

<h2>Формальное описание модели</h2>
Опишем формально метод парзеновского окна с простейшим финитным ядром.

Дан набор из $m$ объектов. Объект $S^i$ приписан классу $y^i \in Y$, то есть класс j описывается $m_j$ объектами, которые называют эталонами класса. 
В пространстве начальной информации $\mathfrak{I}_i = \mathbb{R}^2$ задана метрика, показывающая насколько близки два объекта в простанстве начальной информации.


Оценка за конкретный класс $j$:

$$\Gamma_j(x)~=~\sum_{\substack{i: (S^i, r_i) \in \mathfrak{S}\\r_i = j}} K\left(\frac{\rho(x, S^i)}{h}\right),$$

где $K:~\mathbb{R}_+~\longrightarrow~\mathbb{R}_+$ - сглаживающее ядро. В данном методе исследуется простейшее финитное ядро:
\[
K(x, S^i) =
\begin{cases}
1, & \text{если $\rho(x, S^i) \leq r$;} \\
0, & \text{если $\rho(x, S^i) > r$.}
\end{cases}
\]

<h2>Использование модели</h2>
Очевидно, что данную модель алгоритма можно использовать в алгоритмическом подходе к задаче классификации. 
Вместе с тем, метод легко реализуем, и поэтому часто применяется на практике вне алгебраического подхода.

<h3>Точное и полное описание действия алгоритмического оператора</h3>
Применяя сглаживающее ядро, формулу можно переписать в виде:

$$\Gamma_j(x)~=~\frac{\sum_{i = 1}^{q}{[r_i = j][\rho(x, S^i) \leq r]}}{\sum_{i = 1}^{q}{[\rho(x, S^i) \leq r]}},$$

где $r$ - радиус ядра, параметр модели, который нужно будет настраивать.


Метод парзеновского окна с финитным ядром можно использовать как модель алгоритмических операторов 
с пространством начальных информаций, таких, которые необходимы метрике для сравнения данного ответа с эталонами, и пространством оценок, 
представленным в виде отрезка от $0$ до $1$. Аргумент является элементом пространства исходной информации. 
Результат работы алгоритма является элементом пространства оценок. 

Заметим, что алгоритм является полностью детерменированным.

<h2>Перечисление параметров модели операторов</h2>

В таблице продемонстрированы параметры модели:

<table border="1">
<caption></caption>
<tr>
<th>Название параметра</th>
<th>Обозначение параметра</th>
<th>Область допустимых значений</th>
<th>Тип параметра по отношению к оптимизации</th>
</tr>
<tr>
<td>Набор эталонов</td>
<td>$\mathfrak{S}$</td>
<td>$(S, r) \in (\mathfrak{J}_i, \mathfrak{J}_f)$</td>
<td>Настраиваемый</td>
</tr>
<tr>
<td>Метрика</td>
<td>$\rho(S,S^i)$</td>
<td>$\mathbb{R}_{+}$ </td>
<td>Фиксированный</td>
</tr>
<tr>
<td>Ядро</td>
<td>финитное</td>
<td></td>
<td>Фиксированный</td>
</tr>
<tr>
<td>Ширина окна</td>
<td>$r$</td>
<td>$(0, +\infty)$</td>
<td>Настраиваемый</td>
</tr>
</table>

<h2>Настройка параметров</h2>
<h3>Значения фиксированных параметров</h3>
Определим используемую нами метрикой. Пусть это будет Евклидова метрика. Выбор метрики не зависит от выборки, однако в общем случае, выбор метрики
может быть основан на эвристических соображениях исходя из определения задачи классификации и свойств объектов.
Заметим также, используемое финитное ядро не зависит от выборки, оно фиксировано. 

<h3>Формулы/процедуры для вычисляемых параметров</h3>
При выбранной нами Евклидовой метрике в двухмерном случае расстояние между двумя объектами $S$ и $S^i$ определятся следующим образом:
$$\rho(S, S^i) = ((S_x - S^i_x)^2 + (S_y - S^i_y)^2)^{\frac{1}{2}}$$.

<h3>Оптимизационная задача в явном виде</h3>
Для того, чтобы вычислить наилучшее значение финитного ядра, нужно решить оптимизационную задачу. 
Оптимизация проходит по двум параметрам: по набору эталонов $\widetilde{S}$ и по радиусу финитного ядра $r$ и состоит из двух шагов. 
Сначала оптимизируется выборка эталонов из контрольной выборки, а уже потом для каждого выбора эталонов ищется радиус ядра. 
Метод Монте-Карло, рассмотренный на практикуме на ЭВМ кафедры ММП в 7 семестре, прекрасно подходит для решения поставленной оптимизационной задачи. 
Причём на первом шаге, критерием остановки будет являться тот факт, что за последние 2 итерации значение функционала не 
изменилось или изменилось не значительно, а на втором шаге будут рассматриваться последние 5 итераций. 
Может возникнуть ситуация, когда в ядро не попадёт ни одной точки, тогда ответом классификатора следует принять тот класс, 
точек которого больше всего или который более вероятен.

<h3>Формулы для аналитически оптимизируемых параметров</h3>
Необходимо минимизировать функционал качества:
$$ Q(B(r, \widetilde{S}), S) \rightarrow \min_{r \in (0, +\infty), \widetilde{S} \subset S}$$

$$ Q(B(r, \widetilde{S}), S) = \frac{1}{ql} \sum_{(S^i,r_i) \in S} \| B(r, \widetilde{S})[S^i] - C^{-1}(r_i) \| ^2 = \frac{1}{ql} \sum_{(S^i,r_i) \in S} \| \frac{\sum_{(S^j,r_j) \in \widetilde{S}}{[r_i = r_j][\rho(S^i, S^j) \leq r]}}{\sum_{(S^j,r_j) \in \widetilde{S}}{[\rho(S^i, S^j) \leq r]}} - C^{-1}(r_i) \| ^2$$

<h3>Метод оптимизации для численно оптимизируемых параметров</h3>
Метод Монте-Карло оптимален для полученного типа задачи. Такой вывод можно сделать основываясь на том, что оптимизация по 
радиусу дискретна на небольшом количестве точек.

Отметим также, что метод применим для любой задачи класса, что является его положительным свойством, и найденный оператор даёт минимум функционала.
В качестве метода оптимизации могут быть выбранны и другие методы, однако данный вопрос остается за пределом настоящего отчета.


<h2>Исследование модели алгоритмических операторов</h2>
<h3>Пример 1. Задача, для которой есть в модели корректный алгоритм</h3>

<img border="0" src="pracwebParzen_ok.png" alt="Parzen good" width="410" height="410">

<br>
<h3>Пример 2. Задача, для которой в модели нет корректного алгоритма</h3>

<img border="0" src="pracwebParzen_bad.png" alt="Parzen bad" width="410" height="410">

<h1>Модель алгоритмов 2. </h1> 
<h1>Метод логарифмических кругов</h1>
<h2>Вербальное описание модели</h2>
В данной модели используется идея того, что объекты одного класса находятся близко 
друг к другу, а объекты разных классов находятся далеко друг от друга. Это предположение аналогично 
предположению в методе парзеновского окна, однако подход к реализации данной гипотезы отличается.
В данной модели предполагается, что в классах можно выбрать эталоны, 
по отношению к которым и будет измеряться близость или дальность объектов к классу.</p>

<h2>Формальное описание действия оператора</h2>
Опишем формально метод логарифимических шаров

\begin{equation}
\begin{split}
	&B(S_i) = ( \Gamma_1(S_i), \cdots, \Gamma_l(S_i))^T;
	\\
	&\Gamma_j(S_i) = z_j \ln {\frac{\rho(S_i, S_{0_j})}{R_{0_j}}},
\end{split}
\end{equation}	
где
<ul>
<li>$ \rho(\cdot, \cdot) $ -- метрика $ \mathfrak{I_i}^2 \rightarrow \mathbb{R}_{+} $.</li>
<li>$ z_j \in \{ +1, -1\} $ -- метка &laquo;близость-дальность&raquo;. Показывает, что мы считаем: &laquo;насколько близок данный объект к эталону класса&raquo; или &laquo;насколько далек&raquo;.</li>
<li>$ S_{0_j} $ -- эталон класса.</li>
<li>$ R_{0_j} $ -- радиус класса.</li>
</ul>
</p>


<h2>Использование модели</h2>
Очевидно, что данную модель алгоритма можно использовать в алгоритмическом подходе к 
задаче классификации. 
Вместе с тем, метод легко реализуем, и поэтому часто применяется на практике 
вне алгебраического подхода.

	
<h2>Перечисление параметров модели операторов</h2>

<table border="1">
<caption></caption>
<tr>
<th>Название параметра</th>
<th>Обозначение параметра</th>
<th>Область допустимых значений</th>
<th>Тип параметра по отношению к оптимизации</th>
</tr>
<tr>
<td>Метрика	</td>
<td> $\rho$ </td>
<td> $ \mathfrak{I_i}^2 \rightarrow \mathbb{R}_{+} $ </td>
<td> Константа</td>
</tr>
<tr>
<td>Метка &laquo;близость-дальность&raquo; </td>
<td> $ z_j $ </td><td> $ \{ +1, -1 \} $   </td>
<td> Константа</td>
</tr>
<tr>
<td>Эталон класса </td>
<td> $ S_{0_j} $ </td>
<td> $ \mathfrak{I_i} $ </td>
<td> Оптимизируемый</td>
</tr>
<tr><td>Радиус класса </td>
<td> $ R_{0_j} $ </td>
<td> $ \mathbb{R}_{+} $ </td>
<td> Константа</td>
</tr>
</table>

<h2>Настройка параметров</h2>
<h3>Значения фиксированных параметров</h3>
Положим фиксируемые параметры.
<ul>
<li>$ \rho(\cdot, \cdot) $ -- Евклидова метрика;</li>
<li>$ R_{0_j} \equiv R_0 - const \textrm{ } \forall j \in \{ \textrm{Класс 1, Класс 2, ..., Класс l} \} $;</li>
<li>$ z_j \equiv -1 \textrm{ } \forall j \in \{ \textrm{Класс 1, Класс 2, ..., Класс l} \}$.</li>
</ul>

<h3>Формулы/процедуры для вычисляемых параметров</h3>

Единственным оптимизируемым параметром является эталон каждого класса $ S_{0_j} $. Поиск оптимального значения параметра $ S_{0_j} $ происходит в результате минимизации функционала (\ref{quality functional for operator}). Выпишем явный вид $ Q $ как функции от $(S_{0_1}, \cdots, S_{0_l})$.
\begin{equation}
\begin{split}
	Q(S_{0_1}, \cdots, S_{0_l}, \tilde{S}) &= \frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\Gamma_j(S_i) - c_i(j))^2} } = \\ 
	&=\frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(- \ln {\frac{\rho(S_i, S_{0_j})}{R_0}} - c_i(j))^2} } = \\
	&=\frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\ln {\rho(S_i, S_{0_j})} - \ln {R_0} + c_i(j))^2} } \longrightarrow \underset{S_{0_1}, \cdots, S_{0_l}}{\min}, 
\end{split}
\end{equation}

<h3>Формулы для аналитически оптимизируемых параметров</h3>

Заметим, что каждый эталон $ S_{0_j} $ можно рассматривать отдельно, таким образом, получаем $ l $ задач оптимизации вида:
\begin{equation}
	Q(S_{0_j}, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {(\ln {\rho(S_i, S_{0_j})} - \ln {R_0} + c_i(j))^2} \longrightarrow \underset{ S_{0_j} }{\min}
\end{equation}


<h3>Метод оптимизации для численно оптимизируемых параметров</h3>
Полученная оптимизационная задача относится к классу квадратичных задач. 
Будем искать ее решение, как и в методе парзеновского окна, с помощью метода простого и эффективного метода Монте-Карло.

<h2>Исследование модели алгоритмических операторов</h2>
<h3>Пример 1. Задача, для которой есть в модели корректный алгоритм</h3>

<img border="0" src="pracweb_shary_ok.png" alt="Shary good" width="410" height="410">

<br>
<h3>Пример 2. Задача, для которой в модели нет корректного алгоритма</h3>

<img border="0" src="pracweb_shary_bad.png" alt="Shary bad" width="410" height="410">


<h1>Семейство корректирующих операций 1.</h1> 

<h1>Комитет старшинства с индивидуальными классами.</h1>

<h2>Вербальное описание идеи</h2>
Идея комитета старшинства с индивидуальными классами заключается в том, что алгоритмические операции 
упорядочиваются списком. После этого, по порядку списка мы перебираем результаты алгоритмических операторов, пока не выполниться критерий останова.
Если критерий не выполнился, и мы дошли до конца, то выбирается результат последнего алгоритмического оператора.

<h2>Описание семейства</h2>
Корректирующие операторы из семейства комитета старшинаства основываеются на упорядочивание 
алгоритмических операторов и последовательном выборе результата согласно порядку следования.
Этот способ работы корректирующей операции вводится в литературе под разными названиями: комитет с логикой
старшинства, решающий список правил, машина покрывающих множеств. Последнее название относится скорее к
корректирующией операции комитета старшинства с наборами классов.


<h2>Использование корректирующей операции</h2>

Пусть настраиваются суперпозиция алгоритмических операторов $B_1,\dots,B_p$ с 
векторами оценок $\Gamma_1, \dots, \Gamma_p$.  Корректирующая операция в общем случае выглядит так:

\begin{equation}
	\begin{split}
		 &F(\vec {\Gamma_1},\dots,\vec {\Gamma_p}) = (F_1(\Gamma_{1_1},\dots,\Gamma_{p_1}), \cdots, F_l(\Gamma_{1_1},\dots,\Gamma_{p_1}))^T\\
	\end{split}
	\end{equation}

Тогда каждому аргументу $\Gamma_r, 1 \leq r < p$ 
назначается класс $z_r$. Перебираем аргументы последовательно. Если на очередном шаге $r$ в векторе 
$\Gamma_r$ доминирует компонента $z_r$, то перебор прекращается и возвращается $\Gamma_r$. 
Иначе переходим к следующей оценке. Если достигли последнего аргумента $p$, то 
возвращаем $\Gamma_p$. Эта корректирующая операция относится к классу выбирающих, 
также ее называют решающим списком.

<br>
<h2>Перечисление параметров комитета старшинства с индивидуальными классами</h2>
<br>
В таблице продемонстрированы параметры семейства:
<br>
<center>

<table border="1">
<caption></caption>
<tr>
<th>Название параметра</th>
<th>Обозначение параметра</th>
<th>Область допустимых значений</th>
<th>Тип параметра</th>
</tr>
<tr>
<td><center>Классы комитета</center></td>
<td><center>$Z = \{z_1, \dots, z_{p-1}\}$</center></td>
<td><center>$\{1,\dots,l\}^{p-1}$</center></td>
<td>Численно оптимизируемый</td>
</tr>
</table>
</center>
<br>

<h2>Настройка параметров</h2>
<h3>Метод оптимизации для численно оптимизируемых параметров</h3>

Так как размер строящейся суперпозиции небольшой, то оптимизации классов 
для комитета старшинства проводится с помощью полного перебора всех возможных классов, 
соотвествующих каждому алгоритмическому оператору. Выбирается та совокупность классов 

<br>
<br>

<center>
$z_1,\dots, z_{p-1}$ 
</center>
<br>
для которых достигается минимум функционала корректирующей операции на обучающей выборке.

Также, для оптимизации классов могут быть применены другие оптимизационные метода, однако данный вопрос остается за пределами данного отчета.

	
<h1>Корректирующая операция.</h1> 

<h1>Монотонная линейная корректирующая операция</h1>

<h2>Вербальное описание</h2>
Семейство монотонных линейных КО &mdash; это семейство линейных преобразований над пространством оценок,
которая для данного набора оценок алгоритмических операторов возвращает значение из пространства оценок. 
Такая КО представляет собой линейную комбинацию оценок алгоритмических операторов с неотрицательными весами. 

<h2>Описание семейства</h2>
Корректирующие операции данного вида выглядят следующим образом:
$$F(B_1, \dots, B_P) = a_1 B_1 + \dots + a_p B_p, $$
где $a_1, \dots, a_p \ge 0, B_1, \dots, B_p $ - оценки соответствующих операторов

<h2>Использование корректирующей операции</h2>
Монотонная линейная корректирующая операция принимает на вход оценки $p$ алгоритмических операторов: $\vec{\Gamma}^1, \dots, \vec{\Gamma}^p$. Параметрами являются $p$ неотрицательных весов: $a_1, \dots, a_p$. 
Результатом корректирующей операции является оценка, подсчитываемая по следующей формуле:

<br>
<br>
<center>
	$\vec{\Gamma} = \alpha_1\vec{\Gamma}^1 + \ldots + \alpha_p\vec{\Gamma}^p$, &nbsp; где $\{\alpha_1, \dots, \alpha_p\ \in \mathbb{R}_+\}.$
</center>	
<br>

Результат работы корректирующего оператора полностью опеределен оценками алгоритмических моделей и своими параметрами и принадлежит пространству оценок.



<h2>Перечисление параметров монотонной линейной корректирующей операции</h2>
<br>
В таблице продемонстрированы параметры семейства:
<br>

<center>

<table border="1">
<caption></caption>
<tr>
<th>Название параметра</th>
<th>Обозначение параметра</th>
<th>Область допустимых значений</th>
<th>Тип параметра</th>
</tr>
<tr>
<td><center>Арность оператора</center></td>
<td><center>$p$</center></td>
<td><center>$\mathbb{N}$</center></td>
<td>Оптимизируемый</td>
</tr>
<tr>
<td><center>Веса оценок</center></td>
<td><center>$\vec{a}$</center></td>
<td><center>$\mathbb{R}_+^p$</center></td>
<td>Оптимизируемый</td>
</tr>
</table>
</center>
<br>
<h2>Настройка параметров</h2>
<h3>Метод оптимизации для численно оптимизируемых параметров</h3>
Необходимо настроить арность оператора и вектор весов. 

Арность оператора будем увеличивать на единицу, добавляя по одному распознающему оператору, 
пока не будет выполнен критерий останова &mdash;  достижение локального минимума функционала качества, 
то есть последний добавленный оператор не уменьшил функционал.

При фиксированной арности требуется минимизировать следующий функционал:
$$Q(\vec{\alpha}, S) = \frac{1}{ql} \sum_{i = 1}^{q} \|  \alpha_1\vec{\Gamma}^1 + \ldots + \alpha_p\vec{\Gamma}^p - C^{-1}(y_i) \| ^2.$$

Это задача условной непрерывной оптимизации по непрерывным параметрам. 
Для оптимизации вектора весов применим метод покоординатного спуска с убывающим шагом. 
Изначально положим все координаты вектора весов равными $0.1$, шаг равен $1$. 
Далее выбирается координата, увеличение которой на величину шага максимально уменьшит функционал качества. 
Ее значение увеличивается на величину шага. 
Если же такое увеличение любой из координат не уменьшает функционал, то величина шага уменьшается вдвое. 
Итерации продолжаются, пока величина шага больше $10^{-3}$.
<!--
<h1>Расширение модели 1 семейством 1 </h1>
  <h2>Исследование суперпозиции</h2>
  <h2>Пример 1. Задача, для которой в модели есть корректный оператор.</h2>
  <h2>Пример 2. Задача, для которой в модели нет корректного оператора</h2>
  
 <h1>Расширение модели 2 семейством 2 </h1>
  <h2>Исследование суперпозиции</h2>
  <h2>Пример 1. Задача, для которой в модели есть корректный оператор.</h2>
  <h2>Пример 2. Задача, для которой в модели нет корректного оператора</h2>
  
 <h1>Расширение моделей 1 и 2 семействами 1 и 2 </h1>
  <h2>Исследование суперпозиции</h2>
  <h2>Пример 1. Задача, для которой в модели есть корректный оператор.</h2>
  <h2>Пример 2. Задача, для которой в модели нет корректного оператора</h2>

  <h1>Описание программной платформы</h1>
  
  <h1>Заключение</h1>
  -->
{% endblock report %}

<!-- vim: set ft=htmldjango si sw=2 : -->
