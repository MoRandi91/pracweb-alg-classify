{% extends "reports_base.html" %}
{% block author %}Евгений Нижибицкий{% endblock %}
{% block topic %}Настройка суперпозиции алгебраических операторов с помощью корректирующих операций{% endblock %}

{% block report %}
    <h1>Введение</h1>

	В рамках этого задания по практикуму мы проводим исследование алгебраического
	подхода в задаче классификации со многими классами. Суть алгебраического подхода
	в том, что мы используем некоторые корректирующие операции, позволяющие объединять
	результаты других алгоритмов. Таким образом, мы на основе нескольких
	алгоритмов посредтственного качества можем построить одну хорошую
	композицию.

	В данной работе рассмотрены два слабых алгоритма и две корректирующие операции.

	Для настройки алгоритмов решалась задача минимизации функционала качества,
	т.е. фактически задача оптимизации. Эта задача была разделена на несколько
	более простых подзадач с помощью декомпозиции, а с помощью последующей
	суперпозицией был получен сильный классифицирующий алгоритм.

	<h1>Задача классификации</h1>
	<h2>Общая постановка</h2>
	На неформальном языке задача классификации заключается в некотором "разумном" отнесении объектов к классам.
	Будем рассматривать $q$ объектов и $l$ классов. Задача классификации -- задача обучения с учителем.
	Поэтому рассматривается также обучающая выборка: множество $m$ объектов, для которых известны принадлежности к классам.

	Опишем общую постановку задачи формально. Пусть:
	<UL>
	<LI> $I_{ob}$ &mdash; пространство описаний объектов. Дескриптивная функция D сопоставляет каждому объекту
	его признаковое описание. Например, $D(s) = \|\rho_t(s, s^k)\|_{mn}$, где s &mdash; рассмтриваемый объект,
	$s^k$ &mdash; объект обучающей выборки $k = 1, \dots m$, $\rho_t$ &mdash; некоторая метрика, $t = 1, \dots n$.
	Tогда $I_{ob} = \mathfrak{C_{mn}}(\mathbb{R}_{+})$.
	<LI> $I_{cl}$&mdash; пространство описаний классов. Например, описанием j-го класса может быть вектор
	$(P_j(s^1), \dots P_j(s^m)$, где $P_j)$ &mdash; предикат, говорящий о принадлежности объектов данному классу.
	Тогда $I_{cl} = E_2^m$.
	<LI>$\mathfrak{I_i}$ &mdash; пространство начальных информаций. Это пространство матриц размерности $q$ на $l$:
	$\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{I})}$,
	где $\mathfrak{I}$ &mdash; пространство совместных описаний объектов и классов, т.е. $\mathfrak{I} = I_{ob} \times I_{cl}$.
	<LI>$\mathfrak{I_f}$ &mdash; пространство финальных информаций. Это пространство матриц той же размрности: $\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{\tilde{I}})}$,
	где $\mathfrak{\tilde{I}} = E_2$, т.е. каждый элемент матрицы говорит о принадлежности соответсвующего объекта соответсвующему классу;
	<LI>$I^u$ &mdash; универсальные ограничения. Эти ограничения не проверяются констрактивно, а задаются как некоторый "мешок" отображений,
	например, непрерывные отображения, монотонные, или в общем случае &mdash; отображения некоторой категории.
	<LI>$I^l$ &mdash; локальные ограничения. Эти ограничения задаются конструктивно. Например, это требование принимать истинные значения на прецедентах.
	Такое требование называется корректностью алгоритма. Заметим, что в данном задании мы не будем вводить такое требование.
	<LI>$\mathfrak{m}$ &mdash; модель алгоритмов, или семейство алгоритмов, удовлетворяющее определенным локальным и глобальным ограничениям.
	$\mathfrak{m:*}$ &mdash; семейство всех возможных алгоримтов.
	</UL>

	Задача заключается в построении алгоритма (отображения) $A$, действующего из пространства начальных информация в пространство финальных информация
	и удовлетворяющих глобальным и локальным ограничениям:
	<p>
	\[
	A \in \mathfrak{m}^*: \mathfrak{I_i} \rightarrow \mathfrak{I_f}, \quad	A \in \mathfrak{m}^*[I^u], A \in \mathfrak{m}^*[I^l]
	\]
	</p>

	<h2>Конкретизация постановки в условиях практикума</h2>
	Перейдем от общей остановки задачи к непостредственно той, которую будем рассматривать в рамках данного задания.

	<UL>
	<LI> Объектами являются точки на плоскости. Пространство описания объектов $I_{ob} = \mathbb{R}^2$.

	<LI> Универсальные ограничения $I^u$: однородность объектов (порядок объектов неважен для алгоритма) и независимость объектов
	(алгоритм осущеcтвляет классификацию каждого отдельного объекта независимо).
	Следующим универсальным ограничением является принадлежность объекта только к одному классу.
	Из этого огранчения следует, что классы не являются независимыми.
	Однако классы являются в нашем случае однородными: алгоритм не различает классы и относится одинаково ко всем.
	Кроме того, мы будем рассматривать алгоритмы, принадлежащие только определенному семейству.
	Далее в отчете мы подробно опишем оба рассматриваемых семейства.

	<LI> Из независимости объектов следует, что можно рассматривать задачу классификации отдельно для каждого объекта.
	Кроме того, договоримся рассматривать информацию о прецедентах и классах как параметры алгоритма.
	Тогда можно сказать, что пространство начальных информаций $\mathfrak{I_i} = \mathbb{R}^2$.

	<LI> Из условия о принадлежности только одному классу следует, что можно задать результат классификации объекта
	меткой класса: числом из множества $\{1, \dots l\}$.
	Так как рассматривается алгоритм для классификации отдельного объекта, получаем пространство финальных информаций
	$\mathfrak{I_f} = \{1, \dots l\}$.
	</UL>


	<h1>Оптимизационный подход &mdash; для алгоритмов</h1>
	В поставленной задаче мы не требуем условия корректности алгоритма,
	т.к. поиск корректного алгоритма часто является неразрешимой задачей, а также может приводить к переобучению.
	Поэтому необходим иной способ учета обучающей информации и выбора алгоритма из семейства.
	При оптимизационном подходе вводится некоторый функционал качества и выбирается алгоритм, на котором достигается минимум:
	\[
	Q(A, \tilde{S}) \longrightarrow \min,
	\]
	где $\tilde{S}$ &mdash; обучающая выборка.
	Конкретный вид фукнционала может быть различным. Например, это может быть число ошибок алгоритма на обучающей выборке.
	Тогда мы будем выбирать корректный алгоритм (в случае, если такой алгоритм существует для данной выборки при остальных ограничениях задачи).
	В рамках этого задания мы не будем пользоваться функционалом вида $ Q(A, \tilde{S}) $.
	В дальнейшем мы рассмотрим алгоритм $ A $ в виде композиции алгоритмического оператора и решающего правила и
	будем решать оптимизационную задачу для оператора, а не для алгоритма.

<h1>Алгебраический подход &mdash; идея декомпозиции</h1>
	<p>
	В общем случае работать с пространством начальных и финальных информаций неудобно,
	поэтому в рамках алгебраического подхода решения задач классификации переходят в некоторое пространство,
	называемое пространство оценок. Его выбирают произвольно, так, чтобы было удобно.
	Для выполнения этого перехода в другое пространство алгоритм рассматривают как композицию алгоритмического оператора
	и решающего правила $A = C \circ B$,
	где $ B: \mathfrak{I_i} \rightarrow \mathfrak{I_e} $ &mdash; алгоритмический оператор,
	$ C: \mathfrak{I_e} \rightarrow \mathfrak{I_f} $ &mdash; решающее правило.
	Здесь $\mathfrak{I_e}$ &mdash; пространство оценок. В данном задании выберем $\mathfrak{I_e} = \mathbb{R}^l$.
	</p>

	<img src="diagram0.png"  width="200">

	<p>
	В рамках данного задания $ B \in \mathfrak{m_0} $,
	где $ \mathfrak{m_0} = \mathfrak{m_{0_1}} \cup \mathfrak{m_{0_2}} $ &mdash; некоторое семейство операторов &mdash; объединение двух семейств,
	конкретный вид которых будет рассмотрен далее. А решающее правило зафиксируем как $ C = argmax $.
	</p>

	<h1>Оптимизационный подход &mdash; для операторов</h1>
	Будем искать не оптимальный алгоритм, а оптимальный оператор,
	зафиксировав при этом решающее правило,
	т.е. введем функционал вида $ Q(B, \tilde{S}) $. Этот функционал можно выбирать произвольным образом.

	Заметим, однако, что при данном подходе мы сталкиваемся с проблемой правильных оценок для элементов обучающей выборки,
	т.к. мы знаем только метки классов для них, а не оценки.

	В ходе выполнения данной работы, был выбран функционал качества
	\begin{equation}\label{quality functional for operator}
		Q(B, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel B(S_i) - c_i \parallel^2} \longrightarrow \underset{B}{\min},
	\end{equation}
	где $ c_i $ &mdash; оценки для объектов: бинарный вектор размерности $l$ с единсвтвенной единицей в позиции,
	соответсвующей классу, которму принадлежит объект.


<h1>Алгебраический подход &mdash; идея суперпозиции</h1>
	<p>
	Представим теперь наш алгоритм не в виде $ A = C \circ B$,
	а в виде $ A = C \circ F \circ B $, где $F: \mathfrak{I_e} \rightarrow \mathfrak{I_e}$ &mdash; корректирующая операция.
	</p>

	<img src="diagram1.png"  width="220">

	<p>
	Основная идея алгебраического подхода состоит в следующем.
	В общем случае выбранная нами модель операторов может не иметь оптимума,
	поэтому будем использовать алгебраическое расширение модели.
	Будем строить не один оператор, а несколько, и использовать их суперпозицию $A = C \circ F(B_1, \dots, B_p)$.
	</p>

	<img src="diagram2.png"  width="310">

	<p>
	Корректирующие операции $ F \in \mathfrak{f} $, где $ \mathfrak{f} = \mathfrak{f_1} \cup \mathfrak{f_2} $ &mdash; семейство корректирующих операций,
	состоящее из двух подсемейств, где $ \mathfrak{f_i} = \{G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e} | p \in \mathbb{N} \}, i = 1, 2 $.
	</p>

	На самом деле корректирующая операция есть некоторое другое отображение
	\begin{equation}
		F: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e}\}^p \rightarrow \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \}
	\end{equation}
	Но она индицируется операцией
	\begin{equation}
		G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e}
	\end{equation}
	если
	\begin{equation}
		F(B_1, \cdots, B_p)(S) = G(B_1(S), \cdots, B_p(S))
	\end{equation}

	<h1>Оптимизационный подход &mdash; для суперпозиций</h1>
	Выше мы ввели функционал качества для настройки одного оператора.
	\begin{equation}
		Q: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} \times (\mathfrak{I_i}, \mathfrak{I_e})^q \rightarrow \mathbb{R}
	\end{equation}
	Т.к. $ F(\cdot) \in \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} $, то мы можем в тот же функционал подставить вместо одного оператора суперпозицию.
	\begin{equation}
		Q(F(B_1, \cdots, B_p), \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel F(B_1(S_i), \cdots, B_p(S_i)) - c_i \parallel^2}
	\end{equation}
	Таким образом, получаем оптимизационную задачу:
	\begin{equation}\label{quality functional for superposition}
		Q(F(B_1, \cdots, B_p), \tilde{S}) \longrightarrow \underset{p, B_1, \cdots, B_p, F}{\min}
	\end{equation}


	<h1>Итерационный процесс построения суперпозиции</h1>
	Будем последовательно добавлять операторы в суперпозицию.

	<UL>
		<LI> $B_1 = \underset{B}{argmin} Q(B, \tilde{S})$
		<LI> $(F, B_1, B_2) = \underset{F, B_1, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		Решать эту задачу слишком сложно, поэтому будем использовать неэквивалентное упрощение.
		В качестве первого оператора будем использовать оператор полученный на предыдущем шаге. Т.е.
		$(F, B_1, B_2) = \underset{F, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		<LI> $(F, B_1, B_2, B_3) = \underset{F, B_3}{argmin} Q(F(B_1, B_2, B_3), \tilde{S})$
		$\cdots$
	</UL>
	Это внешний цикл настройки суперпозиции, который останавливается,
	когда $|Q_{new} - Q_{old}| < \varepsilon $, где $Q_{new}$ и $Q_{old}$ &mdash; значение функционала,
	достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.

	При каждой итерации внешнего цикла надо решать задачу оптимизации
	$(F, B_1,\cdots, B_p) = \underset{F, B_p}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$.
	Для этого входим во внутренний цикл, который будет решать эту задачу методом покоординатного спуска(опять используем неэквивалентное упрощение).

	<OL>
		<LI>$F^0$ &mdash; задаем некоторое начальное приближение для корректирующей операции
		<LI>$B_p^i = \underset{B_p}{argmin} Q(F^{i-1}(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$
		<LI>$F^i = \underset{F}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p^i), \tilde{S})$
	</OL>

	 Проводим итерации цикла 1 - 2 до того, как $|Q'_{new} - Q'_{old}| < \delta $, где $Q'_{new}$ и $Q'_{old}$ &mdash; значение функционала,
	 достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.

	 Для реализации изложенного итерационного подхода необходимо уметь выполнять следующие действия:
	 <UL>
	 	<LI> $B^* = \underset{B}{argmin}Q(B, \tilde{S})$;
	 	<LI> $F^*_p \mapsto F^0_{p+1}$;
	 	<LI> $B^* = \underset{B}{argmin}Q(F(B_1, \cdots, B_{p-1}, B), \tilde{S})$;
	 	<LI> $F^* = \underset{F}{argmin}Q(F(B_1, \cdots, B_p), \tilde{S})$
	 	<LI> критерий останова для внутреннего цикла $|Q'_{new} - Q'_{old}| < \delta $;
	 	<LI> критерий останова для цикла наращивания p $|Q_{new} - Q_{old}| < \varepsilon $;
	 </UL>
		Для добавления сразу двух операторов:
	<UL>
	 	<LI> $F^*_p \mapsto F^0_{p+2}$;
	 	<LI> $(B^*_{p+1}, B^*_{p+2}) = \underset{B', B''}{argmin}Q(F(B_1, \cdots, B_p, B', B''), \tilde{S})$;
	</UL>
		Для перенастройки ранее добавленного оператора:
	<UL>
	 	<LI> $B^*_i = \underset{B}{argmin}Q(F(B_1, \cdots, B_{i-1}, B, B_{i+1}, \cdots, B_p), \tilde{S})$.
	 </UL>

	<h1>Типы параметров по отношению к оптимизационному подходу</h1>
		Параметры можно разделить на две группы: оптимизируемые и неоптимизируемые.
		Неоптимизируемые параметры либо фиксируются, либо настраиваются по выборке (с помощью какой-либо эвристики).
		Оптимизируемые параметры могут быть либо найдены аналитиески, либо с помощью численной процедуры оптимизации.

	<h1>Модель операторов K ближайших соседей (kNN)</h1>

		<h2>Вербальное описание модели</h2>
		Основным принципом метода ближайших соседей является то, что объект присваивается тому классу,
		который является наиболее распространённым среди соседей данного элемента.

		Этот метод опирается на гипотезу компактности:
		предположение о том,
		что мера сходства объектов выбрана достаточно удачно,
		и схожие объекты скорее лежат в одном классе, чем в разных.
		В этом случае граница между классами имеет достаточно простую форму,
		а классы образуют компактно локализованные области в пространстве объектов.

		<h2>Формальное описание модели</h2>

		Пусть на пространстве объектов $\mathfrak{I}_i$
		задана метрика $\rho(S_i,S_j), S_i, S_j \in \mathfrak{I}_i$,
		определяющая сходство объектов.
		Для классифицируемого объекта $S$ упорядочим
		обучающую выборку по убыванию значения $\rho(S, S_i)$:
		\[
			\rho(S,S_s^{(1)}) \leqslant \rho(S,S_s^{(2)}) \leqslant \dots \leqslant \rho(S,S_s^{(q)})
		\],
		где через $S_s^{(i)}$ обозначается $i$-й сосед объекта $S$ из выборки $\tilde S$,
		$q$ &mdash; мощность обучающей выборки: $|\tilde S| = q$.
		В наиболее общем виде алгоритм ближайших соседей есть
		\[
			A(S) = \arg\max\limits_{j \in \{1,\dots,l\}} \Gamma_j(S), \;
			\Gamma_j = \sum_{i=1}^q [K_s^{(i)} = K_j] \omega (i,S),
		\]
		где $K_s^{(i)}$ &mdash; известный ответ на объекте $S_s^{(i)}$,
		$l$ &mdash; число классов,
		$\omega(i,S)$ &mdash; весовая функция,
		равная в случае $k$ ближайших соседей $\omega(i,S) = [i \leqslant k]$.

		Мы решаем задачу классификации,
		используя алгебраический подход,
		в рамках которого алгоритмический оператор $B \in \mathfrak{M}^0$ действует
		из пространства начальных информаций $\mathfrak{I}_i$
		в пространство оценок $\mathfrak{I}_e$.
		В этом случае оператор kNN
		возвращает оценки принадлежностей классифицируемого объекта классам
		и при классификации на $l$ классов имеет вид
		\[
			B(S) = (e_1, \dots, e_l), \; e_j = \frac{\Gamma_j(S)}{\sum_{m=0}^l e_m} =
			\frac{\sum_{i=1}^q [K_s^{(i)} = K_j][i \leqslant k]}{\sum_{m=0}^l e_m} =
			\frac{\sum_{i=1}^k [K_s^{(i)} = K_j]}{\sum_{m=0}^l e_m}.
		\]
		В рассматриваемой задаче пространство $\mathfrak{I}_i$ есть $\mathbb{R}^2$,
		поэтому в качестве $\rho(S_i,S_j)$ выбрана
		евклидова метрика $\rho(S_i,S_j) = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$,
		где $S_i = (x_i,y_i), S_j = (x_j,y_j)$.


		<h2>Параметры модели </h2>

		<table>
		<caption></caption>
		<tr>
		<th>Название</th>
		<th>Обозначение</th>
		<th>Область значений</th>
		<th>Тип</th>
		</tr>
		<tr>
		<td>Обучающая выборка</td>
		<td>$\widetilde{S} = (x,y)$ </td>
		<td>$\mathbb{R}^2$</td>
		<td>Настраиваемый (процедурно-получаемый)</td>
		</tr>
		<tr>
		<td>Число соседей</td>
		<td>$k$</td>
		<td>$\mathbb{N}$  </td>
		<td>Настраиваемый (численно-оптимизируемый)</td>
		</tr>
		<tr>
		<td>Метрика</td>
		<td>$\rho$ </td>
		<td>$\mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}_+$</td>
		<td>Фиксированный</td>
		</tr>
		<tr>
		<td>Весовая функция </td>
		<td>$\omega$  </td>
		<td>$\mathbb{Z}_+ \times \mathbb{R}^2 \to \mathbb{R}_+$</td>
		<td>Фиксированный</td>
		</tr>
		</table>


		<h2>Настройка (выбор оператора из модели)</h2>

		Настроить модель оператора &mdash;
		значит присвоить значение каждому из его параметров.
		В рассматриваемой модели настраиваемым параметром
		является число соседей $k$.
		Поиск оптимального значения параметра $k$ происходит
		в результате минимизации функционала потерь
		$Q(B,\widetilde S) = \Vert B(\widetilde S) - C^{-1}(r(\widetilde S)) \Vert^2$,
		который имеет следующий вид:
		\[
			Q(k,\widetilde S) = \sum_{i=1}^q {\Vert e_i - K_i \Vert^2_{\mathbb{R}^l}} =
			\sum_{i=1}^q \sum_{j=1}^l {(e_{ij} - K_{ij})}^2 \to \min_k,
		\]
		где
		$e_{ij} = \frac{\sum_{t\neq i} [r(S_t) = K_j][t \leqslant k]}{\sum_{m=1}^l e_{im}}$ &mdash;
		оценка принадлежности $i$-ого объекта $K_j$-ому классу,
		$K_i$ &mdash; $l$-мерный вектор из 0 и 1: $K_{ij} = [r(S_i) = K_j]$,
		то есть координата равна 1, тогда и только тогда,
		когда $i$-ый объект принадлежит $j$-ому классу.
		При минимизации данного функционала метод оптимизации
		будет стремиться увеличить оценку за правильный класс
		и уменьшить оценку за неверный класс,
		чтобы минимизировать норму разности.

		<h3>Настройка оператора</h3>

		Так как минимизация производится по целочисленному параметру $k$,
		принимающему значения из отрезка $1,\dots,q$,
		где $q$ &mdash; длина выборки,
		то используется метод полного перебора.
		Начальная инициализация $k = 1$:
		подсчитывается функционал потерь $Q(1,\widetilde S)$.
		Далее $k$ увеличивается итерационно с шагом $1$ до тех пор,
		пока $Q(i,\widetilde S) \leqslant Q(i-1, \widetilde S)$.
		Если функционал $Q$ не уменьшается на протяжении 5 итераций,
		параметр $k$, найденный ранее, считается оптимальным.

		<h2>Исследование модели алгоритмических операторов</h2>

		<h3>Пример задачи, для которой в модели есть корректный алгоритм</h3>

		Если классы делятся на локализованные области, для каждой из которых объекты этой области существенно ближе друг для друга, чем объекты других областей,
		 то с помощью метода $k$ ближайших соседей можно получить корректный алгоритм.

		Пример:

		<img border="0" src="knn_good.png" alt="kNN" width="410" height="410">

		<h3>Пример задачи, для которой в модели нет корректного оператора</h3>

		Существует множество случаев, когда $kNN$ не работает, например, когда один из классов гораздо более разрежен, чем другой.

		Пример:

		<img border="0" src="knn_bad.png" alt="kNN" width="410" height="410">

	<h1>Модель операторов. Алгоритмы вычисления оценок (АВО)</h1>
		<h2>Вербальное описание модели</h2>
		Ниже перечислены основные принципы АВО:

		<ul>
		<li> Решение о классификации объекта принимается с помощью
				анализа оценок близости объекта к классам.
				За какой класс оценка близости выше &mdash;
				к тому классу и относят объект.
				Оценки вычисляет <i>распознающий оператор</i>.
				Классифицирует объекты на основе оценок
				их близостей к классам <i>решающее правило</i>.
		</li>
		<li>    При вычислении оценок близости к классам учитывают
				близость/дальность объекта к эталонам.
				Близость &mdash; схожесть описаний,
				малое расстояние между значениями признаков.
				При этом оценка близости объекта к классу тем выше,
				чем ближе он к эталонным объектам данного класса
				и дальше от эталонных объектов других классов.
		</li>
		<li>
				Близость распознаваемого объекта $S$ к эталонному $S'$
				определяется на основе расстояний $\rho_i(S, S'), i = 1,\dots, n$,
				и формализуется понятием <i>функция близости</i>.
		</li>
		</ul>

		<h2>Формальное описание модели</h2>

		Пусть $M_1, \dots, M_n$ &mdash; некоторые множества
		с введенными на них функциями
		\[
			\rho_i : M_t \times M_t \to R_+,
			\rho_i (x, y) = \rho_i (y, x),
		\]
		признаки $f_i \in M_i$.
		$\Omega \neq \emptyset, \Omega \subseteq \{1, \dots, n\}$ &mdash;
		<i>опорное множество</i>.
		$\Omega^*$ &mdash; <i>система опорных множеств</i>:

		\[
			\Omega^* \neq \emptyset, \Omega^* \subseteq 2^{\{1,\dots,n\}} \setminus \{\emptyset\}.
		\]
		$B_\Omega (S_i, S)$ &mdash; <i>функция близости</i>,
		где $\{S_i\}_{i=1}^q$ &mdash; эталонные объекты.

		Распознающий оператор $B$ модели АВО
		для всех контрольных объектов $S^t, t=1,\dots, m$
		и классов $K_j, j = 1, \dots, l$ вычисляет
		<i>оценку принадлежности объекта</i> $S^t$ к классу $K_j$
		по формуле:
		\[
			\Gamma_{tj}[B] = \sum\limits_{a,b = 0,0}^{1,1} x_{ab}(j)
							 \sum\limits_{\Omega \in \Omega^*}
							 \sum\limits_{S_i \in \widetilde{K_j^a}}
							 w_i w(\Omega) B_{\Omega, \tilde \varepsilon}^{b}(S_i, S^t),
		\]
		где $w_i \in R_+$ &mdash; вес <i>i-ого эталонного объекта</i>,
		$w(\Omega) \in R_+$ &mdash; <i>вес опорного множества $\Omega$</i>,
		$B_{\Omega, \tilde \varepsilon}^{b}(S_i, S^t)$ &mdash; <i>функция близости</i>, такая что
		\[
			B_{\Omega,  \tilde \varepsilon}^{0}(S_i, S^t) = 1 - B_{\Omega, \tilde \varepsilon}(S_i, S^t),
			B_{\Omega,  \tilde \varepsilon}^{1}(S_i, S^t) = B_{\Omega, \tilde \varepsilon}(S_i, S^t),
		\]
		а $\tilde \varepsilon = (\varepsilon_1, \dots, \varepsilon_n)$ &mdash;
		некоторые пороговые значения для каждого признака,
		смысл которых будет объяснен дальше.
		$\widetilde{K_j^a}$ &mdash; отношение объектов $K_j$-ого класса и эталонов:
		\[
			\widetilde{K_j^1} = \widetilde S \cap \widetilde{K_j},
			\widetilde{K_j^0} = \widetilde S \setminus \widetilde{K_j},
		\]
		где $\widetilde{K_j} = \{S \in \widetilde S \mid r(S) = K_j\}$.

		Заметим, что положительный параметр $x_{11}$
		вносит в формулу слагаемое,
		которое «поощряет» близость объекта $S^t$ к $j$-ому классу,
		положительный параметр $x_{00}$ «поощряет»
		удаленность от объектов других классов.
		Аналогично, отрицательный параметр $x_{10}$
		«наказывает» за удаленность от объектов $j$-ого класса,
		а $x_{01}$ &mdash; за близость к объектам других классов.

		<br>
		В рамках задания по практикуму рассматриваются оценки
		одного объекта вне зависимости от остальных, поэтому
		алгоритмический оператор $B(S)$ примет вид:
		\[
			B(S) = (e_1, \dots, e_l), \; e_j = \frac{\Gamma_j(S)}{\sum_{m=0}^l e_m} =
			\frac{\sum\limits_{a,b = 0,0}^{1,1} x_{a,b}(j) \sum\limits_{\Omega \in \Omega^*}
			\sum\limits_{S_i \in \widetilde{K_j^a}} w_i w(\Omega) B_{\Omega, \tilde \varepsilon}^{b}(S_i,S)} {\sum\limits_{m=0}^l e_m}.
		\]


		<h2>Перечисление параметров модели операторов</h2>
		Рассмотрим модель со следующими упрощениями:
		<ul>

			<li> $x_{00}, x_{11} \in \{0,1\} \,
				   x_{01}, x_{10} \in \{0, -1\} \,
				   x_{ab} = x_{ab}(j)$ для всех
				   $j \in \{1, \dots, l\} , a,b \in \{0,1\}$;
			</li><li> для всех объектов $S_i$ веса $w_i = 1$;
			</li><li> для всех признаков их веса $w(\omega) = 1$,
				  для всех опорных множеств их веса равны
				  сумме весовых коэффициентов входящих в них признаков, то есть
				  $w(\Omega) = \sum\limits_{\omega \in \Omega} w(\omega) = |\Omega|$;
			</li><li> функция близости имеет вид:
				\[
					B_{\Omega, \tilde \varepsilon}(S_i, S) =
					\begin{cases}
						1, & \text{если $\forall \omega \in \Omega (\rho(f_\omega(S_i), f_\omega(S)) \leqslant \varepsilon_w)$} \\
						0, & \text{иначе;}
					\end{cases}
				\]
			</li><li> в качестве системы опорных множеств $\Omega^*$
				  возьмем все одноэлементные подмножества, то есть
				  $\Omega^* = \{\Omega \in 2^{\{1, \dots, n\}}\setminus\{\emptyset\}\mid |\Omega| = 1\}$.
				  Для данной системы опорных множеств
				  существуют эффективные формулы вычисления оценок.
				  Для параметра $x_{11}$ они имеют вид:
				  \[
						\Gamma_j(S) =
						\begin{cases}
							0, & \text{$|\Upsilon_i| < k$, } \\
							\sum\limits_{S_i \in \widetilde{K_j}} w_i (\sum\limits_{\omega \in \Upsilon_i} w(\Omega)) C_{|\Upsilon_i|-1}^{k-1}, & \text{иначе,}
						\end{cases}
				  \]
				  где $\Upsilon_i = \{\omega \mid \rho_{\omega}(S_i, S) \leqslant \varepsilon_{\omega}\}$.

				  Т.к. $w_i = 1 \, \forall i , \, k = 1, \, w(\omega)=1$, то
				  \[
						\Gamma_j(S) =
						\begin{cases}
							0, & \text{$|\Upsilon_i| = 0$, } \\
							\sum\limits_{S_i \in \widetilde{K_j}}|\Upsilon_i|, & \text{иначе.}
						\end{cases}
				  \]
				  Аналогичные оценки получаются для параметров $x_{00}, x_{01}, x_{10}$.
				  </li>
		</ul>

		 <br>
		 Таким образом таблица параметров имеет следующий вид:
		 <table border="1">
		<tr>
		<th>Названия параметра</th>
		<th>Область значений</th>
		<th>Настройка</th>
		</tr>
		<tr>
		<td>Эталоны</td>
		<td>$\widetilde{S} = (x,y) \in R^2$</td>
		<td>Настраиваемый (процедурно-получаемый)</td>
		</tr>
		<tr>
		<td>Метрика</td>
		<td>$\rho : R^2 \times R^2 \to R_+$ </td>
		<td>Фиксированный</td>
		</tr>
		<tr>
		<td>Веса эталонов</td>
		<td>$w_i \in R_+$ </td>
		<td>Фиксированный</td>
		</tr>
		<tr>
		<td>Веса опорных множеств</td>
		<td>$w(\Omega) \in R_+$  </td>
		<td>Фиксированный</td>
		</tr>
		<tr>
		<td>Опорные множества </td>
		<td>$\Omega \in \Omega^*$  </td>
		<td>Настраиваемый (процедурно-получаемый)</td>
		</tr>
		<tr>
		<td>Функция близости   </td>
		<td>$B_{\Omega, \tilde \varepsilon}(S',S)$    </td>
		<td>Настраиваемый (численно-оптимизируемый)</td>
		</tr>
		</table>

		<h2>Настройка параметров</h2>
		Настраиваемыми параметрами
		являются пороговые значения $\widetilde \varepsilon$ в функции близости.
		Поиск оптимального значения параметров происходит
		в результате минимизации функционала потерь
		$Q(B,\widetilde S) = \Vert B(\widetilde S) - C^{-1}(r(\widetilde S)) \Vert^2$,
		который имеет следующий вид:
		\[
			Q(k,\widetilde S) = \sum_{i=1}^q {\Vert e_i - K_i \Vert^2_{R^l}} =
			\sum_{i=1}^q \sum_{j=1}^l {(e_{ij} - K_{ij})}^2 \to \min\limits_{\widetilde\varepsilon},
		\]
		где
		$e_{ij} = \frac{\sum_{t \neq i} \Gamma_j(S_t)}{\sum_{m=1}^l e_{im}}$ &mdash;
		оценка принадлежности $i$-ого объекта $K_j$-ому классу,
		$K_i$ &mdash; $l$-мерный вектор из 0 и 1: $K_{ij} = [r(S_i) = K_j]$,
		то есть координата равна 1, тогда и только тогда,
		когда $i$-ый объект принадлежит $j$-ому классу.
		При минимизации данного функционала метод оптимизации
		будет стремиться увеличить оценку за правильный класс
		и уменьшить оценку за неверный класс,
		чтобы минимизировать норму разности.

		<h2>Исследование модели алгоритмических операторов</h2>

		<h3>Пример задачи, для которой в модели есть корректный алгоритм</h3>

		Если классы делятся на локализованные области, для каждой из которых объекты этой области существенно ближе друг для друга, чем объекты других областей,
		 то с помощью АВО можно получить корректный алгоритм.

		Пример:

		<img border="0" src="avo_good.png" alt="AVO" width="410" height="410">

		<h3>Пример задачи, для которой в модели нет корректного оператора</h3>

		Существует множество случаев, когда АВО не работает &mdash; например, когда один из классов гораздо более разрежен, чем другой.

		Пример:

		<img border="0" src="avo_bad.png" alt="AVO" width="410" height="410">

	<h1>Семейство корректирующих операций. Независимый оптимум</h1>
		<h2>Вербальное описание</h2>
		Идея корректирующей операции заключается в том,
		что для каждого объекта она выбирает из всех алгоритмических моделей ту,
		которую считает лучше остальных.
		<h2>Формальное описание</h2>
		Сравнивать алгоритмические модели корректирующая операция может только по оценкам:
		$$ \vec{\Gamma} = \underset{\Gamma_j}{\operatorname{argmax}} U(\Gamma_j) $$
		<h2>Использование модели</h2>
		Результат работы корректирующего оператора полностью определен оценками алгоримических моделей
		и своими параметрами (параметрами функционала качества). Корректирующая операция на вход
		имеет $p$ векторов оценок. С помощью функционала качества $U$ получаются оценки каждого вектора.
		Затем возвращается тот вектор оценок который имеет наибольший результат.

		<h2>Перечисление параметров модели операторов</h2>

		В таблице приведены параметры модели:
		<br>
		<br>

		<center>

		<table border="1">
		<caption></caption>
		<tr>
		<th>Название параметра</th>
		<th>Обозначение параметра</th>
		<th>Область допустимых значений</th>
		<th>Тип параметра</th>
		</tr>
		<tr>
		<td><center>Функционал качества для оценок</center></td>
		<td><center>$U$</center></td>
		<td><center>$\mathbb{R}^l \rightarrow \mathbb{R}$</center></td>
		<td>Фиксированный</td>
		</tr>
		</table>
		</center>
		<br>
		<h2>Настройка параметров</h2>
		Зафиксируем функционал $U : \mathbb{R}^l \rightarrow \mathbb{R}$.
		В рамках практической работы будем использовать функционал,
		который оценивает уверенность каждого алгоритмического оператора в ответе.
		Формально это можно записать в следующем виде
		$$ U(\Gamma) = \max_{i \in [1, l]} \Gamma_i - \max_{i \in [1, l] \backslash  \underset{j \in [1, l]}{\operatorname{argmax}} \Gamma_j } \Gamma_j$$
		Другими словами уверенность алгоритмической операции на объекте
		оценивается как разность между максимальной оценкой за класс и второй максимальной оценкой.

	<h1>Семейство корректирующих операций. Комитет старшинства с индивидуальными классами.</h1>

		<h2>Вербальное описание идеи</h2>
		Идея комитета старшинства с индивидуальными классами заключается в том, что алгоритмические операции
		упорядочиваются списком. После этого, по порядку списка мы перебираем результаты алгоритмических операторов, пока не выполниться критерий останова.
		Если критерий не выполнился, и мы дошли до конца, то выбирается результат последнего алгоритмического оператора.

		<h2>Описание семейства</h2>
		Корректирующие операторы из семейства комитета старшинаства основываеются на упорядочивание
		алгоритмических операторов и последовательном выборе результата согласно порядку следования.
		Этот способ работы корректирующей операции вводится в литературе под разными названиями: комитет с логикой
		старшинства, решающий список правил, машина покрывающих множеств. Последнее название относится скорее к
		корректирующией операции комитета старшинства с наборами классов.


		<h2>Использование корректирующей операции</h2>

		Пусть настраиваются суперпозиция алгоритмических операторов $B_1,\dots,B_p$ с
		векторами оценок $\Gamma_1, \dots, \Gamma_p$.  Корректирующая операция в общем случае выглядит так:

		\begin{equation}
			\begin{split}
				 &F(\vec {\Gamma_1},\dots,\vec {\Gamma_p}) = (F_1(\Gamma_{1_1},\dots,\Gamma_{p_1}), \cdots, F_l(\Gamma_{1_1},\dots,\Gamma_{p_1}))^T\\
			\end{split}
			\end{equation}

		Тогда каждому аргументу $\Gamma_r, 1 \leq r < p$
		назначается класс $z_r$. Перебираем аргументы последовательно. Если на очередном шаге $r$ в векторе
		$\Gamma_r$ доминирует компонента $z_r$, то перебор прекращается и возвращается $\Gamma_r$.
		Иначе переходим к следующей оценке. Если достигли последнего аргумента $p$, то
		возвращаем $\Gamma_p$. Эта корректирующая операция относится к классу выбирающих,
		также ее называют решающим списком.

		<br>
		<h2>Перечисление параметров комитета старшинства с индивидуальными классами</h2>
		<br>
		<br>

		<center>

		<table>
		<caption></caption>
		<tr>
		<th>Название параметра</th>
		<th>Обозначение параметра</th>
		<th>Область допустимых значений</th>
		<th>Тип параметра</th>
		</tr>
		<tr>
		<td><center>Классы комитета</center></td>
		<td><center>$Z = \{z_1, \dots, z_{p-1}\}$</center></td>
		<td><center>$\{1,\dots,l\}^{p-1}$</center></td>
		<td>Численно оптимизируемый</td>
		</tr>
		</table>
		</center>
		<br>

		<h2>Настройка параметров</h2>
		Так как размер строящейся суперпозиции небольшой, то оптимизации классов
		для комитета старшинства проводится с помощью полного перебора всех возможных классов,
		соотвествующих каждому алгоритмическому оператору. Выбирается та совокупность классов

		<br>
		<br>

		<center>
		$z_1,\dots, z_{p-1}$
		</center>
		<br>
		для которых достигается минимум функционала корректирующей операции на обучающей выборке.

	<h1>Заключение</h1>
		Во время выполнения данного задания мы на практике применяли
		алгебраический подход при решении задачи классификации.
		Были построены сцперпозиции алгоритмических операторов с
		корректирующими операциями над ними и решающего правила.
		Параметры алгоритма искались с помощью минимизации
		функционала качества на обучающей выборке, которая в свою
		очередь была достигнута путем минимизаций более простых функционалов,
		что заметно упрощает поиск лучших параметров для суперпозиции.
		По итогам экспериментов мы получили подтверждение того, что
		композиция алгоритмических операторов дает лучшие по сравнению
		с каждым из операторов в отдельности результаты.

	<h1>Список литературы</h1>
	<ol>
	<li> Рудаков К. В. &mdash; Алгебраическая теория универсальных и локальных ограничений для алгоритмов распознавания.
	</ol>

{% endblock report %}

<!-- vim: set ft=htmldjango si sw=2 : -->
