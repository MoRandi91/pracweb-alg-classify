{% extends "reports_base.html" %}
{% block author %}Березин Алексей{% endblock %}
{% block topic %}Настройка суперпозиции алгебраических операторов с помощью корректирующих операций{% endblock %}

{% block report %}
    <h1>Введение</h1>
	
	В данном отчете рассмотривается алгебраический подход применительно к задаче классификации.
	Основная идея подхода заключается в построении  алгоритма хорошего кчества на основе множества
	алгоритмических операторов путем их объединения в алгоритмическую композицию при помощи корректрирующей операции. 
	
	Задача разделяется на два шага: сначала осуществляется переход в удобное нам вспомогательное пространство оценок,
	а из него уже в итоговое пространство.
	Также в этом подходе возникает возможность рассматривать различные суперпозиции 
	алгоритмических операторов. Это расширяет семейство алгоритмов и может привести к повышению качества
	получаемого решения.
	В данном задании приводится реализация данного подхода на практике,
	рассматриваем несколько моделей операторов и корректирующих семейств,
	позволяющие получить хорошие алгоритмы классификации.
	
	<h1>Задача классификации</h1>
	<h2>Общая постановка</h2>
	На неформальном языке задача классификации заключается в некотором "разумном" отнесении объектов к классам. 
	Будем рассматривать $q$ объектов и $l$ классов. Задача классификации -- задача обучения с учителем.
	Поэтому рассматривается также обучающая выборка: множество $m$ объектов, для которых известны принадлежности к классам.
	
	Опишем общую постановку задачи формально. Пусть:
	<UL>
	<LI> $I_{ob}$ &mdash; пространство описаний объектов. Дескриптивная функция D сопоставляет каждому объекту
	его признаковое описание. Например, $D(s) = \|\rho_t(s, s^k)\|_{mn}$, где s &mdash; рассмтриваемый объект, 
	$s^k$ &mdash; объект обучающей выборки $k = 1, \dots m$, $\rho_t$ &mdash; некоторая метрика, $t = 1, \dots n$. 
	Tогда $I_{ob} = \mathfrak{C_{mn}}(\mathbb{R}_{+})$.
	<LI> $I_{cl}$&mdash; пространство описаний классов. Например, описанием j-го класса может быть вектор 
	$(P_j(s^1), \dots P_j(s^m)$, где $P_j)$ &mdash; предикат, говорящий о принадлежности объектов данному классу.
	Тогда $I_{cl} = E_2^m$.
	<LI>$\mathfrak{I_i}$ &mdash; пространство начальных информаций. Это пространство матриц размерности $q$ на $l$:
	$\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{I})}$,
	где $\mathfrak{I}$ &mdash; пространство совместных описаний объектов и классов, т.е. $\mathfrak{I} = I_{ob} \times I_{cl}$.
	<LI>$\mathfrak{I_f}$ &mdash; пространство финальных информаций. Это пространство матриц той же размрности: $\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{\tilde{I}})}$,
	где $\mathfrak{\tilde{I}} = E_2$, т.е. каждый элемент матрицы говорит о принадлежности соответсвующего объекта соответсвующему классу; 
	<LI>$I^u$ &mdash; универсальные ограничения. Эти ограничения не проверяются констрактивно, а задаются как некоторый "мешок" отображений,
	например, непрерывные отображения, монотонные, или в общем случае &mdash; отображения некоторой категории.
	<LI>$I^l$ &mdash; локальные ограничения. Эти ограничения задаются конструктивно. Например, это требование принимать истинные значения на прецедентах.
	Такое требование называется корректностью алгоритма. Заметим, что в данном задании мы не будем вводить такое требование.
	<LI>$\mathfrak{m}$ &mdash; модель алгоритмов, или семейство алгоритмов, удовлетворяющее определенным локальным и глобальным ограничениям.
	$\mathfrak{m:*}$ &mdash; семейство всех возможных алгоримтов. 
	</UL>
	
	Задача заключается в построении алгоритма (отображения) $A$, действующего из пространства начальных информация в пространство финальных информация
	и удовлетворяющих глобальным и локальным ограничениям:
	<p>
	\[
	A \in \mathfrak{m}^*: \mathfrak{I_i} \rightarrow \mathfrak{I_f}, \quad	A \in \mathfrak{m}^*[I^u], A \in \mathfrak{m}^*[I^l]
	\]
	</p>
	
	<h2>Конкретизация постановки в условиях практикума</h2>
	Перейдем от общей остановки задачи к непостредственно той, которую будем рассматривать в рамках данного задания.
	
	<UL>
	<LI> Объектами являются точки на плоскости. Пространство описания объектов $I_{ob} = \mathbb{R}^2$.
	
	<LI> Универсальные ограничения $I^u$: однородность объектов (порядок объектов неважен для алгоритма) и независимость объектов
	(алгоритм осущеcтвляет классификацию каждого отдельного объекта независимо).
	Следующим универсальным ограничением является принадлежность объекта только к одному классу.
	Из этого огранчения следует, что классы не являются независимыми.
	Однако классы являются в нашем случае однородными: алгоритм не различает классы и относится одинаково ко всем.
	Кроме того, мы будем рассматривать алгоритмы, принадлежащие только определенному семейству.
	Далее в отчете мы подробно опишем оба рассматриваемых семейства.
	
	<LI> Из независимости объектов следует, что можно рассматривать задачу классификации отдельно для каждого объекта.
	Кроме того, договоримся рассматривать информацию о прецедентах и классах как параметры алгоритма.
	Тогда можно сказать, что пространство начальных информаций $\mathfrak{I_i} = \mathbb{R}^2$.
	
	<LI> Из условия о принадлежности только одному классу следует, что можно задать результат классификации объекта
	меткой класса: числом из множества $\{1, \dots l\}$. 
	Так как рассматривается алгоритм для классификации отдельного объекта, получаем пространство финальных информаций 
	$\mathfrak{I_f} = \{1, \dots l\}$.
	</UL>
	

	<h1>Оптимизационный подход &mdash; для алгоритмов</h1>
	В поставленной задаче мы не требуем условия корректности алгоритма,
	т.к. поиск корректного алгоритма часто является неразрешимой задачей, а также может приводить к переобучению.
	Поэтому необходим иной способ учета обучающей информации и выбора алгоритма из семейства.
	При оптимизационном подходе вводится некоторый функционал качества и выбирается алгоритм, на котором достигается минимум:
	\[
	Q(A, \tilde{S}) \longrightarrow \min,
	\]
	где $\tilde{S}$ &mdash; обучающая выборка.
	Конкретный вид фукнционала может быть различным. Например, это может быть число ошибок алгоритма на обучающей выборке.
	Тогда мы будем выбирать корректный алгоритм (в случае, если такой алгоритм существует для данной выборки при остальных ограничениях задачи).
	В рамках этого задания мы не будем пользоваться функционалом вида $ Q(A, \tilde{S}) $. 
	В дальнейшем мы рассмотрим алгоритм $ A $ в виде композиции алгоритмического оператора и решающего правила и 
	будем решать оптимизационную задачу для оператора, а не для алгоритма.
	
	<h1>Алгебраический подход &mdash; идея декомпозиции</h1>
	<p>
	В общем случае работать с пространством начальных и финальных информаций неудобно, 
	поэтому в рамках алгебраического подхода решения задач классификации переходят в некоторое пространство, 
	называемое пространство оценок. Его выбирают произвольно, так, чтобы было удобно. 
	Для выполнения этого перехода в другое пространство алгоритм рассматривают как композицию алгоритмического оператора 
	и решающего правила $A = C \circ B$, 
	где $ B: \mathfrak{I_i} \rightarrow \mathfrak{I_e} $ &mdash; алгоритмический оператор, 
	$ C: \mathfrak{I_e} \rightarrow \mathfrak{I_f} $ &mdash; решающее правило. 
	Здесь $\mathfrak{I_e}$ &mdash; пространство оценок. В данном задании выберем $\mathfrak{I_e} = \mathbb{R}^l$.
	</p>
	
	<img src="diagram0.png"  width="200">
	
	<p>
	В рамках данного задания $ B \in \mathfrak{m_0} $, 
	где $ \mathfrak{m_0} = \mathfrak{m_{0_1}} \cup \mathfrak{m_{0_2}} $ &mdash; некоторое семейство операторов &mdash; объединение двух семейств,
	конкретный вид которых будет рассмотрен далее. А решающее правило зафиксируем как $ C = argmax $.
	</p>
	
	<h1>Оптимизационный подход &mdash; для операторов</h1>
	Будем искать не оптимальный алгоритм, а оптимальный оператор, 
	зафиксировав при этом решающее правило, 
	т.е. введем функционал вида $ Q(B, \tilde{S}) $. Этот функционал можно выбирать произвольным образом.
	
	Заметим, однако, что при данном подходе мы сталкиваемся с проблемой правильных оценок для элементов обучающей выборки, 
	т.к. мы знаем только метки классов для них, а не оценки.
	
	В ходе выполнения данной работы, был выбран функционал качества
	\begin{equation}\label{quality functional for operator}
		Q(B, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel B(S_i) - c_i \parallel^2} \longrightarrow \underset{B}{\min},	
	\end{equation}
	где $ c_i $ &mdash; оценки для объектов: бинарный вектор размерности $l$ с единсвтвенной единицей в позиции,
	соответсвующей классу, которму принадлежит объект.
	
	
<h1>Алгебраический подход &mdash; идея суперпозиции</h1>
	<p>
	Представим теперь наш алгоритм не в виде $ A = C \circ B$, 
	а в виде $ A = C \circ F \circ B $, где $F: \mathfrak{I_e} \rightarrow \mathfrak{I_e}$ &mdash; корректирующая операция.
	</p>
	
	<img src="diagram1.png"  width="220">
	
	<p>
	Основная идея алгебраического подхода состоит в следующем. 
	В общем случае выбранная нами модель операторов может не иметь оптимума, 
	поэтому будем использовать алгебраическое расширение модели. 
	Будем строить не один оператор, а несколько, и использовать их суперпозицию $A = C \circ F(B_1, \dots, B_p)$.
	</p>
	
	<img src="diagram2.png"  width="310">
	
	<p>
	Корректирующие операции $ F \in \mathfrak{f} $, где $ \mathfrak{f} = \mathfrak{f_1} \cup \mathfrak{f_2} $ &mdash; семейство корректирующих операций, 
	состоящее из двух подсемейств, где $ \mathfrak{f_i} = \{G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e} | p \in \mathbb{N} \}, i = 1, 2 $.
	</p>
	
	На самом деле корректирующая операция есть некоторое другое отображение
	\begin{equation}
		F: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e}\}^p \rightarrow \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \}
	\end{equation}
	Но она индицируется операцией
	\begin{equation}
		G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e}
	\end{equation}
	если
	\begin{equation}
		F(B_1, \cdots, B_p)(S) = G(B_1(S), \cdots, B_p(S))
	\end{equation}
	
	<h1>Оптимизационный подход &mdash; для суперпозиций</h1>
	Выше мы ввели функционал качества для настройки одного оператора.
	\begin{equation}
		Q: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} \times (\mathfrak{I_i}, \mathfrak{I_e})^q \rightarrow \mathbb{R}
	\end{equation}
	Т.к. $ F(\cdot) \in \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} $, то мы можем в тот же функционал подставить вместо одного оператора суперпозицию.
	\begin{equation}
		Q(F(B_1, \cdots, B_p), \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel F(B_1(S_i), \cdots, B_p(S_i)) - c_i \parallel^2}
	\end{equation}
	Таким образом, получаем оптимизационную задачу:
	\begin{equation}\label{quality functional for superposition}
		Q(F(B_1, \cdots, B_p), \tilde{S}) \longrightarrow \underset{p, B_1, \cdots, B_p, F}{\min}
	\end{equation}
	
		
	<h1>Итерационный процесс построения суперпозиции</h1>
	Будем последовательно добавлять операторы в суперпозицию.
	
	<UL>
		<LI> $B_1 = \underset{B}{argmin} Q(B, \tilde{S})$
		<LI> $(F, B_1, B_2) = \underset{F, B_1, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		Решать эту задачу слишком сложно, поэтому будем использовать неэквивалентное упрощение. 
		В качестве первого оператора будем использовать оператор полученный на предыдущем шаге. Т.е.
		$(F, B_1, B_2) = \underset{F, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		<LI> $(F, B_1, B_2, B_3) = \underset{F, B_3}{argmin} Q(F(B_1, B_2, B_3), \tilde{S})$
		$\cdots$
	</UL>
	Это внешний цикл настройки суперпозиции, который останавливается, 
	когда $|Q_{new} - Q_{old}| < \varepsilon $, где $Q_{new}$ и $Q_{old}$ &mdash; значение функционала, 
	достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	При каждой итерации внешнего цикла надо решать задачу оптимизации 
	$(F, B_1,\cdots, B_p) = \underset{F, B_p}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$. 
	Для этого входим во внутренний цикл, который будет решать эту задачу методом покоординатного спуска(опять используем неэквивалентное упрощение).
	
	<OL>
		<LI>$F^0$ &mdash; задаем некоторое начальное приближение для корректирующей операции
		<LI>$B_p^i = \underset{B_p}{argmin} Q(F^{i-1}(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$
		<LI>$F^i = \underset{F}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p^i), \tilde{S})$
	</OL>
	
	 Проводим итерации цикла 1 - 2 до того, как $|Q'_{new} - Q'_{old}| < \delta $, где $Q'_{new}$ и $Q'_{old}$ &mdash; значение функционала, 
	 достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	 Для реализации изложенного итерационного подхода необходимо уметь выполнять следующие действия:
	 <UL>
	 	<LI> $B^* = \underset{B}{argmin}Q(B, \tilde{S})$;
	 	<LI> $F^*_p \mapsto F^0_{p+1}$;
	 	<LI> $B^* = \underset{B}{argmin}Q(F(B_1, \cdots, B_{p-1}, B), \tilde{S})$;
	 	<LI> $F^* = \underset{F}{argmin}Q(F(B_1, \cdots, B_p), \tilde{S})$
	 	<LI> критерий останова для внутреннего цикла $|Q'_{new} - Q'_{old}| < \delta $;
	 	<LI> критерий останова для цикла наращивания p $|Q_{new} - Q_{old}| < \varepsilon $;
	 </UL>	
		Для добавления сразу двух операторов:
	<UL>	
	 	<LI> $F^*_p \mapsto F^0_{p+2}$;
	 	<LI> $(B^*_{p+1}, B^*_{p+2}) = \underset{B', B''}{argmin}Q(F(B_1, \cdots, B_p, B', B''), \tilde{S})$;
	</UL>
		Для перенастройки ранее добавленного оператора:
	<UL>
	 	<LI> $B^*_i = \underset{B}{argmin}Q(F(B_1, \cdots, B_{i-1}, B, B_{i+1}, \cdots, B_p), \tilde{S})$.
	 </UL>
	
	<h1>Типы параметров по отношению к оптимизационному подходу</h1>
	Параметры можно разделить на две группы: оптимизируемые и неоптимизируемые.
	Неоптимизируемые параметры либо фиксируются, либо настраиваются по выборке (с помощью какой-либо эвристики).
	Оптимизируемые параметры могут быть либо найдены аналитиески, либо с помощью численной процедуры оптимизации.	
	

<h1>Модель операторов. Алгоритмы вычисления оценок (АВО)</h1>
<h2>Вербальное описание модели</h2>
Ниже перечислены основные принципы АВО:

<ul>
<li> Решение о классификации объекта принимается с помощью
        анализа оценок близости объекта к классам.
        За какой класс оценка близости выше &mdash;
        к тому классу и относят объект.
        Оценки вычисляет <i>распознающий оператор</i>.
        Классифицирует объекты на основе оценок
        их близостей к классам <i>решающее правило</i>.
</li>
<li>    При вычислении оценок близости к классам учитывают
        близость/дальность объекта к эталонам.
        Близость &mdash; схожесть описаний,
        малое расстояние между значениями признаков.
        При этом оценка близости объекта к классу тем выше,
        чем ближе он к эталонным объектам данного класса
        и дальше от эталонных объектов других классов.
</li>
<li>
        Близость распознаваемого объекта $S$ к эталонному $S'$
        определяется на основе расстояний $\rho_i(S, S'), i = 1,\dots, n$,
        и формализуется понятием <i>функция близости</i>.
</li>
</ul>

<h2>Формальное описание модели</h2>

Пусть $M_1, \dots, M_n$ &mdash; некоторые множества
с введенными на них функциями
\[
    \rho_i : M_t \times M_t \to R_+,
    \rho_i (x, y) = \rho_i (y, x),
\]
признаки $f_i \in M_i$.
$\Omega \neq \emptyset, \Omega \subseteq \{1, \dots, n\}$ &mdash;
<i>опорное множество</i>.
$\Omega^*$ &mdash; <i>система опорных множеств</i>:

\[
    \Omega^* \neq \emptyset, \Omega^* \subseteq 2^{\{1,\dots,n\}} \setminus \{\emptyset\}.
\]
$B_\Omega (S_i, S)$ &mdash; <i>функция близости</i>,
где $\{S_i\}_{i=1}^q$ &mdash; эталонные объекты.

Распознающий оператор $B$ модели АВО
для всех контрольных объектов $S^t, t=1,\dots, m$
и классов $K_j, j = 1, \dots, l$ вычисляет
<i>оценку принадлежности объекта</i> $S^t$ к классу $K_j$
по формуле:
\[
    \Gamma_{tj}[B] = \sum\limits_{a,b = 0,0}^{1,1} x_{ab}(j)
                     \sum\limits_{\Omega \in \Omega^*}
                     \sum\limits_{S_i \in \widetilde{K_j^a}}
                     w_i w(\Omega) B_{\Omega, \tilde \varepsilon}^{b}(S_i, S^t),
\]
где $w_i \in R_+$ &mdash; вес <i>i-ого эталонного объекта</i>,
$w(\Omega) \in R_+$ &mdash; <i>вес опорного множества $\Omega$</i>,
$B_{\Omega, \tilde \varepsilon}^{b}(S_i, S^t)$ &mdash; <i>функция близости</i>, такая что
\[
    B_{\Omega,  \tilde \varepsilon}^{0}(S_i, S^t) = 1 - B_{\Omega, \tilde \varepsilon}(S_i, S^t),
    B_{\Omega,  \tilde \varepsilon}^{1}(S_i, S^t) = B_{\Omega, \tilde \varepsilon}(S_i, S^t),
\]
а $\tilde \varepsilon = (\varepsilon_1, \dots, \varepsilon_n)$ &mdash;
некоторые пороговые значения для каждого признака,
смысл которых будет объяснен дальше.
$\widetilde{K_j^a}$ &mdash; отношение объектов $K_j$-ого класса и эталонов:
\[
    \widetilde{K_j^1} = \widetilde S \cap \widetilde{K_j},
    \widetilde{K_j^0} = \widetilde S \setminus \widetilde{K_j},
\]
где $\widetilde{K_j} = \{S \in \widetilde S \mid r(S) = K_j\}$.

Заметим, что положительный параметр $x_{11}$
вносит в формулу слагаемое,
которое «поощряет» близость объекта $S^t$ к $j$-ому классу,
положительный параметр $x_{00}$ «поощряет»
удаленность от объектов других классов.
Аналогично, отрицательный параметр $x_{10}$
«наказывает» за удаленность от объектов $j$-ого класса,
а $x_{01}$ &mdash; за близость к объектам других классов.

<br>
В рамках задания по практикуму рассматриваются оценки
одного объекта вне зависимости от остальных, поэтому
алгоритмический оператор $B(S)$ примет вид:
\[
    B(S) = (e_1, \dots, e_l), \; e_j = \frac{\Gamma_j(S)}{\sum_{m=0}^l e_m} =
    \frac{\sum\limits_{a,b = 0,0}^{1,1} x_{a,b}(j) \sum\limits_{\Omega \in \Omega^*}
    \sum\limits_{S_i \in \widetilde{K_j^a}} w_i w(\Omega) B_{\Omega, \tilde \varepsilon}^{b}(S_i,S)} {\sum\limits_{m=0}^l e_m}.
\]


<h2>Перечисление параметров модели операторов</h2>
Рассмотрим модель со следующими упрощениями:
<ul>

    <li> $x_{00}=0, x_{11}=1 \,
           x_{01} = -1, x_{10} = 0 \,
           x_{ab} = x_{ab}(j)$ для всех
           $j \in \{1, \dots, l\} , a,b \in \{0,1\}$;
    </li><li> для всех объектов $S_i$ веса $w_i = 1$;
    </li><li> для всех признаков их веса $w(\omega) = 1$,
          для всех опорных множеств их веса равны
          сумме весовых коэффициентов входящих в них признаков, то есть
          $w(\Omega) = \sum\limits_{\omega \in \Omega} w(\omega) = |\Omega|$;
    </li><li> функция близости имеет вид:
     \[
            B_{\Omega, \tilde \varepsilon}(S_i, S) =
            \begin{cases}
                1, & \text{если $\forall \omega \in \Omega |\{S_t:(\rho(f_\omega(S_i), f_\omega(S)) \ge \rho(f_\omega(S_t), f_\omega(S)) \} | \leqslant \varepsilon_w)$} \\
                0, & \text{иначе;}
            \end{cases}
        \]
    </li><li> в качестве системы опорных множеств $\Omega^*$
          возьмем все множество $\Omega^*$, то есть опорное множество будет единственным
         <!-- $\Omega^* = \{\Omega \in 2^{\{1, \dots, n\}}\setminus\{\emptyset\}\mid |\Omega| = 1\}$.-->
<!--         
		 Для данной системы опорных множеств
          существуют эффективные формулы вычисления оценок.
          Для параметра $x_{11}$ они имеют вид:
          \[
                \Gamma_j(S) =
                \begin{cases}
                    0, & \text{$|\Upsilon_i| < k$, } \\
                    \sum\limits_{S_i \in \widetilde{K_j}} w_i (\sum\limits_{\omega \in \Upsilon_i} w(\Omega)) C_{|\Upsilon_i|-1}^{k-1}, & \text{иначе,}
                \end{cases}
          \]
          где $\Upsilon_i = \{\omega \mid \rho_{\omega}(S_i, S) \leqslant \varepsilon_{\omega}\}$.

          Т.к. $w_i = 1 \, \forall i , \, k = 1, \, w(\omega)=1$, то
          \[
                \Gamma_j(S) =
                \begin{cases}
                    0, & \text{$|\Upsilon_i| = 0$, } \\
                    \sum\limits_{S_i \in \widetilde{K_j}}|\Upsilon_i|, & \text{иначе.}
                \end{cases}
          \]
          Аналогичные оценки получаются для параметров $x_{00}, x_{01}, x_{10}$.
		  -->
		  </li>		  
</ul>

 <br>
 Таким образом таблица параметров имеет следующий вид:
 <table border="1">
<tr>
<th>Названия параметра</th>
<th>Область значений</th>
<th>Настройка</th>
</tr>
<tr>
<td>Эталоны</td>
<td>$\widetilde{S} = (x,y) \in R^2$</td>
<td>Настраиваемый (процедурно-получаемый)</td>
</tr>
<tr>
<td>Метрика</td>
<td>$\rho : R^2 \times R^2 \to R_+$ </td>
<td>Фиксированный</td>
</tr>
<tr>
<td>Веса эталонов</td>
<td>$w_i \in R_+$ </td>
<td>Фиксированный</td>
</tr>
<tr>
<td>Веса опорных множеств</td>
<td>$w(\Omega) \in R_+$  </td>
<td>Фиксированный</td>
</tr>
<tr>
<td>Опорные множества </td>
<td>$\Omega \in \Omega^*$  </td>
<td>Настраиваемый (процедурно-получаемый)</td>
</tr>
<tr>
<td>Функция близости   </td>
<td>$B_{\Omega, \tilde \varepsilon}(S',S)$    </td>
<td>Настраиваемый (численно-оптимизируемый)</td>
</tr>
</table>

<h2>Настройка параметров</h2>
Настраиваемыми параметрами
являются пороговые значения $\widetilde \varepsilon$ в функции близости.
Поиск оптимального значения параметров происходит
в результате минимизации функционала потерь
$Q(B,\widetilde S) = \Vert B(\widetilde S) - C^{-1}(r(\widetilde S)) \Vert^2$,
который имеет следующий вид:
\[
    Q(k,\widetilde S) = \sum_{i=1}^q {\Vert e_i - K_i \Vert^2_{R^l}} =
    \sum_{i=1}^q \sum_{j=1}^l {(e_{ij} - K_{ij})}^2 \to \min\limits_{\widetilde\varepsilon},
\]
где
$e_{ij} = \frac{\sum_{t \neq i} \Gamma_j(S_t)}{\sum_{m=1}^l e_{im}}$ &mdash;
оценка принадлежности $i$-ого объекта $K_j$-ому классу,
$K_i$ &mdash; $l$-мерный вектор из 0 и 1: $K_{ij} = [r(S_i) = K_j]$,
то есть координата равна 1, тогда и только тогда,
когда $i$-ый объект принадлежит $j$-ому классу.
При минимизации данного функционала метод оптимизации
будет стремиться увеличить оценку за правильный класс
и уменьшить оценку за неверный класс,
чтобы минимизировать норму разности.
	
	<h2>Исследование модели алгоритмических операторов</h2>
		<h3>Пример задачи, для которой в модели есть корректный оператор</h3>
		<embed src="avo_good_ber.svg" type="image/svg+xml"  width="500" height="410">
		<h3>Пример задачи, для которой в модели нет корректного оператора</h3>
	   <p>В данном примере логически понятно, что разделяющая поверхность должна быть расположена так же, как и в предыдущем примере.</p>
	   <p>
		<embed src="avo_bad_ber.svg" type="image/svg+xml" width="500" height="410">
	   <p>
	


<h1>Модель алгоритмов: Решающие деревья</h1>
<h2>Вербальное описание модели</h2>
<p>
Модель представляет собой одно одноуровневое решающее дерево, состоящее из корня, в котором находится условие на признаки объектов и двух листов, в которых находятся оценки принадлежности объектов к классам.
</p>
<h2>Формальное описание модели</h2>
<p>
Пусть $S \in \mathbb{R}^2$ &mdash; объект распознавания. В корне дерева располагается предикат вида $[x_i < d]$, где $x_i$ &mdash; $i$-ый признак объекта $S$, $d$ &mdash; некоторая константа. В листах дерева располагаются оценки принадлежности к классам. Так как решается задача классификации на $l$ классов, то в листах находятся $l$-мерные вектора. Будем обозначать вектора оценок, находящиеся в листах, как $v_{left}$, $v_{right}$ и будем считать, что их компоненты принадлежат отрезку $[0,1]$.  
</p>	
<h2>Использование модели</h2>	
<p>
Для объекта S и заданных параметров модели оценки вычисляются по следующей формуле:
\[
B(S) = v_{left}[x_i < d] + v_{right}[x_i \geq d], 
\]
где
<ul> $x_i$ &mdash; $i$-ый признак объекта S; описание объекта распознавания;
<li> $d$ &mdash; порог в предикате; параметр оператора;
<li> $v_{left}, v_{right}$ &mdash; оценки в листах; параметр оператора.
</ul>
Таким образом, если значение предиката для объекта $S$ в узле дерева истинно, то в качетсве значения оператора возвращается вектор в левом листе, если ложно, то значение в правом листе.
</p>	
<h2>Параметры модели</h2>
<table border="1">
<caption></caption>
<tr>
<th>Название</th>
<th>Обозначение</th>
<th>Область допустимых значений</th>
<th>Тип параметра по отношению к оптимизации</th>
</tr>
<tr>
<td>Номер признака в предикате</td>
<td>$i$</td>
<td>$\{1,2\}$</td>
<td>Оптимзируемый</td>
</tr>
<tr>
<td>Порог в предикате</td>
<td>$d$</td>
<td>$\mathbb{R}$</td>
<td>Оптимзируемый</td>
</tr>
<tr>
<td>Оценки в листах</td>
<td>$v_{left}$, $v_{right}$</td>
<td>$[0,1]^l$</td>
<td>Вычисляемый</td>
</tr>
</table>

<h2>Настройка модели</h2>
<h3>Вычисляемые параметры</h3>
<p>
Оценки в листах строятся следующим образом: значение $j$-ой компоненты вектора $v_{left}$ &mdash; это доля объектов $j$-ого класса, для которых значение предиката истинно, значение $j$-ой компоненты вектора $v_{right}$ &mdash; это доля объектов $j$-ого класса, для которых значение предиката ложно.
</p>
<h3>Оптимизируемые параметры</h3>
<p>
Для оптимизируемых параметров ставится следующая оптимизационная задача:
\[
(i^*, d^*) = \arg\min_{i \in \{1,2\}, d \in \mathbb{R}}{Q(B(i,d),S)}
\]

Так как $i \in \{1,2\}$, то для его оптимизации воспользуемся полным перебором - перебираем все возможные номера координат, которые могут быть в узле.

Параметр $d$ при фиксирванном параметре $i$ настраивается с помощью равномерного поиска. В качестве концов начального отрезка берутся минимальное и максимальное значения соответствующей координаты в выборке прецедентов. Такой метод оптимизации выбран, потому что кажется целесообразным покрыть плоскость равномерной сеткой и перебирать всевозможные значения $d$ &mdash; некоторая аналогия с полным перебором для непрерывного аргумента. Данный метод находит оптимум для унимодальных функций. В нашем случае, функционал не обязательно является унимодальным, соотвественно мы не всегда будет находить оптимиум. Однако данный метод применим для любого набора прецедентов и прост в реализации.
</p>

<h2>Исследование модели алгоритмических операторов</h2>
		<h3>Пример задачи, для которой в модели есть корректный оператор</h3>		
		<embed src="tree_good.svg" type="image/svg+xml"  width="500" height="410">
		<h3>Пример задачи, для которой в модели нет корректного оператора</h3>
		<p> Таким примером является классическая задача XOR.</p>
		 <p> <embed src="tree_bad.svg" type="image/svg+xml"  width="500" height="410"> </p>
		
<h1>Семейство корректирующих операций 1. Монотонные аффинные.</h1>

	<h2>Описание семейства</h2>

	В данной работе использовались монотонные аффинные корректирующие операции. 
	Такая КО представляет собой линейную комбинацию оценок алгоритмических операторов с неотрицательными весами. 
	Аффинность получаем путем добавления еще одного «псевдооператора» с вектором оценок, состоящим из единиц.

	<h2>Использование корректирующей операции</h2>

	Опишем способ использования монотонной аффинной корректирующей операции. 
	КО принимает на вход оценки $p$ алгоритмических операторов: $\vec{\Gamma}^1, \dots, \vec{\Gamma}^p$. 
	Параметрами являются $p+1$ неотрицательный вес: $a_0, a_1, \dots, a_p$. 
	Результатом корректирующей операции является оценка, подсчитываемая по следующей формуле: <br><br>

	<center>
		$\vec{\Gamma} = \alpha_0 + \alpha_1\vec{\Gamma}^1 + \ldots + \alpha_p\vec{\Gamma}^p$, &nbsp; где $\{\alpha_0, \dots, \alpha_p\ \in \mathbb{R}_+\}.$
	</center>	
	<br>

	Результат работы корректирующего оператора полностью опеределен оценками алгоритмических моделей и своими параметрами и принадлежит пространству оценок.

	<h2>Перечисление параметров модели операторов</h2>

	В таблице приведены параметры модели:<br><br>

	<center>
	<table border="1">
	<caption></caption>
	<tr>
	<th>Название параметра</th>
	<th>Обозначение параметра</th>
	<th>Область допустимых значений</th>
	<th>Тип параметра</th>
	</tr>
	<tr>
	<td><center>Арность оператора</center></td>
	<td><center>$p$</center></td>
	<td><center>$\mathbb{N}$</center></td>
	<td>Оптимизируемый</td>
	</tr>
	<tr>
	<td><center>Веса оценок</center></td>
	<td><center>$\vec{a}$</center></td>
	<td><center>$\mathbb{R}_+^{p+1}$</center></td>
	<td>Оптимизируемый</td>
	</tr>
	</table>
	</center>
	<br>

	<h2>Настройка параметров</h2>

	<h3>Настройка весов</h3>

	При фиксированной арности требуется минимизировать следующий функционал:
	$$Q(\vec{\alpha}, S) = \frac{1}{ql} \sum_{i = 1}^{q} \|  \alpha_0 + \alpha_1\vec{\Gamma}^1 + \ldots + \alpha_p\vec{\Gamma}^p - C^{-1}(y_i) \| ^2.$$

	Это задача условной непрерывной оптимизации по непрерывным параметрам. 
	Положим, что $\alpha_j \in \{0, 0.01, 0.02, 0.03, \dots, 1 \}$. 
	Таким образом, переходим к задаче оптимизации по дискретным параметрам.
	Она решается полным перебором по всем $\alpha_j$. Принимается конфигурация, на которой получается наименьшее значение оптимизируемого функционала.


<h1>Семейство корректирующих операций: Комитет старшинства с индивидуальными классами.</h1>

<h2>Вербальное описание идеи</h2>
Идея комитета старшинства с индивидуальными классами заключается в том, что алгоритмические операции 
упорядочиваются списком. После этого, по порядку списка мы перебираем результаты алгоритмических операторов, пока не выполниться критерий останова.
Если критерий не выполнился, и мы дошли до конца, то выбирается результат последнего алгоритмического оператора.

<h2>Описание семейства</h2>
Корректирующие операторы из семейства комитета старшинаства основываеются на упорядочивание 
алгоритмических операторов и последовательном выборе результата согласно порядку следования.
Этот способ работы корректирующей операции вводится в литературе под разными названиями: комитет с логикой
старшинства, решающий список правил, машина покрывающих множеств. Последнее название относится скорее к
корректирующией операции комитета старшинства с наборами классов.


<h2>Использование корректирующей операции</h2>

Пусть настраиваются суперпозиция алгоритмических операторов $B_1,\dots,B_p$ с 
векторами оценок $\Gamma_1, \dots, \Gamma_p$.  Корректирующая операция в общем случае выглядит так:

\begin{equation}
	\begin{split}
		 &F(\vec {\Gamma_1},\dots,\vec {\Gamma_p}) = (F_1(\Gamma_{1_1},\dots,\Gamma_{p_1}), \cdots, F_l(\Gamma_{1_1},\dots,\Gamma_{p_1}))^T\\
	\end{split}
	\end{equation}

Тогда каждому аргументу $\Gamma_r, 1 \leq r < p$ 
назначается класс $z_r$. Перебираем аргументы последовательно. Если на очередном шаге $r$ в векторе 
$\Gamma_r$ доминирует компонента $z_r$, то перебор прекращается и возвращается $\Gamma_r$. 
Иначе переходим к следующей оценке. Если достигли последнего аргумента $p$, то 
возвращаем $\Gamma_p$. Эта корректирующая операция относится к классу выбирающих, 
также ее называют решающим списком.

<br>
<h2>Перечисление параметров комитета старшинства с индивидуальными классами</h2>
<br>
<br>

<center>

<table border="1">
<caption></caption>
<tr>
<th>Название параметра</th>
<th>Обозначение параметра</th>
<th>Область допустимых значений</th>
<th>Тип параметра</th>
</tr>
<tr>
<td><center>Классы комитета</center></td>
<td><center>$Z = \{z_1, \dots, z_{p-1}\}$</center></td>
<td><center>$\{1,\dots,l\}^{p-1}$</center></td>
<td>Численно оптимизируемый</td>
</tr>
</table>
</center>
<br>

<h2>Настройка параметров</h2>
Так как размер строящейся суперпозиции небольшой, то оптимизации классов 
для комитета старшинства проводится с помощью полного перебора всех возможных классов, 
соотвествующих каждому алгоритмическому оператору. Выбирается та совокупность классов 

<br>
<br>

<center>
$z_1,\dots, z_{p-1}$ 
</center>
<br>
для которых достигается минимум функционала корректирующей операции на обучающей выборке.
{% endblock report %}
