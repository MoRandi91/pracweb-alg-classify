{% extends "reports_base.html" %}
{% block author %}Анна Потапенко{% endblock %}
{% block topic %}Настройка суперпозиции алгебраических операторов с помощью корректирующих операций{% endblock %}

{% block report %}
    <h1>Введение</h1>
	
	Одна из наиболее распространенных задач машинного обучения &mdash; задача классификации.
	Она состоит в распределении объектов по классам в результате обучения по прецедентам.
	В данной работе мы рассмотрим алгебраический подход к задаче классификации.
	Идея этого подхода простая и красивая. Трудно построить сразу нужный алгоритм, 
	действующий из пространства начальных информаций в пространство финальных информаций.
	Поэтому мы делаем это в два шага: сначала переходим в удобное нам вспомогательное пространство оценок,
	а из него уже в итоговое пространство.
	Также в этом подходе возникает возможность рассматривать различные суперпозиции 
	алгоритмических операторов. Это расширяет семейство алгоритмов и может привести к повышению качества
	получаемого решения.
	В данном задании мы применяем этот подход на практике,
	рассматриваем несколько моделей операторов и корректирующих семейств,
	их суперпозиции, делаем выводы о качестве работы построенных алгоритмов.
	
	<h1>Задача классификации</h1>
	<h2>Общая постановка</h2>
	На неформальном языке задача классификации заключается в некотором "разумном" отнесении объектов к классам. 
	Будем рассматривать $q$ объектов и $l$ классов. Задача классификации -- задача обучения с учителем.
	Поэтому рассматривается также обучающая выборка: множество $m$ объектов, для которых известны принадлежности к классам.
	
	Опишем общую постановку задачи формально. Пусть:
	<UL>
	<LI> $I_{ob}$ &mdash; пространство описаний объектов. Дескриптивная функция D сопоставляет каждому объекту
	его признаковое описание. Например, $D(s) = \|\rho_t(s, s^k)\|_{mn}$, где s &mdash; рассмтриваемый объект, 
	$s^k$ &mdash; объект обучающей выборки $k = 1, \dots m$, $\rho_t$ &mdash; некоторая метрика, $t = 1, \dots n$. 
	Tогда $I_{ob} = \mathfrak{C_{mn}}(\mathbb{R}_{+})$.
	<LI> $I_{cl}$&mdash; пространство описаний классов. Например, описанием j-го класса может быть вектор 
	$(P_j(s^1), \dots P_j(s^m)$, где $P_j)$ &mdash; предикат, говорящий о принадлежности объектов данному классу.
	Тогда $I_{cl} = E_2^m$.
	<LI>$\mathfrak{I_i}$ &mdash; пространство начальных информаций. Это пространство матриц размерности $q$ на $l$:
	$\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{I})}$,
	где $\mathfrak{I}$ &mdash; пространство совместных описаний объектов и классов, т.е. $\mathfrak{I} = I_{ob} \times I_{cl}$.
	<LI>$\mathfrak{I_f}$ &mdash; пространство финальных информаций. Это пространство матриц той же размрности: $\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{\tilde{I}})}$,
	где $\mathfrak{\tilde{I}} = E_2$, т.е. каждый элемент матрицы говорит о принадлежности соответсвующего объекта соответсвующему классу; 
	<LI>$I^u$ &mdash; универсальные ограничения. Эти ограничения не проверяются констрактивно, а задаются как некоторый "мешок" отображений,
	например, непрерывные отображения, монотонные, или в общем случае &mdash; отображения некоторой категории.
	<LI>$I^l$ &mdash; локальные ограничения. Эти ограничения задаются конструктивно. Например, это требование принимать истинные значения на прецедентах.
	Такое требование называется корректностью алгоритма. Заметим, что в данном задании мы не будем вводить такое требование.
	<LI>$\mathfrak{m}$ &mdash; модель алгоритмов, или семейство алгоритмов, удовлетворяющее определенным локальным и глобальным ограничениям.
	$\mathfrak{m:*}$ &mdash; семейство всех возможных алгоримтов. 
	</UL>
	
	Задача заключается в построении алгоритма (отображения) $A$, действующего из пространства начальных информация в пространство финальных информация
	и удовлетворяющих глобальным и локальным ограничениям:
	<p>
	\[
	A \in \mathfrak{m}^*: \mathfrak{I_i} \rightarrow \mathfrak{I_f}, \quad	A \in \mathfrak{m}^*[I^u], A \in \mathfrak{m}^*[I^l]
	\]
	</p>
	
	<h2>Конкретизация постановки в условиях практикума</h2>
	Перейдем от общей остановки задачи к непостредственно той, которую будем рассматривать в рамках данного задания.
	
	<UL>
	<LI> Объектами являются точки на плоскости. Пространство описания объектов $I_{ob} = \mathbb{R}^2$.
	
	<LI> Универсальные ограничения $I^u$: однородность объектов (порядок объектов неважен для алгоритма) и независимость объектов
	(алгоритм осущеcтвляет классификацию каждого отдельного объекта независимо).
	Следующим универсальным ограничением является принадлежность объекта только к одному классу.
	Из этого огранчения следует, что классы не являются независимыми.
	Однако классы являются в нашем случае однородными: алгоритм не различает классы и относится одинаково ко всем.
	Кроме того, мы будем рассматривать алгоритмы, принадлежащие только определенному семейству.
	Далее в отчете мы подробно опишем оба рассматриваемых семейства.
	
	<LI> Из независимости объектов следует, что можно рассматривать задачу классификации отдельно для каждого объекта.
	Кроме того, договоримся рассматривать информацию о прецедентах и классах как параметры алгоритма.
	Тогда можно сказать, что пространство начальных информаций $\mathfrak{I_i} = \mathbb{R}^2$.
	
	<LI> Из условия о принадлежности только одному классу следует, что можно задать результат классификации объекта
	меткой класса: числом из множества $\{1, \dots l\}$. 
	Так как рассматривается алгоритм для классификации отдельного объекта, получаем пространство финальных информаций 
	$\mathfrak{I_f} = \{1, \dots l\}$.
	</UL>
	

	<h1>Оптимизационный подход &mdash; для алгоритмов</h1>
	В поставленной задаче мы не требуем условия корректности алгоритма,
	т.к. поиск корректного алгоритма часто является неразрешимой задачей, а также может приводить к переобучению.
	Поэтому необходим иной способ учета обучающей информации и выбора алгоритма из семейства.
	При оптимизационном подходе вводится некоторый функционал качества и выбирается алгоритм, на котором достигается минимум:
	\[
	Q(A, \tilde{S}) \longrightarrow \min,
	\]
	где $\tilde{S}$ &mdash; обучающая выборка.
	Конкретный вид фукнционала может быть различным. Например, это может быть число ошибок алгоритма на обучающей выборке.
	Тогда мы будем выбирать корректный алгоритм (в случае, если такой алгоритм существует для данной выборки при остальных ограничениях задачи).
	В рамках этого задания мы не будем пользоваться функционалом вида $ Q(A, \tilde{S}) $. 
	В дальнейшем мы рассмотрим алгоритм $ A $ в виде композиции алгоритмического оператора и решающего правила и 
	будем решать оптимизационную задачу для оператора, а не для алгоритма.
	
	<h1>Алгебраический подход &mdash; идея декомпозиции</h1>
	<p>
	В общем случае работать с пространством начальных и финальных информаций неудобно, 
	поэтому в рамках алгебраического подхода решения задач классификации переходят в некоторое пространство, 
	называемое пространство оценок. Его выбирают произвольно, так, чтобы было удобно. 
	Для выполнения этого перехода в другое пространство алгоритм рассматривают как композицию алгоритмического оператора 
	и решающего правила $A = C \circ B$, 
	где $ B: \mathfrak{I_i} \rightarrow \mathfrak{I_e} $ &mdash; алгоритмический оператор, 
	$ C: \mathfrak{I_e} \rightarrow \mathfrak{I_f} $ &mdash; решающее правило. 
	Здесь $\mathfrak{I_e}$ &mdash; пространство оценок. В данном задании выберем $\mathfrak{I_e} = \mathbb{R}^l$.
	</p>
	
	<img src="diagram0.png"  width="200">
	
	<p>
	В рамках данного задания $ B \in \mathfrak{m_0} $, 
	где $ \mathfrak{m_0} = \mathfrak{m_{0_1}} \cup \mathfrak{m_{0_2}} $ &mdash; некоторое семейство операторов &mdash; объединение двух семейств,
	конкретный вид которых будет рассмотрен далее. А решающее правило зафиксируем как $ C = argmax $.
	</p>
	
	<h1>Оптимизационный подход &mdash; для операторов</h1>
	Будем искать не оптимальный алгоритм, а оптимальный оператор, 
	зафиксировав при этом решающее правило, 
	т.е. введем функционал вида $ Q(B, \tilde{S}) $. Этот функционал можно выбирать произвольным образом.
	
	Заметим, однако, что при данном подходе мы сталкиваемся с проблемой правильных оценок для элементов обучающей выборки, 
	т.к. мы знаем только метки классов для них, а не оценки.
	
	В ходе выполнения данной работы, был выбран функционал качества
	\begin{equation}\label{quality functional for operator}
		Q(B, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel B(S_i) - c_i \parallel^2} \longrightarrow \underset{B}{\min},	
	\end{equation}
	где $ c_i $ &mdash; оценки для объектов: бинарный вектор размерности $l$ с единсвтвенной единицей в позиции,
	соответсвующей классу, которму принадлежит объект.
	
	
<h1>Алгебраический подход &mdash; идея суперпозиции</h1>
	<p>
	Представим теперь наш алгоритм не в виде $ A = C \circ B$, 
	а в виде $ A = C \circ F \circ B $, где $F: \mathfrak{I_e} \rightarrow \mathfrak{I_e}$ &mdash; корректирующая операция.
	</p>
	
	<img src="diagram1.png"  width="220">
	
	<p>
	Основная идея алгебраического подхода состоит в следующем. 
	В общем случае выбранная нами модель операторов может не иметь оптимума, 
	поэтому будем использовать алгебраическое расширение модели. 
	Будем строить не один оператор, а несколько, и использовать их суперпозицию $A = C \circ F(B_1, \dots, B_p)$.
	</p>
	
	<img src="diagram2.png"  width="310">
	
	<p>
	Корректирующие операции $ F \in \mathfrak{f} $, где $ \mathfrak{f} = \mathfrak{f_1} \cup \mathfrak{f_2} $ &mdash; семейство корректирующих операций, 
	состоящее из двух подсемейств, где $ \mathfrak{f_i} = \{G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e} | p \in \mathbb{N} \}, i = 1, 2 $.
	</p>
	
	На самом деле корректирующая операция есть некоторое другое отображение
	\begin{equation}
		F: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e}\}^p \rightarrow \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \}
	\end{equation}
	Но она индицируется операцией
	\begin{equation}
		G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e}
	\end{equation}
	если
	\begin{equation}
		F(B_1, \cdots, B_p)(S) = G(B_1(S), \cdots, B_p(S))
	\end{equation}
	
	<h1>Оптимизационный подход &mdash; для суперпозиций</h1>
	Выше мы ввели функционал качества для настройки одного оператора.
	\begin{equation}
		Q: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} \times (\mathfrak{I_i}, \mathfrak{I_e})^q \rightarrow \mathbb{R}
	\end{equation}
	Т.к. $ F(\cdot) \in \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} $, то мы можем в тот же функционал подставить вместо одного оператора суперпозицию.
	\begin{equation}
		Q(F(B_1, \cdots, B_p), \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel F(B_1(S_i), \cdots, B_p(S_i)) - c_i \parallel^2}
	\end{equation}
	Таким образом, получаем оптимизационную задачу:
	\begin{equation}\label{quality functional for superposition}
		Q(F(B_1, \cdots, B_p), \tilde{S}) \longrightarrow \underset{p, B_1, \cdots, B_p, F}{\min}
	\end{equation}
	
		
	<h1>Итерационный процесс построения суперпозиции</h1>
	Будем последовательно добавлять операторы в суперпозицию.
	
	<UL>
		<LI> $B_1 = \underset{B}{argmin} Q(B, \tilde{S})$
		<LI> $(F, B_1, B_2) = \underset{F, B_1, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		Решать эту задачу слишком сложно, поэтому будем использовать неэквивалентное упрощение. 
		В качестве первого оператора будем использовать оператор полученный на предыдущем шаге. Т.е.
		$(F, B_1, B_2) = \underset{F, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		<LI> $(F, B_1, B_2, B_3) = \underset{F, B_3}{argmin} Q(F(B_1, B_2, B_3), \tilde{S})$
		$\cdots$
	</UL>
	Это внешний цикл настройки суперпозиции, который останавливается, 
	когда $|Q_{new} - Q_{old}| < \varepsilon $, где $Q_{new}$ и $Q_{old}$ &mdash; значение функционала, 
	достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	При каждой итерации внешнего цикла надо решать задачу оптимизации 
	$(F, B_1,\cdots, B_p) = \underset{F, B_p}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$. 
	Для этого входим во внутренний цикл, который будет решать эту задачу методом покоординатного спуска(опять используем неэквивалентное упрощение).
	
	<OL>
		<LI>$F^0$ &mdash; задаем некоторое начальное приближение для корректирующей операции
		<LI>$B_p^i = \underset{B_p}{argmin} Q(F^{i-1}(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$
		<LI>$F^i = \underset{F}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p^i), \tilde{S})$
	</OL>
	
	 Проводим итерации цикла 1 - 2 до того, как $|Q'_{new} - Q'_{old}| < \delta $, где $Q'_{new}$ и $Q'_{old}$ &mdash; значение функционала, 
	 достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	 Для реализации изложенного итерационного подхода необходимо уметь выполнять следующие действия:
	 <UL>
	 	<LI> $B^* = \underset{B}{argmin}Q(B, \tilde{S})$;
	 	<LI> $F^*_p \mapsto F^0_{p+1}$;
	 	<LI> $B^* = \underset{B}{argmin}Q(F(B_1, \cdots, B_{p-1}, B), \tilde{S})$;
	 	<LI> $F^* = \underset{F}{argmin}Q(F(B_1, \cdots, B_p), \tilde{S})$
	 	<LI> критерий останова для внутреннего цикла $|Q'_{new} - Q'_{old}| < \delta $;
	 	<LI> критерий останова для цикла наращивания p $|Q_{new} - Q_{old}| < \varepsilon $;
	 </UL>	
		Для добавления сразу двух операторов:
	<UL>	
	 	<LI> $F^*_p \mapsto F^0_{p+2}$;
	 	<LI> $(B^*_{p+1}, B^*_{p+2}) = \underset{B', B''}{argmin}Q(F(B_1, \cdots, B_p, B', B''), \tilde{S})$;
	</UL>
		Для перенастройки ранее добавленного оператора:
	<UL>
	 	<LI> $B^*_i = \underset{B}{argmin}Q(F(B_1, \cdots, B_{i-1}, B, B_{i+1}, \cdots, B_p), \tilde{S})$.
	 </UL>
	
	<h1>Типы параметров по отношению к оптимизационному подходу</h1>
	Параметры можно разделить на две группы: оптимизируемые и неоптимизируемые.
	Неоптимизируемые параметры либо фиксируются, либо настраиваются по выборке (с помощью какой-либо эвристики).
	Оптимизируемые параметры могут быть либо найдены аналитиески, либо с помощью численной процедуры оптимизации.	
	
<h1>Модель алгоритмов 1: Наивный байесовский классификатор</h1>
	
	<h2>Вербальное описание модели</h2>
	
	В основе Байесовской модели лежит преположение о том, 
	что $\mathfrak{S} \times \{ \textrm{Класс 1, Класс 2, ..., Класс l} \}$
	является вероятностным пространством. 
	Основываясь на этом предположении, производится максимизация апостериорной вероятности $\mathbb{P}(r_i | s_i)$ по всем объектам обучающей выборки.
	
	Далее вводится понятие штрафа за неправильный ответ алгоритма. $\lambda_{r, q}$ --
    величина штрафа за ответ алгоритма $q$, если правильный ответ $r$, где 
    $q, r \in \{ \textrm{Класс 1, Класс 2, ..., Класс l} \}$. 
	Предполагается, что $\lambda_{r, r} = 0$, а 
	$\lambda_{r, q} \equiv \lambda_r \forall q \in \{ \textrm{Класс 1, Класс 2, ..., Класс l} \} \backslash \{ r \} \}$.

	<<Наивный>> Байесовский классификатор дополнительно прдполагает, что признаки объектов независимы.
	
	<h2>Формальное описание действия оператора</h2>
		
	Выражая апостериорную вероятность по формуле Байеса и используя предположение о независимости признаков описания объектов, 
	получаем следующую формулу оценки для объекта:
	\begin{equation}
	\begin{split}
		&B(S_i) = ( \Gamma_1(S_i), \cdots, \Gamma_l(S_i))^T; \\
		&\Gamma_j(S_i) = \ln {\lambda_j \cdot \tilde{P_j} } + \ln {\tilde{p}_{j_x}( S_i( x ) )} + \ln { \tilde{p}_{j_y} ( S_i(y) )},
	\end{split}
	\end{equation}
		
	где
	<UL>
	<LI>$\Gamma_j$ -- $j$-ая компонента вектора ответа оператора $B$. Вспомогательная переменная.
	<LI>$\lambda_j$ -- величина штрафа при неправильной классификации объекста из Класса $j$. Параметр оператора.
	<LI>$\tilde{P}_j$ -- оценка априорной вероятности Класса $j$. Параметр оператора.
	<LI>$\tilde{p}_{j_x}$ -- оценка функции плотности координаты $x$ объектов в Классе $j$ $p(x | \textrm{Класс } j)$. Параметр оператора
	<LI>$S_i( x )$ -- координата $x$ объекта $S_i$. Вспомогательная переменная.
	<LI>$\tilde{p}_{j_y}$ -- оценка функции плотности координаты $y$ объектов в Классе $j$ $p(y | \textrm{Класс } j)$. Параметр оператора
	<LI>$S_i( y )$ -- координата $y$ объекта $S_i$. Вспомогательная переменная.
	</UL>
	
	Cпособ оценки вероятностей является параметром оператора. 
	В данной работе был выбран параметрический метод оценки вероятностей,
	согласно которому предполагается, что искомые вероятности принадлежат некоторому параметрическому семейству, 
	а параметры этого семейства являются параметрами оператора. 
	В данной работе в качестве семейства выберем семейство нормальных распределений с параметрами мат.ожидание и дисперсия. 
	Эти параметры оцениваются с помощью метода максимума правдоподобия.
	Оценкой априорной вероятности класса будем считать долю объектов обучающей выборки, принадлежащих этому классу.
	Учитывая сделанные предположения, полуаем:
	
	\begin{equation}
	\begin{split}
		&\tilde{p}_{j_x}(S_i(x)) = \frac{1}{\sqrt{2\pi\sigma_{j_x}^2}}\exp\left(  {-\frac{(S_i(x) - \mu_{j_x})^2}{2\sigma_{j_x}^2}}\right)  ; \\
		&\tilde{p}_{j_y}(S_i(x)) = \frac{1}{\sqrt{2\pi\sigma_{j_y}^2}}\exp\left( {-\frac{(S_i(y) - \mu_{j_y})^2}{2\sigma_{j_y}^2}}\right) ; \\
		&\tilde{P}_j = \sum_{r_i = \textrm{Класс }j}{1};
	\end{split}
	\end{equation}
	где
	<UL>
	<LI>$\mu_{j_x}$ -- оценка мат.ожидания для координаты $x$(выборочное мат.ожидание) для Класса $ j $. Параметр оператора.
	<LI>$\sigma_{j_x}$ -- оценка дисперсии для координаты $x$(выборочная дисперсия) для Класса $ j $. Параметр оператора.
	<LI>$\mu_{j_y}$ -- оценка мат.ожидания для координаты $y$(выборочное мат.ожидание) для Класса $ j $. Параметр оператора.
	<LI>$\sigma_{j_y}$ -- оценка дисперсии для координаты $y$(выборочная дисперсия) для Класса $ j $. Параметр оператора.
	</UL>
	
	<h2>Параметры оператора</h2>
	<table border="1">
	<tr>
	<th>Название</th>
	<th>Обозначение</th>
	<th>Область  значений</th>
	<th>Тип параметра</th>
	</tr>
	<tr>
	<td>Величина штрафа</td>
	<td>$\lambda_j$</td>
	<td>$\mathbb{R}$</td>
	<td>Оптимизируемый</td>
	</tr>
	<tr>
	<td>Оценка априорной вероятности</td>
	<td>$\tilde{P}_j$</td>
	<td>$\mathfrak{I_f} \rightarrow [0, 1]$</td>
	<td>Настраиваемый</td>
	</tr>
	<tr>
	<td>Оценка функции плотности $x$</td>
	<td>$\tilde{p}_{j_x}$</td>
	<td>$ \mathbb{R} \rightarrow \mathbb{R}_{+} $</td>
	<td>Настраиваемый</td>
	</tr>
	<tr>
	<td>Оценка функции плотности $y$</td>
	<td>$\tilde{p}_{j_y}$</td>
	<td>$ \mathbb{R} \rightarrow \mathbb{R}_{+} $</td>
	<td>Настраиваемый</td>
	</tr>
	<tr>
	<td>Оценка мат.ожидания $x$</td>
	<td>$\mu_{j_x}$</td>
	<td>$\mathbb{R}$ </td>
	<td>Настраиваемый</td>
	</tr>
	<tr>
	<td>Оценка мат.ожидания $y$</td>
	<td>$\mu_{j_y}$</td>
	<td>$\mathbb{R}$</td>
	<td>Настраиваемый</td>
	</tr>
    <tr>
	<td>Оценка дисперсии $x$</td>
	<td>$\sigma_{j_x}$</td>
	<td>$\mathbb{R}$</td>
	<td>Настраиваемый</td>
	</tr>
    <tr>
	<td>Оценка дисперсии $y$</td>
	<td>$\sigma_{j_y}$</td>
	<td>$\mathbb{R}$</td>
	<td>Настраиваемый</td>
	</tr>	
    </table>
    	
 
    <h2>Настройка (выбор оператора из модели)</h2>
   	Единственным оптимизируемым параметром является величина штрафа каждого класса $\lambda_j$. 
	Будем искать оптимальное значение параметра $\lambda_j$ с помощью минимизации расмотренного функционала. 
	Выпишем явный вид $ Q $ как функции от $(\lambda_1, \cdots, \lambda_l)$.
	\begin{equation}\label{quality functional for lambdas}
	\begin{split}
		Q(\lambda_1, \cdots, \lambda_l, \tilde{S}) &= \frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\Gamma_j(S_i) - c_i(j))^2} } = \\
		&= \frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\ln {\lambda_j \cdot \tilde{P_j} } + \ln {\tilde{p}_{j_x}( S_i( x ) )} + \ln { \tilde{p}_{j_y} ( S_i(y) ))} - c_i(j))^2} } = \\
		&= \frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\ln {\lambda_j} - M_i)^2} } \longrightarrow \underset{\lambda_1, \cdots, \lambda_l}{\min},
	\end{split}
	\end{equation}
	
	где $ M_i $ -- некоторые константы, не зависящие от $ \lambda_j $. Заметим, что каждую $ \lambda_j $ можно рассматривать отдельно, таким образом, 
	получаем $ l $ задач оптимизации вида:
	\begin{equation}
		Q(\lambda_j, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {(\ln {\lambda_j} - M_i)^2} \longrightarrow \underset{\lambda_j}{\min}
	\end{equation}
	
	Далее сделаем переобозначение: $ \Lambda_j = \ln {\lambda_j} $, и получим оптимизационную задачу:
	\begin{equation}\label{quality functional for Lambda}
		Q(\Lambda_j, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {(\Lambda_j - M_i)^2} \longrightarrow \underset{\Lambda_j}{\min}
	\end{equation}
	Данная оптимизационная задача относится к классу квадратичных задач, ее можно решать методом наименьших квадратов. 
	Решение выписывается следующим образом:
	\begin{equation}
	\begin{split}
		&\lambda_j = \exp{\frac{\sum_{i = 1}^q {M_i}}{q}}, \\
		&M_i = c_i(j) - \ln{\tilde{P}_j} - \ln{\tilde{p}_{j_x}(S_i(x))} - \ln{\tilde{p}_{j_y}(S_i(y))}
	\end{split}
	\end{equation}
	
	Вычисляемые параметры находятся путем максимизации правдоподобия обучающей выборки, формулы элементарно выводятся.
	
	
	<h2>Исследование модели алгоритмических операторов</h2>
		<h3>Пример задачи, для которой в модели есть корректный оператор</h3>
		<img border="0" src="bayes_good.png" alt="good for bayes" width="410" height="410">
		<h3>Пример задачи, для которой в модели нет корректного оператора</h3>
		<img border="0" src="bayes_bad.png" alt="good for bayes" width="410" height="410">
	

<h1>Модель алгоритмов 2: Метод логарифмических шаров</h1>
		
	<h2>Вербальное описание модели</h2>

		Метод логарифмческих шаров является одним из методов сравнения с эталоном.
		Вводится метрика, и предполагается, что близкие объекты принадлежат одному и тому же классу.
		Для метода логарифмических шаров фиксируется евклидова метрика.
		Вводится понятие эталонов, и расстояние до класса измеряется как растояние до эталона.

	<h2>Формальное описание действия оператора</h2>
		\begin{equation}
		\begin{split}
			&B(S_i) = ( \Gamma_1(S_i), \cdots, \Gamma_l(S_i))^T;
			\\
			&\Gamma_j(S_i) = z_j \ln {\frac{\rho(S_i, S_{0_j})}{R_{0_j}}},
		\end{split}
		\end{equation}	
		где
		<ul>
			<li>$ \rho(\cdot, \cdot) $ -- метрика $ \mathfrak{I_i}^2 \rightarrow \mathbb{R}_{+} $.</li>
			<li>$ z_j \in \{ +1, -1\} $ -- метка близость-дальность.</li>
			<li>$ S_{0_j} $ -- эталон класса.</li>
			<li>$ R_{0_j} $ -- радиус класса.</li>
		</ul>

		В нашем случае:
		<ul>
			<li>$ \rho(\cdot, \cdot) $ -- Евклидова метрика;</li>
			<li>$ R_{0_j} \equiv R_0 - const \textrm{ } \forall j \in \{ \textrm{Класс 1, Класс 2, ..., Класс l} \} $;</li>
			<li>$ z_j \equiv -1 \textrm{ } \forall j \in \{ \textrm{Класс 1, Класс 2, ..., Класс l} \}$.</li>
		</ul>
		
			
	<h2>Параметры оператора</h2>

		<table border="1">
		<tr><td>Название </td><td> Обозначение   </td><td> Область значений  </td><td> Тип параметра</tr>
		<tr><td>Метрика	</td><td> $\rho$ </td><td> $ \mathfrak{I_i}^2 \rightarrow \mathbb{R}_{+} $ </td><td> Фиксированный</td></tr>
		<tr><td>Метка близость-дальность </td><td> $ z_j $ </td><td> $ \{ +1, -1 \} $   </td><td> Фиксированный</td></tr>
		<tr><td>Эталон класса </td><td> $ S_{0_j} $ </td><td> $ \mathfrak{I_i} $ </td><td> Оптимизируемый</td></tr>
		<tr><td>Радиус класса </td><td> $ R_{0_j} $ </td><td> $ \mathbb{R}_{+} $ </td><td> Фиксированный</td></tr>
		</table>

	<h2>Настройка (выбор оператора из модели)</h2>

		Оптимизируемыми параметрами являются эталоны классов $ S_{0_j} $. 
		Ищем оптимальные значения с помощью минимизации расмотренного функционала.
		Выпишем явный вид $ Q $ как функции от $(S_{0_1}, \cdots, S_{0_l})$.
		\begin{equation}
		\begin{split}
			Q(S_{0_1}, \cdots, S_{0_l}, \tilde{S}) &= \frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\Gamma_j(S_i) - c_i(j))^2} } = \\ 
			&=\frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(- \ln {\frac{\rho(S_i, S_{0_j})}{R_0}} - c_i(j))^2} } = \\
			&=\frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\ln {\rho(S_i, S_{0_j})} - \ln {R_0} + c_i(j))^2} } \longrightarrow \underset{S_{0_1}, \cdots, S_{0_l}}{\min}, 
		\end{split}
		\end{equation}
	
		Каждый эталон $ S_{0_j} $ можно рассматривать отдельно, 
		таким образом, получаем $ l $ задач оптимизации вида:
		\begin{equation}
			Q(S_{0_j}, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {(\ln {\rho(S_i, S_{0_j})} - \ln {R_0} + c_i(j))^2} \longrightarrow \underset{ S_{0_j} }{\min}
		\end{equation}
		
		Данная оптимизационная задача относится к классу квадратичных задач. 
		Воспользуемся методом Монте-Карло для её решения.
		
	<h2>Исследование модели алгоритмических операторов</h2>
		<h3>Пример задачи, для которой в модели есть корректный оператор</h3>
		<img border="0" src="shary_good.png" alt="good for shary" width="410" height="410">
		<h3>Пример задачи, для которой в модели нет корректного оператора</h3>
		<img border="0" src="shary_bad.png" alt="bad for shary" width="410" height="410">
		
	<h1>Семейство корректирующих операций 1: Линейные корректирующие операции</h1>
	
	<h2>Вербальное описание семейства</h2>
		Идея очень проста: если есть несколько различных оценок использовать в качестве суперпозиционной оценки их линейную комбинацию. 
		Монотонные линейные корректирующие операции подразумевают неотрицательность весов в линейной комбинации,
		мы же для упрощения семейства будем использовать веса из диапазон [0,1].
	
	<h2>Формальное описание действия операции</h2>
		Рассматриваемая корректирующая операция имеет вид:
		\begin{equation}
			 F(\vec {\Gamma_1},\dots,\vec {\Gamma_p}) = \alpha_1 \vec {\Gamma_1} + \dots + \alpha_p \vec{\Gamma_p},
		\end{equation}
		где веса $\alpha_j \in [0,1]$, $\Gamma_j$ -- исходные оценки.
		
		Заметим, что применением корректирующих операций из данного семейства к модели алгоритмов мы действительно его расширяем(по крайней мере не сужаем), 
		т.к. данное семейство содержит тождественную корректирующую операцию.

	<h2>Параметры операции</h2>
		<table border="1">
		<tr><td>Название </td><td> Обозначение   </td><td> Область значений  </td><td> Тип параметра</tr>
		<tr><td>Арность	</td><td> $p$ </td><td> $ \mathbb{N}$ </td><td> Фиксированный</td></tr>
		<tr><td>Коэффициенты </td><td> $\alpha_j$ </td><td> $[0,1]$  </td><td> Оптимизируемый</td></tr>
		</table>	
	
	<h2>Настройка корректирующей операции</h2>
		Оптимизируемыми параметрами являются коэффициенты $ \alpha_j $. 
		Поиск оптимальных значений для $ \alpha_j $ просходит c помощью минимизации расмотренного функционала. 
		Выпишем $ Q $ как фукнцию от $ \alpha_j $:
		\begin{equation}
			Q(\alpha_1, \cdots, \alpha_p, \tilde{S}) = \sum_{i = 1}^q {\parallel \alpha_1 B_1(S_i) + \dots + \alpha_p B_p(S_i) - c_i \parallel ^2}
		\end{equation}
		Данная оптимизационная задача является квадратичной. Воспользуемся методом Моне-Карло для её решения.
		
		
	<h1>Семейство корректирующих операций 2: Комитет большинства</h1>
		<h2>Вербальное описание семейства</h2>
		Идея этой операции тоже проста. Если есть несколько оценок, то они усредняются.
		Таким образом, выбирается тот класс, за который голосует больше операторов. 
		
		<h2>Формальное описание действия операции</h2>
			Представим оценку для объекта $\tilde{S}$ как $e^{\tilde{S}} = F(B_1(\tilde{S}), \dots, B_p(\tilde{S})) = (e^{\tilde{S}}_1, \dots, e^{\tilde{S}}_l) \in \mathfrak{I}_e$,
			где $(B_1, \dots, B_p) \in \left(\mathfrak{M}[\pi]\right)^p$ -- исходный набор операторов.
			Тогда действие операторо можно описать как:
			\[
				e^{\tilde{S}}_i = \mathbb{I}\left[\frac{1}{p}(B_1^i(\tilde{S}) + \dots +~B_p^i(\tilde{S})) > \alpha_i\right], i = 1 \dots l.
			\]			
			Для определения корректирующей операции надо задать набор порогов $\alpha_i$ для каждого класса.
		
			Заметим, что применением корректирующих операций из данного семейства к модели алгоритмов мы действительно его расширяем(по крайней мере не сужаем), 
			т.к. данное семейство $\left(\mathfrak{M}[\pi]\right)^p \to \mathfrak{I}$ содержит тождественную корректирующую операцию.
		
		<h2>Параметры операции</h2>
			<table  border="1">
				<tr>
					<td>Название</td>
					<td>Обозначение</td>
					<td>Область значений</td>
					<td>Тип</td>
				</tr>
				<tr>
					<td>Число операторов</td>
					<td>$p$</td>
					<td>$\mathbb{Z}_{+}$</td>
					<td>Настраиваемый</td>
				</tr>
				<tr>
					<td>Операторы</td>
					<td>$(B_1, \dots, B_p)$</td>
					<td>$(\mathfrak{M}[\pi])^p$</td>
					<td>Настраиваемый</td>
				</tr>
				<tr>
					<td>Пороги</td>
					<td>$\alpha_i, i=1,\dots,l$</td>
					<td>[-1, 1]</td>
					<td>Оптимизируемый</td>
				</tr>
			</table>
		
		Ограничение на значения $\alpha_i$ связано с тем, что функция активации принимает значения от $-1$ до $1$.
		
		<h2>Настройка корректирующей операции</h2>
			Оптимизируемыми параметрами являются коэффициенты $ \alpha_i, i=1,\dots,l$. 
			Оптимальные значения $ \alpha_i, i=1,\dots,l$ будем искать с помощью минимизации рассмотренного функционала. 
			Выпишем $ Q $ как фукнцию от $ \alpha_k $:
			\begin{equation}
				Q(\alpha_1, \cdots, \alpha_l, \tilde{S}) = \sum_{j = 1}^q {\sum_{i = 1}^l { \mathbb{I}\left[\sum_{k = 1}^p B_i^k (S_j) > \alpha_i\right] - c_j(i) } } \longrightarrow \underset{\alpha_i}{\min}
			\end{equation} 
			
			Данную оптимизационную задачу будем решать с помощью метода Монте-Карло.
			
	
	<h1>Расширение модели 1 семейством 1 </h1>
		<h2>Задача, решаемая корректно как в рамках модели, так и в рамках расширенной модели</h2>
		<h2>Задача, не решаемая корректно в рамках модели, но решаемая в рамках расширенной модели</h2>
		<h2>Задача, не решаемая корректно ни в рамках модели, ни в рамках расширенной модели</h2>
		
	<h1>Расширение модели 2 семейством 2 </h1>
		<h2>Задача, решаемая корректно как в рамках модели, так и в рамках расширенной модели</h2>
		<h2>Задача, не решаемая корректно в рамках модели, но решаемая в рамках расширенной модели</h2>
		<h2>Задача, не решаемая корректно ни в рамках модели, ни в рамках расширенной модели</h2>
		
	<h1>Расширение моделей 1 и 2 семействами 1 и 2 </h1>
		<h2>Задача, решаемая корректно как в рамках моделей, так и в рамках расширенных моделей</h2>
		<h2>Задача, не решаемая корректно в рамках моделей, но решаемая в рамках расширенных моделей</h2>
		<h2>Задача, не решаемая корректно ни в рамках моделей, ни в рамках расширенных моделей</h2>
	
	<h1>Заключение</h1>
		В ходе выполнения данной практической работы был применён алгебраи-
		ческий подход к решению задачи классификации. Была построена суперпо-
		зиция алгоритмических операторов, корректирующей операции над ними и
		решающего правила. Поиск параметров алгоритма производился в ходе реше-
		ния задачи минимизации функцинала качества на обучающей выборке. При
		решении задача минимизации по нескольким параметрам была разделена на
		несколько подзадач минимизации более простых функционалов. Это суще-
		ственно упростило поиск оптимальных параметров суперпозиции. В резуль-
		тате экспериментов было подтверждено, что композиция алгоритмических
		операторов может давать лучшие результаты, чем каждый из операторов по
		отдельности.
		
	<h1>Список литературы</h1>
	<OL>
	<LI> Рудаков К. В. -- Алгебраическая теория универсальных и локальных ограничений для алгоритмов распознавания.
	<LI> Сайт про html: http://htmlbook.ru/
	</OL>
{% endblock report %}

<!-- vim: set ft=htmldjango si sw=2 : -->
