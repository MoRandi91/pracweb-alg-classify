<html>
<head>
  <meta charset="windows-125" />
  <title>Любимцева Мария</title>
	<script src="http://code.jquery.com/jquery-1.9.1.min.js"></script>

	<script src='tableofcontents.js'></script>
	<link rel='stylesheet' href='tableofcontents.css' />
	<script>
	$(document).ready(function()
	{
		$('#toc').tableofcontents(); //'#toc' - toc is id of div where to render 
	});
	</script>

	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		tex2jax: {
			inlineMath: [ ['$','$'], ["\\(","\\)"] ],
			processEscapes: true
		}
	});
	</script>
	
</head>
 
<body>

	<div id='toc'></div>

	<h1>Введение</h1>
	
	Одна из наиболее распространенных задач машинного обучения &mdash; задача классификации.
	Она состоит в распределении объектов по классам в результате обучения по прецедентам.
	В данной работе мы рассмотрим алгебраический подход к задаче классификации.
	Идея этого подхода простая и красивая. Трудно построить сразу нужный алгоритм, 
	действующий из пространства начальных информаций в пространство финальных информаций.
	Поэтому мы делаем это в два шага: сначала переходим в удобное нам вспомогательное пространство оценок,
	а из него уже в итоговое пространство.
	Также в этом подходе возникает возможность рассматривать различные суперпозиции 
	алгоритмических операторов. Это расширяет семейство алгоритмов и может привести к повышению качества
	получаемого решения.
	В данном задании мы применяем этот подход на практике,
	рассматриваем несколько моделей операторов и корректирующих семейств,
	их суперпозиции, делаем выводы о качестве работы построенных алгоритмов.
	
	<h1>Задача классификации</h1>
	<h2>Общая постановка</h2>
	На неформальном языке задача классификации заключается в некотором "разумном" отнесении объектов к классам. 
	Будем рассматривать $q$ объектов и $l$ классов. Задача классификации -- задача обучения с учителем.
	Поэтому рассматривается также обучающая выборка: множество $m$ объектов, для которых известны принадлежности к классам.
	
	Опишем общую постановку задачи формально. Пусть:
	<UL>
	<LI> $I_{ob}$ &mdash; пространство описаний объектов. Дескриптивная функция D сопоставляет каждому объекту
	его признаковое описание. Например, $D(s) = \|\rho_t(s, s^k)\|_{mn}$, где s &mdash; рассмтриваемый объект, 
	$s^k$ &mdash; объект обучающей выборки $k = 1, \dots m$, $\rho_t$ &mdash; некоторая метрика, $t = 1, \dots n$. 
	Tогда $I_{ob} = \mathfrak{C_{mn}}(\mathbb{R}_{+})$.
	<LI> $I_{cl}$&mdash; пространство описаний классов. Например, описанием j-го класса может быть вектор 
	$(P_j(s^1), \dots P_j(s^m)$, где $P_j)$ &mdash; предикат, говорящий о принадлежности объектов данному классу.
	Тогда $I_{cl} = E_2^m$.
	<LI>$\mathfrak{I_i}$ &mdash; пространство начальных информаций. Это пространство матриц размерности $q$ на $l$:
	$\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{I})}$,
	где $\mathfrak{I}$ &mdash; пространство совместных описаний объектов и классов, т.е. $\mathfrak{I} = I_{ob} \times I_{cl}$.
	<LI>$\mathfrak{I_f}$ &mdash; пространство финальных информаций. Это пространство матриц той же размрности: $\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{\tilde{I}})}$,
	где $\mathfrak{\tilde{I}} = E_2$, т.е. каждый элемент матрицы говорит о принадлежности соответсвующего объекта соответсвующему классу; 
	<LI>$I^u$ &mdash; универсальные ограничения. Эти ограничения не проверяются констрактивно, а задаются как некоторый "мешок" отображений,
	например, непрерывные отображения, монотонные, или в общем случае &mdash; отображения некоторой категории.
	<LI>$I^l$ &mdash; локальные ограничения. Эти ограничения задаются конструктивно. Например, это требование принимать истинные значения на прецедентах.
	Такое требование называется корректностью алгоритма. Заметим, что в данном задании мы не будем вводить такое требование.
	<LI>$\mathfrak{m}$ &mdash; модель алгоритмов, или семейство алгоритмов, удовлетворяющее определенным локальным и глобальным ограничениям.
	$\mathfrak{m:*}$ &mdash; семейство всех возможных алгоримтов. 
	</UL>
	
	Задача заключается в построении алгоритма (отображения) $A$, действующего из пространства начальных информация в пространство финальных информация
	и удовлетворяющих глобальным и локальным ограничениям:
	<p>
	\[
	A \in \mathfrak{m}^*: \mathfrak{I_i} \rightarrow \mathfrak{I_f}, \quad	A \in \mathfrak{m}^*[I^u], A \in \mathfrak{m}^*[I^l]
	\]
	</p>
	
	<h2>Конкретизация постановки в условиях практикума</h2>
	Перейдем от общей остановки задачи к непостредственно той, которую будем рассматривать в рамках данного задания.
	
	<UL>
	<LI> Объектами являются точки на плоскости. Пространство описания объектов $I_{ob} = \mathbb{R}^2$.
	
	<LI> Универсальные ограничения $I^u$: однородность объектов (порядок объектов неважен для алгоритма) и независимость объектов
	(алгоритм осущеcтвляет классификацию каждого отдельного объекта независимо).
	Следующим универсальным ограничением является принадлежность объекта только к одному классу.
	Из этого огранчения следует, что классы не являются независимыми.
	Однако классы являются в нашем случае однородными: алгоритм не различает классы и относится одинаково ко всем.
	Кроме того, мы будем рассматривать алгоритмы, принадлежащие только определенному семейству.
	Далее в отчете мы подробно опишем оба рассматриваемых семейства.
	
	<LI> Из независимости объектов следует, что можно рассматривать задачу классификации отдельно для каждого объекта.
	Кроме того, договоримся рассматривать информацию о прецедентах и классах как параметры алгоритма.
	Тогда можно сказать, что пространство начальных информаций $\mathfrak{I_i} = \mathbb{R}^2$.
	
	<LI> Из условия о принадлежности только одному классу следует, что можно задать результат классификации объекта
	меткой класса: числом из множества $\{1, \dots l\}$. 
	Так как рассматривается алгоритм для классификации отдельного объекта, получаем пространство финальных информаций 
	$\mathfrak{I_f} = \{1, \dots l\}$.
	</UL>
	

	<h1>Оптимизационный подход &mdash; для алгоритмов</h1>
	В поставленной задаче мы не требуем условия корректности алгоритма,
	т.к. поиск корректного алгоритма часто является неразрешимой задачей, а также может приводить к переобучению.
	Поэтому необходим иной способ учета обучающей информации и выбора алгоритма из семейства.
	При оптимизационном подходе вводится некоторый функционал качества и выбирается алгоритм, на котором достигается минимум:
	\[
	Q(A, \tilde{S}) \longrightarrow \min,
	\]
	где $\tilde{S}$ &mdash; обучающая выборка.
	Конкретный вид фукнционала может быть различным. Например, это может быть число ошибок алгоритма на обучающей выборке.
	Тогда мы будем выбирать корректный алгоритм (в случае, если такой алгоритм существует для данной выборки при остальных ограничениях задачи).
	В рамках этого задания мы не будем пользоваться функционалом вида $ Q(A, \tilde{S}) $. 
	В дальнейшем мы рассмотрим алгоритм $ A $ в виде композиции алгоритмического оператора и решающего правила и 
	будем решать оптимизационную задачу для оператора, а не для алгоритма.
	
	<h1>Алгебраический подход &mdash; идея декомпозиции</h1>
	<p>
	В общем случае работать с пространством начальных и финальных информаций неудобно, 
	поэтому в рамках алгебраического подхода решения задач классификации переходят в некоторое пространство, 
	называемое пространство оценок. Его выбирают произвольно, так, чтобы было удобно. 
	Для выполнения этого перехода в другое пространство алгоритм рассматривают как композицию алгоритмического оператора 
	и решающего правила $A = C \circ B$, 
	где $ B: \mathfrak{I_i} \rightarrow \mathfrak{I_e} $ &mdash; алгоритмический оператор, 
	$ C: \mathfrak{I_e} \rightarrow \mathfrak{I_f} $ &mdash; решающее правило. 
	Здесь $\mathfrak{I_e}$ &mdash; пространство оценок. В данном задании выберем $\mathfrak{I_e} = \mathbb{R}^l$.
	</p>
	
	<img src="diagram0.png"  width="200">
	
	<p>
	В рамках данного задания $ B \in \mathfrak{m_0} $, 
	где $ \mathfrak{m_0} = \mathfrak{m_{0_1}} \cup \mathfrak{m_{0_2}} $ &mdash; некоторое семейство операторов &mdash; объединение двух семейств,
	конкретный вид которых будет рассмотрен далее. А решающее правило зафиксируем как $ C = argmax $.
	</p>
	
	<h1>Оптимизационный подход &mdash; для операторов</h1>
	Будем искать не оптимальный алгоритм, а оптимальный оператор, 
	зафиксировав при этом решающее правило, 
	т.е. введем функционал вида $ Q(B, \tilde{S}) $. Этот функционал можно выбирать произвольным образом.
	
	Заметим, однако, что при данном подходе мы сталкиваемся с проблемой правильных оценок для элементов обучающей выборки, 
	т.к. мы знаем только метки классов для них, а не оценки.
	
	В ходе выполнения данной работы, был выбран функционал качества
	\begin{equation}\label{quality functional for operator}
		Q(B, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel B(S_i) - c_i \parallel^2} \longrightarrow \underset{B}{\min},	
	\end{equation}
	где $ c_i $ &mdash; оценки для объектов: бинарный вектор размерности $l$ с единсвтвенной единицей в позиции,
	соответсвующей классу, которму принадлежит объект.
	
	
<h1>Алгебраический подход &mdash; идея суперпозиции</h1>
	<p>
	Представим теперь наш алгоритм не в виде $ A = C \circ B$, 
	а в виде $ A = C \circ F \circ B $, где $F: \mathfrak{I_e} \rightarrow \mathfrak{I_e}$ &mdash; корректирующая операция.
	</p>
	
	<img src="diagram1.png"  width="220">
	
	<p>
	Основная идея алгебраического подхода состоит в следующем. 
	В общем случае выбранная нами модель операторов может не иметь оптимума, 
	поэтому будем использовать алгебраическое расширение модели. 
	Будем строить не один оператор, а несколько, и использовать их суперпозицию $A = C \circ F(B_1, \dots, B_p)$.
	</p>
	
	<img src="diagram2.png"  width="310">
	
	<p>
	Корректирующие операции $ F \in \mathfrak{f} $, где $ \mathfrak{f} = \mathfrak{f_1} \cup \mathfrak{f_2} $ &mdash; семейство корректирующих операций, 
	состоящее из двух подсемейств, где $ \mathfrak{f_i} = \{G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e} | p \in \mathbb{N} \}, i = 1, 2 $.
	</p>
	
	На самом деле корректирующая операция есть некоторое другое отображение
	\begin{equation}
		F: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e}\}^p \rightarrow \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \}
	\end{equation}
	Но она индицируется операцией
	\begin{equation}
		G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e}
	\end{equation}
	если
	\begin{equation}
		F(B_1, \cdots, B_p)(S) = G(B_1(S), \cdots, B_p(S))
	\end{equation}
	
	<h1>Оптимизационный подход &mdash; для суперпозиций</h1>
	Выше мы ввели функционал качества для настройки одного оператора.
	\begin{equation}
		Q: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} \times (\mathfrak{I_i}, \mathfrak{I_e})^q \rightarrow \mathbb{R}
	\end{equation}
	Т.к. $ F(\cdot) \in \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} $, то мы можем в тот же функционал подставить вместо одного оператора суперпозицию.
	\begin{equation}
		Q(F(B_1, \cdots, B_p), \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel F(B_1(S_i), \cdots, B_p(S_i)) - c_i \parallel^2}
	\end{equation}
	Таким образом, получаем оптимизационную задачу:
	\begin{equation}\label{quality functional for superposition}
		Q(F(B_1, \cdots, B_p), \tilde{S}) \longrightarrow \underset{p, B_1, \cdots, B_p, F}{\min}
	\end{equation}
	
		
	<h1>Итерационный процесс построения суперпозиции</h1>
	Будем последовательно добавлять операторы в суперпозицию.
	
	<UL>
		<LI> $B_1 = \underset{B}{argmin} Q(B, \tilde{S})$
		<LI> $(F, B_1, B_2) = \underset{F, B_1, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		Решать эту задачу слишком сложно, поэтому будем использовать неэквивалентное упрощение. 
		В качестве первого оператора будем использовать оператор полученный на предыдущем шаге. Т.е.
		$(F, B_1, B_2) = \underset{F, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		<LI> $(F, B_1, B_2, B_3) = \underset{F, B_3}{argmin} Q(F(B_1, B_2, B_3), \tilde{S})$
		$\cdots$
	</UL>
	Это внешний цикл настройки суперпозиции, который останавливается, 
	когда $|Q_{new} - Q_{old}| < \varepsilon $, где $Q_{new}$ и $Q_{old}$ &mdash; значение функционала, 
	достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	При каждой итерации внешнего цикла надо решать задачу оптимизации 
	$(F, B_1,\cdots, B_p) = \underset{F, B_p}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$. 
	Для этого входим во внутренний цикл, который будет решать эту задачу методом покоординатного спуска(опять используем неэквивалентное упрощение).
	
	<OL>
		<LI>$F^0$ &mdash; задаем некоторое начальное приближение для корректирующей операции
		<LI>$B_p^i = \underset{B_p}{argmin} Q(F^{i-1}(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$
		<LI>$F^i = \underset{F}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p^i), \tilde{S})$
	</OL>
	
	 Проводим итерации цикла 1 - 2 до того, как $|Q'_{new} - Q'_{old}| < \delta $, где $Q'_{new}$ и $Q'_{old}$ &mdash; значение функционала, 
	 достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	 Для реализации изложенного итерационного подхода необходимо уметь выполнять следующие действия:
	 <UL>
	 	<LI> $B^* = \underset{B}{argmin}Q(B, \tilde{S})$;
	 	<LI> $F^*_p \mapsto F^0_{p+1}$;
	 	<LI> $B^* = \underset{B}{argmin}Q(F(B_1, \cdots, B_{p-1}, B), \tilde{S})$;
	 	<LI> $F^* = \underset{F}{argmin}Q(F(B_1, \cdots, B_p), \tilde{S})$
	 	<LI> критерий останова для внутреннего цикла $|Q'_{new} - Q'_{old}| < \delta $;
	 	<LI> критерий останова для цикла наращивания p $|Q_{new} - Q_{old}| < \varepsilon $;
	 </UL>	
		Для добавления сразу двух операторов:
	<UL>	
	 	<LI> $F^*_p \mapsto F^0_{p+2}$;
	 	<LI> $(B^*_{p+1}, B^*_{p+2}) = \underset{B', B''}{argmin}Q(F(B_1, \cdots, B_p, B', B''), \tilde{S})$;
	</UL>
		Для перенастройки ранее добавленного оператора:
	<UL>
	 	<LI> $B^*_i = \underset{B}{argmin}Q(F(B_1, \cdots, B_{i-1}, B, B_{i+1}, \cdots, B_p), \tilde{S})$.
	 </UL>
	
	<h1>Типы параметров по отношению к оптимизационному подходу</h1>
	Параметры можно разделить на две группы: оптимизируемые и неоптимизируемые.
	Неоптимизируемые параметры либо фиксируются, либо настраиваются по выборке (с помощью какой-либо эвристики).
	Оптимизируемые параметры могут быть либо найдены аналитиески, либо с помощью численной процедуры оптимизации.	

	<h1>Модель алгоритмов 1. Аффинная модель</h1>
	<h2>Описание модели</h2>
	В данной работе в качестве модели алгоритмических операторов была выбрана аффинная модель.

	Аффинная модель &mdash; это линейная модель над спрямляющим пространством степени $1$, 
	которая для каждого объекта возвращает оценки за принадлежность каждому из классов. 
	Оценки зависят от объекта, а также от $l (d+1)$ параметров, где $l$ &mdash; количество классов, 
	$d$ &mdash; размерность пространства. 

	<h2>Использование модели</h2>

	Для данной модели пространство начальных информаций совпадает с пространством описания объектов, 
	а пространством оценок является весь интервал $(-\infty, \infty)$. 

	Для объекта $S$ и заданных параметров модели оценки вычисляются по следующим формулам:<br>

	<center>
		$\displaystyle \Gamma_j = w_{j0} + w_{j1}x_1 + w_{j2}x_2$, &nbsp;&nbsp  т.е. $\vec{\Gamma} = WS$.
	</center>

	<h2>Перечисление параметров модели операторов</h2>

	В таблице приведены параметры модели:<br><br>
	<center>
	<table border="1">
	<caption></caption>
	<tr>
	<th>Название параметра</th>
	<th>Обозначение параметра</th>
	<th>Область допустимых значений</th>
	<th>Тип параметра</th>
	</tr>
	<tr>
	<td><center>Коэффициенты</center></td>
	<td><center>$W = \{w_{ji}\} $</center></td>
	<td><center>$w_{ji} = (-\infty,\infty)$</center></td>
	<td>Оптимизируемые</td>
	</tr>
	</table>
	</center>
	
	<h2>Настройка параметров</h2>
	Для настройки воспользуемся правилом Хебба. Для этого нужно немного модифицировать нашу модель, 
	потому что правило Хебба применяется к линейным классификаторам. Добавим к каждому объекту одну единичную координату. 
	Тогда линейный классификатор в таком пространстве будет афинной моделью в исходном пространстве.

	Для оптимизации алгоритмического оператора был выбран следующий критерий качества
	$$Q(B, S) = \frac{1}{ql} \sum_{i = 1}^{q} \| B(s_i) - C^{-1}(y_i) \| ^2,$$
	В качестве $S$ бралась обучающая выборка.

	Для оптимизации данного функционала применялся метод градиентного спуска. 
	Пересчет параметров модели для данного объекта будет осуществляться по формулам

	$$w_{j0}^{k+1} = w_0^k - 2\eta^{k+1}(w_0^k + w_1^k x_1 + w_2^k x_2),$$

	$$w_{ji}^{k+1} = w_i^k - 2\eta^{k+1}(w_0^k + w_1^k x_1 + w_2^k x_2)x_i,$$

	где $\eta^{k+1}$ &mdash; скорость обучения.

	<h2>Исследование модели алгоритмических операторов</h2>
	<h3>Задача, для которой в модели есть корректный алгоритм</h3>

	Рассмотрим задачу, представленную на первом рисунке. Положим, что синие &mdash; класс 1, оранжевые &mdash; класс 2.<br>

	<embed src="AO_good_example.svg" type="image/svg+xml" width="500" height="410">

	<h4>Теоретическое доказательство наличия решения</h4>

	Алгоритм с коэффициентами $w_1 = \{ 2, -1, 0 \}$, $w_2 = \{ -2, 1, 0 \}$ является корректным.

	<h4>Результат построения корректного оператора программной системой</h4>
	<br>  
	<embed src="AO_good_example_res.svg" type="image/svg+xml" width="500" height="410"> 

	<h3>Задача, для которой в модели нет корректного оператора</h3>

	Для афинной модели есть множество примеров задач, для которого не существует корректного алгоритма. 
	Самый простой &mdash; три точки, лежащие на одной прямой: крайние принадлежат первому классу, та, что посередине &mdash; второму. 
	Более общий пример &mdash; задача XOR. <br>
	
	На рисунках ниже представлен пример еще одной такой задачи и результат работы программной системы.<br>
	
	<embed src="AO_bad_example.svg" type="image/svg+xml" width="500" height="410">  
	<embed src="AO_bad_example_res.svg" type="image/svg+xml" width="500" height="410"> 

	<h1>Семейство корректирующих операций 1. Монотонные аффинные.</h1>

	<h2>Описание семейства</h2>

	В данной работе использовались монотонные аффинные корректирующие операции. 
	Такая КО представляет собой линейную комбинацию оценок алгоритмических операторов с неотрицательными весами. 
	Аффинность получаем путем добавления еще одного «псевдооператора» с вектором оценок, состоящим из единиц.

	<h2>Использование корректирующей операции</h2>

	Опишем способ использования монотонной аффинной корректирующей операции. 
	КО принимает на вход оценки $p$ алгоритмических операторов: $\vec{\Gamma}^1, \dots, \vec{\Gamma}^p$. 
	Параметрами являются $p+1$ неотрицательный вес: $a_0, a_1, \dots, a_p$. 
	Результатом корректирующей операции является оценка, подсчитываемая по следующей формуле: <br><br>

	<center>
		$\vec{\Gamma} = \alpha_0 + \alpha_1\vec{\Gamma}^1 + \ldots + \alpha_p\vec{\Gamma}^p$, &nbsp; где $\{\alpha_0, \dots, \alpha_p\ \in \mathbb{R}_+\}.$
	</center>	
	<br>

	Результат работы корректирующего оператора полностью опеределен оценками алгоритмических моделей и своими параметрами и принадлежит пространству оценок.

	<h2>Перечисление параметров модели операторов</h2>

	В таблице приведены параметры модели:<br><br>

	<center>
	<table border="1">
	<caption></caption>
	<tr>
	<th>Название параметра</th>
	<th>Обозначение параметра</th>
	<th>Область допустимых значений</th>
	<th>Тип параметра</th>
	</tr>
	<tr>
	<td><center>Арность оператора</center></td>
	<td><center>$p$</center></td>
	<td><center>$\mathbb{N}$</center></td>
	<td>Оптимизируемый</td>
	</tr>
	<tr>
	<td><center>Веса оценок</center></td>
	<td><center>$\vec{a}$</center></td>
	<td><center>$\mathbb{R}_+^{p+1}$</center></td>
	<td>Оптимизируемый</td>
	</tr>
	</table>
	</center>
	<br>

	<h2>Настройка параметров</h2>

	<h3>Настройка арности оператора</h3>

	Арность корректирующего оператора вычисляется постепенно: будем увеличивать $p$ на единицу, добавляя по одному распознающему оператору, 
	пока не будет выполнен критерий останова. 

	Критерий останова &mdash; достижение локального минимума функционала качества, 
	(то есть последний добавленный оператор не уменьшил функционал) или $p=10$.

	<h3>Настройка весов</h3>

	При фиксированной арности требуется минимизировать следующий функционал:
	$$Q(\vec{\alpha}, S) = \frac{1}{ql} \sum_{i = 1}^{q} \|  \alpha_0 + \alpha_1\vec{\Gamma}^1 + \ldots + \alpha_p\vec{\Gamma}^p - C^{-1}(y_i) \| ^2.$$

	Это задача условной непрерывной оптимизации по непрерывным параметрам. 
	Положим, что $\alpha_j \in \{0, 0.01, 0.02, 0.03, \dots, 1 \}$. 
	Таким образом, переходим к задаче оптимизации по дискретным параметрам.
	Она решается полным перебором по всем $\alpha_j$. Принимается конфигурация, на которой получается наименьшее значение оптимизируемого функционала.

	<h1>Расширение модели 1 семейством 1</h1>
	
	Было реализовано расширение модели алгоритмических оператов. Опишем процедуру построения расширения.<br>
	
	Первый шаг: добавляем один распознающий оператор, корректирующая операция тождественна.<br>
	
	Итерационный процесс:
	<UL>
	<LI> Добавляем один распознающий оператор;
	<LI> Настройка весов;
	<LI> Пока уменьшается значение функционала и количество распознающих операторов меньше 10.
	</UL>
	
	<h2>Исследование суперпозиции</h2>
	Семейство корректирующих операций не сужает семейство алгоритмических операторов, 
	так как в нем содержится тождественный оператор &mdash; при $p=1$ $\alpha_0=0$, $\alpha_1=1$.
	
	<h2>Задача, для которой в модели есть корректный оператор</h2>
	Рассмотрим задачу, для которой в модели был корректный оператор. 
	Так как в семейство корректирующих операций не сужает семейство алгоритмических операторов,
	то и в расширении модели будет присутствовать корректный алгоритм.
	
	<h2>Задача, для которой в модели нет корректного оператора</h2>
	
	<h2>Доказательство корректности расширения</h2>
	При помощи операторов из нашей модели и корректирующий операций для любой обучающей выборки,
	в которой объекты разных классов имеют разные описания, мы можем построить такое решение,
	где каждый обучающий объект отделен ото всех остальных.
		
	<h1>Модель алгоритмических операторов 2. Однослойные нейронные сети</h1>

	<h2>Описание модели</h2>
	Первая модель операторов --- модель нейронных сетей. Она основывается на моделировании работы нейронов человеческого мозга. 
	Нейроны представляются в виде узлов сети, способных принимать сигнал и, преобразовывая, выдавать его. С
	вязи между набором нейронов определяют структуру нейросети. Есть выделенные слои нейронов: 
	входной слой и выходной слой. На входной слой поступают сигналы, полученные от объектов, 
	а на выходном слое получаются преобразованные сигналы входного слоя. 
	В данном случае, сигналы выходного слоя интерперетируются как элемент пространства оценок.
	
	<h2>Использование модели</h2>

	<p>В качестве второй модели алгоритмических операторов было выбрано семейство однослойных нейронных сетей.</p>

	<p>В качестве входного слоя использовались спрямляющее пространство размерности 1.</p>

	<p>В выходном слое использовались $l$ выходных нейронов - каждый нейрон соотвествует некоторому своему классу, 
	при этом каждому классу соотвествует ровно один нейрон. То есть соответствие взаимно однозначно.</p>

	<p>Для определенности будем считать, что $j$-ый нейрон соответствует $j$-ому классу. 
	Данная модель алгоритмических операторов действует следующим образом. 
	Сначала признаки объекта $S \in \mathbb{R}^2 $ переводятся в спрямляющее пространство 1, 
	которое представляет собой исходные призаки объекта и $-1$ (правильно 1, но здесь для удобства будет считать, что -1). 
	Обозначим эти признаки как $s_1, s_2, s_3$ - их всего $3$, так как исходное пространство размерности 2. 
	Тогда значение на выходном $i$-ом нейроне вычисляется следующим образом: 
	$a_i(S) = \sigma_i(w^i_1s_1+w^i_2s_2 + w^i_3s_3)$, где $w^i_j$ - веса синаптической связи, 
	соединяющие входной слой с $i$-ым нейроном, а $\sigma_i()$ - функция активации $i$-ого нейрона. 
	В качестве оценки за класс $i$ берется значение на $i$-ом нейроне.</p>

	<h2>Параметры модели</h2>
	<br>
	<center>
	<table border="1">
	<caption></caption>
	<tr>
	<th>Название параметра</th>
	<th>Обозначение параметра</th>
	<th>Область допустимых значений</th>
	<th>Тип параметра</th>
	</tr>
	<tr>
	<td><center>Функции активации</center></td>
	<td><center>$\sigma_i$</center></td>
	<td><center>$C^1(\mathbb{R})$</center></td>
	<td>Фиксированный</td>
	</tr>
	<tr>
	<td><center>Веса синаптических связей</center></td>
	<td><center>$w$</center></td>
	<td><center>$\mathbb{R}^{3l}$</center></td>
	<td>Численно оптимизируемый</td>
	</tr>
	</table>
	</center>
	<br>

	<h2>Настройка параметров</h2>

	<p>В качестве функций активации для всех нейронов была выбрана сигмоидная функция:
	$$\sigma(x) = \frac{1}{1+e^{-x}}$$</p>

	<p>Веса синаптической связей настраивались с помощью широко используемого алгоритма настройки нейронный сетей - Back Propagation:
	<ul>
	<li>Инициализируем веса $w^i_j, i = 1,\dots,l, j = 1,2,3$ случаными числами из отрезка $[0;1]$.</li>
	<li>Пока значение функционала не стабилизировалась и пока количество выполненных итераций не превысило значения $max_{iter}$ повторять:</li>
	<li>Выбираем случаный образом объект $S$ из выборки прецедентов. Обозначим его признаки $s_1,s_2,s_3$.</li>
	<li>Модифицируем веса синаптических связей: $w^i_j = w^i_j - \nu (a_i(S) - r_i) \sigma'(w^i_1s_1+w^i_2s_2 + w^i_3s_3)s_j$, 
	где $r_i$ - псевдообращение решающего правила.</li>
	<li>Вычисляем новое значение функционала и увеличиваем число выполненных итераций на 1. Переходим на шаг 2.</li>
	</ul>
	</p>

	<p>В качестве критерия останова  по функионалу используется следующий критерий:
	$$(|Q_{new} - Q_{old}| < 0.01*Q_{old})$$.</p>

	<p>Параметры $\nu$ - темп обучения нейронной сети, и $max_{iter}$ - максимальное количество итераций, 
	настаривались эмпирически по модельным данным. В итоге $\nu = 1$, $max_{iter} = 10000$.</p>

	<h2>Исследование модели алгоритмических операторов</h2>

	<h3>Задача, для которой в модели есть корректый оператор.</h3>

	<p>Рассмотрим задачу, представленную на первом рисунке. Положим, что синие &mdash; класс 1, оранжевые &mdash; класс 2.</p>
	
	<p>На втором рисунке представлен результат работы программной системы.</p>
	
	<embed src="AO_good_example.svg" type="image/svg+xml" width="500" height="410">	
	<embed src="neural_good_res.svg" type="image/svg+xml" width="500" height="410">

	<h2>Задача, для которой в модели нет корректного оператора.</h2>
	
	<p>Классический пример &mdash; задача XOR. В книге "Перцептроны" Марвина Минского и Сеймура Паперта показано,
	что данная задача не решается с помощью однослойных нейросетей.</p>
	
	<p>На рисунках ниже представлена сама задача и результат работы программной системы.</p>
	
	<embed src="neural_bad.svg" type="image/svg+xml" width="500" height="410">	
	<embed src="neural_bad_res.svg" type="image/svg+xml" width="500" height="410">	
	
	<h1>Семейство корректирующих операций 2. Комитет большинства</h1>
	<h2>Описание семейства</h2>
	<p>
	Основная идея комитета большинства заключается в применении нескольких операторов к одному объекту и 
	отнесении его к тому классу, за который суммарная оценка операторов максимальна.
	</p>	
	
	$$F(\vec {\Gamma_1},\dots,\vec {\Gamma_p}) = (F_1(\Gamma_{1 1},\dots,\Gamma_{p 1}), \cdots, F_l(\Gamma_{1 l},\dots,\Gamma_{p l}))^T,$$
	
	$$ F_j(\Gamma_{1_j},\dots,\Gamma_{p_j}) = \mathbb{I}\left[ \frac{1}{p} \sum_{k = 1}^p{\Gamma_{k_j}} > \alpha_j \right] $$
	 	
		
	<h2>Перечисление параметров семейства корректирующих операций</h2>

	<br>
	<center>
	<table border="1">
	<caption></caption>
	<tr>
	<th>Название параметра</th>
	<th>Обозначение параметра</th>
	<th>Область допустимых значений</th>
	<th>Тип параметра</th>
	</tr>
	<tr>
	<td><center>Число операторов</center></td>
	<td><center>$p$</center></td>
	<td><center>$\mathbb{N}$</center></td>
	<td>Настраиваемый</td>
	</tr>
	<tr>
	<td><center>Пороги</center></td>
	<td><center>$\alpha_i, i=1,\dots,l$</center></td>
	<td><center>$[-1, 1]$</center></td>
	<td>Оптимизируемый</td>
	</tr>
	</table>
	</center>
	<br>

	<h2>Настройка параметров</h2>
	
	Настройка коэффициентов $ \alpha_i, i=1,\dots,l$ при фиксации числа операторов.
	
	Оптимальные значения $ \alpha_i, i=1,\dots,l$ находятся при минимизации функционала. Выпишем $ Q $ как фукнцию от $ \alpha_k $:
	\begin{equation}
		Q(\alpha_1, \cdots, \alpha_l, \tilde{S}) = \sum_{j = 1}^q {\sum_{i = 1}^l { \mathbb{I}\left[\sum_{k = 1}^p B_i^k (S_j) > \alpha_i\right] - c_j(i) } } \longrightarrow \underset{\alpha_i}{\min}
	\end{equation} 
	Получили сложную задачу оптимизации. Будем искать ее решение методом Монте-Карло.
	
	<h1>Расширение модели 2 семейством 2</h1>	
	
	<h2>Исследование суперпозиции</h2>
	Семейство корректирующих операций не сужает семейство алгоритмических операторов, 
	так как в нем содержится тождественный оператор &mdash; при $p=1$.
	
	<h2>Задача, для которой в модели есть корректный оператор</h2>
	Рассмотрим задачу, для которой в модели был корректный оператор. 
	Так как в семейство корректирующих операций не сужает семейство алгоритмических операторов,
	то и в расширении модели будет присутствовать корректный алгоритм.
	
	<h2>Задача, для которой в модели нет корректного оператора</h2>	
	
	</body>
</html>

