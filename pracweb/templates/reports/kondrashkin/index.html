{% extends "reports_base.html" %}
{% block author %}Дмитрий Кондрашкин{% endblock %}
{% block topic %}Настройка суперпозиции алгебраических операторов с помощью корректирующих операций{% endblock %}

{% block report %}
    <h1>Введение</h1>

	В данной работе проводятся исследование применения алгебраического подхода к решению задачи многоклассовой классификации. Алгебраический подход заключается в приминении операции, которая позволяет объединить результаты работы нескольких алгоритмов. Данный подход позволяет на основе "слабых" алгоритмов получать композиции хорошего качества.

	Для исследования алгебраического подхода взяты конкретые алгоритмы и корректирующая операция.

	Большинство задач задач в машинном обучении это задачи оптимизации. На протяжении всей работы рассматривается конкретный функционал качества для алгоритма классификации. Чтобы облегчить задачу оптимизации, используют идею декомпозиции и суперпозиции, которые позволяют из простых семейств алгоритмов собрать мощные алгоритм классификации.

	
	<h1>Задача классификации</h1>
	<h2>Общая постановка</h2>
	На неформальном языке задача классификации заключается в некотором "разумном" отнесении объектов к классам. 
	Будем рассматривать $q$ объектов и $l$ классов. Задача классификации -- задача обучения с учителем.
	Поэтому рассматривается также обучающая выборка: множество $m$ объектов, для которых известны принадлежности к классам.
	
	Опишем общую постановку задачи формально. Пусть:
	<UL>
	<LI> $I_{ob}$ &mdash; пространство описаний объектов. Дескриптивная функция D сопоставляет каждому объекту
	его признаковое описание. Например, $D(s) = \|\rho_t(s, s^k)\|_{mn}$, где s &mdash; рассмтриваемый объект, 
	$s^k$ &mdash; объект обучающей выборки $k = 1, \dots m$, $\rho_t$ &mdash; некоторая метрика, $t = 1, \dots n$. 
	Tогда $I_{ob} = \mathfrak{C_{mn}}(\mathbb{R}_{+})$.
	<LI> $I_{cl}$&mdash; пространство описаний классов. Например, описанием j-го класса может быть вектор 
	$(P_j(s^1), \dots P_j(s^m)$, где $P_j)$ &mdash; предикат, говорящий о принадлежности объектов данному классу.
	Тогда $I_{cl} = E_2^m$.
	<LI>$\mathfrak{I_i}$ &mdash; пространство начальных информаций. Это пространство матриц размерности $q$ на $l$:
	$\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{I})}$,
	где $\mathfrak{I}$ &mdash; пространство совместных описаний объектов и классов, т.е. $\mathfrak{I} = I_{ob} \times I_{cl}$.
	<LI>$\mathfrak{I_f}$ &mdash; пространство финальных информаций. Это пространство матриц той же размрности: $\mathfrak{I_i} = \mathfrak{C_{ql}(\mathfrak{\tilde{I}})}$,
	где $\mathfrak{\tilde{I}} = E_2$, т.е. каждый элемент матрицы говорит о принадлежности соответсвующего объекта соответсвующему классу; 
	<LI>$I^u$ &mdash; универсальные ограничения. Эти ограничения не проверяются констрактивно, а задаются как некоторый "мешок" отображений,
	например, непрерывные отображения, монотонные, или в общем случае &mdash; отображения некоторой категории.
	<LI>$I^l$ &mdash; локальные ограничения. Эти ограничения задаются конструктивно. Например, это требование принимать истинные значения на прецедентах.
	Такое требование называется корректностью алгоритма. Заметим, что в данном задании мы не будем вводить такое требование.
	<LI>$\mathfrak{m}$ &mdash; модель алгоритмов, или семейство алгоритмов, удовлетворяющее определенным локальным и глобальным ограничениям.
	$\mathfrak{m:*}$ &mdash; семейство всех возможных алгоримтов. 
	</UL>
	
	Задача заключается в построении алгоритма (отображения) $A$, действующего из пространства начальных информация в пространство финальных информация
	и удовлетворяющих глобальным и локальным ограничениям:
	<p>
	\[
	A \in \mathfrak{m}^*: \mathfrak{I_i} \rightarrow \mathfrak{I_f}, \quad	A \in \mathfrak{m}^*[I^u], A \in \mathfrak{m}^*[I^l]
	\]
	</p>
	
	<h2>Конкретизация постановки в условиях практикума</h2>
	Перейдем от общей остановки задачи к непостредственно той, которую будем рассматривать в рамках данного задания.
	
	<UL>
	<LI> Объектами являются точки на плоскости. Пространство описания объектов $I_{ob} = \mathbb{R}^2$.
	
	<LI> Универсальные ограничения $I^u$: однородность объектов (порядок объектов неважен для алгоритма) и независимость объектов
	(алгоритм осущеcтвляет классификацию каждого отдельного объекта независимо).
	Следующим универсальным ограничением является принадлежность объекта только к одному классу.
	Из этого огранчения следует, что классы не являются независимыми.
	Однако классы являются в нашем случае однородными: алгоритм не различает классы и относится одинаково ко всем.
	Кроме того, мы будем рассматривать алгоритмы, принадлежащие только определенному семейству.
	Далее в отчете мы подробно опишем оба рассматриваемых семейства.
	
	<LI> Из независимости объектов следует, что можно рассматривать задачу классификации отдельно для каждого объекта.
	Кроме того, договоримся рассматривать информацию о прецедентах и классах как параметры алгоритма.
	Тогда можно сказать, что пространство начальных информаций $\mathfrak{I_i} = \mathbb{R}^2$.
	
	<LI> Из условия о принадлежности только одному классу следует, что можно задать результат классификации объекта
	меткой класса: числом из множества $\{1, \dots l\}$. 
	Так как рассматривается алгоритм для классификации отдельного объекта, получаем пространство финальных информаций 
	$\mathfrak{I_f} = \{1, \dots l\}$.
	</UL>
	

	<h1>Оптимизационный подход &mdash; для алгоритмов</h1>
	В поставленной задаче мы не требуем условия корректности алгоритма,
	т.к. поиск корректного алгоритма часто является неразрешимой задачей, а также может приводить к переобучению.
	Поэтому необходим иной способ учета обучающей информации и выбора алгоритма из семейства.
	При оптимизационном подходе вводится некоторый функционал качества и выбирается алгоритм, на котором достигается минимум:
	\[
	Q(A, \tilde{S}) \longrightarrow \min,
	\]
	где $\tilde{S}$ &mdash; обучающая выборка.
	Конкретный вид фукнционала может быть различным. Например, это может быть число ошибок алгоритма на обучающей выборке.
	Тогда мы будем выбирать корректный алгоритм (в случае, если такой алгоритм существует для данной выборки при остальных ограничениях задачи).
	В рамках этого задания мы не будем пользоваться функционалом вида $ Q(A, \tilde{S}) $. 
	В дальнейшем мы рассмотрим алгоритм $ A $ в виде композиции алгоритмического оператора и решающего правила и 
	будем решать оптимизационную задачу для оператора, а не для алгоритма.
	
	<h1>Алгебраический подход &mdash; идея декомпозиции</h1>
	<p>
	В общем случае работать с пространством начальных и финальных информаций неудобно, 
	поэтому в рамках алгебраического подхода решения задач классификации переходят в некоторое пространство, 
	называемое пространство оценок. Его выбирают произвольно, так, чтобы было удобно. 
	Для выполнения этого перехода в другое пространство алгоритм рассматривают как композицию алгоритмического оператора 
	и решающего правила $A = C \circ B$, 
	где $ B: \mathfrak{I_i} \rightarrow \mathfrak{I_e} $ &mdash; алгоритмический оператор, 
	$ C: \mathfrak{I_e} \rightarrow \mathfrak{I_f} $ &mdash; решающее правило. 
	Здесь $\mathfrak{I_e}$ &mdash; пространство оценок. В данном задании выберем $\mathfrak{I_e} = \mathbb{R}^l$.
	</p>
	
	<img src="diagram0.png"  width="200">
	
	<p>
	В рамках данного задания $ B \in \mathfrak{m_0} $, 
	где $ \mathfrak{m_0} = \mathfrak{m_{0_1}} \cup \mathfrak{m_{0_2}} $ &mdash; некоторое семейство операторов &mdash; объединение двух семейств,
	конкретный вид которых будет рассмотрен далее. А решающее правило зафиксируем как $ C = argmax $.
	</p>
	
	<h1>Оптимизационный подход &mdash; для операторов</h1>
	Будем искать не оптимальный алгоритм, а оптимальный оператор, 
	зафиксировав при этом решающее правило, 
	т.е. введем функционал вида $ Q(B, \tilde{S}) $. Этот функционал можно выбирать произвольным образом.
	
	Заметим, однако, что при данном подходе мы сталкиваемся с проблемой правильных оценок для элементов обучающей выборки, 
	т.к. мы знаем только метки классов для них, а не оценки.
	
	В ходе выполнения данной работы, был выбран функционал качества
	\begin{equation}\label{quality functional for operator}
		Q(B, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel B(S_i) - c_i \parallel^2} \longrightarrow \underset{B}{\min},	
	\end{equation}
	где $ c_i $ &mdash; оценки для объектов: бинарный вектор размерности $l$ с единсвтвенной единицей в позиции,
	соответсвующей классу, которму принадлежит объект.
	
	
<h1>Алгебраический подход &mdash; идея суперпозиции</h1>
	<p>
	Представим теперь наш алгоритм не в виде $ A = C \circ B$, 
	а в виде $ A = C \circ F \circ B $, где $F: \mathfrak{I_e} \rightarrow \mathfrak{I_e}$ &mdash; корректирующая операция.
	</p>
	
	<img src="diagram1.png"  width="220">
	
	<p>
	Основная идея алгебраического подхода состоит в следующем. 
	В общем случае выбранная нами модель операторов может не иметь оптимума, 
	поэтому будем использовать алгебраическое расширение модели. 
	Будем строить не один оператор, а несколько, и использовать их суперпозицию $A = C \circ F(B_1, \dots, B_p)$.
	</p>
	
	<img src="diagram2.png"  width="310">
	
	<p>
	Корректирующие операции $ F \in \mathfrak{f} $, где $ \mathfrak{f} = \mathfrak{f_1} \cup \mathfrak{f_2} $ &mdash; семейство корректирующих операций, 
	состоящее из двух подсемейств, где $ \mathfrak{f_i} = \{G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e} | p \in \mathbb{N} \}, i = 1, 2 $.
	</p>
	
	На самом деле корректирующая операция есть некоторое другое отображение
	\begin{equation}
		F: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e}\}^p \rightarrow \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \}
	\end{equation}
	Но она индицируется операцией
	\begin{equation}
		G: \mathfrak{I_e}^p \rightarrow \mathfrak{I_e}
	\end{equation}
	если
	\begin{equation}
		F(B_1, \cdots, B_p)(S) = G(B_1(S), \cdots, B_p(S))
	\end{equation}
	
	<h1>Оптимизационный подход &mdash; для суперпозиций</h1>
	Выше мы ввели функционал качества для настройки одного оператора.
	\begin{equation}
		Q: \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} \times (\mathfrak{I_i}, \mathfrak{I_e})^q \rightarrow \mathbb{R}
	\end{equation}
	Т.к. $ F(\cdot) \in \{\mathfrak{I_i} \rightarrow \mathfrak{I_e} \} $, то мы можем в тот же функционал подставить вместо одного оператора суперпозицию.
	\begin{equation}
		Q(F(B_1, \cdots, B_p), \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {\parallel F(B_1(S_i), \cdots, B_p(S_i)) - c_i \parallel^2}
	\end{equation}
	Таким образом, получаем оптимизационную задачу:
	\begin{equation}\label{quality functional for superposition}
		Q(F(B_1, \cdots, B_p), \tilde{S}) \longrightarrow \underset{p, B_1, \cdots, B_p, F}{\min}
	\end{equation}
	
		
	<h1>Итерационный процесс построения суперпозиции</h1>
	Будем последовательно добавлять операторы в суперпозицию.
	
	<UL>
		<LI> $B_1 = \underset{B}{argmin} Q(B, \tilde{S})$
		<LI> $(F, B_1, B_2) = \underset{F, B_1, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		Решать эту задачу слишком сложно, поэтому будем использовать неэквивалентное упрощение. 
		В качестве первого оператора будем использовать оператор полученный на предыдущем шаге. Т.е.
		$(F, B_1, B_2) = \underset{F, B_2}{argmin} Q(F(B_1, B_2), \tilde{S})$
		<LI> $(F, B_1, B_2, B_3) = \underset{F, B_3}{argmin} Q(F(B_1, B_2, B_3), \tilde{S})$
		$\cdots$
	</UL>
	Это внешний цикл настройки суперпозиции, который останавливается, 
	когда $|Q_{new} - Q_{old}| < \varepsilon $, где $Q_{new}$ и $Q_{old}$ &mdash; значение функционала, 
	достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	При каждой итерации внешнего цикла надо решать задачу оптимизации 
	$(F, B_1,\cdots, B_p) = \underset{F, B_p}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$. 
	Для этого входим во внутренний цикл, который будет решать эту задачу методом покоординатного спуска(опять используем неэквивалентное упрощение).
	
	<OL>
		<LI>$F^0$ &mdash; задаем некоторое начальное приближение для корректирующей операции
		<LI>$B_p^i = \underset{B_p}{argmin} Q(F^{i-1}(B_1, \cdots, B_{p-1}, B_p), \tilde{S})$
		<LI>$F^i = \underset{F}{argmin} Q(F(B_1, \cdots, B_{p-1}, B_p^i), \tilde{S})$
	</OL>
	
	 Проводим итерации цикла 1 - 2 до того, как $|Q'_{new} - Q'_{old}| < \delta $, где $Q'_{new}$ и $Q'_{old}$ &mdash; значение функционала, 
	 достигнутое после очередной итерации и полученное при предыдущей итерации соответственно.
	
	 Для реализации изложенного итерационного подхода необходимо уметь выполнять следующие действия:
	 <UL>
	 	<LI> $B^* = \underset{B}{argmin}Q(B, \tilde{S})$;
	 	<LI> $F^*_p \mapsto F^0_{p+1}$;
	 	<LI> $B^* = \underset{B}{argmin}Q(F(B_1, \cdots, B_{p-1}, B), \tilde{S})$;
	 	<LI> $F^* = \underset{F}{argmin}Q(F(B_1, \cdots, B_p), \tilde{S})$
	 	<LI> критерий останова для внутреннего цикла $|Q'_{new} - Q'_{old}| < \delta $;
	 	<LI> критерий останова для цикла наращивания p $|Q_{new} - Q_{old}| < \varepsilon $;
	 </UL>	
		Для добавления сразу двух операторов:
	<UL>	
	 	<LI> $F^*_p \mapsto F^0_{p+2}$;
	 	<LI> $(B^*_{p+1}, B^*_{p+2}) = \underset{B', B''}{argmin}Q(F(B_1, \cdots, B_p, B', B''), \tilde{S})$;
	</UL>
		Для перенастройки ранее добавленного оператора:
	<UL>
	 	<LI> $B^*_i = \underset{B}{argmin}Q(F(B_1, \cdots, B_{i-1}, B, B_{i+1}, \cdots, B_p), \tilde{S})$.
	 </UL>
	
	<h1>Типы параметров по отношению к оптимизационному подходу</h1>
	Параметры можно разделить на две группы: оптимизируемые и неоптимизируемые.
	Неоптимизируемые параметры либо фиксируются, либо настраиваются по выборке (с помощью какой-либо эвристики).
	Оптимизируемые параметры могут быть либо найдены аналитиески, либо с помощью численной процедуры оптимизации.	


<h1>Модель операторов K ближайших соседей (kNN)</h1>

 Основным принципом метода ближайших соседей является то, что объект присваивается тому классу, 
 который является наиболее распространённым среди соседей данного элемента.


Этот метод опирается на гипотезу компактности:
предположение о том,
что мера сходства объектов выбрана достаточно удачно,
и схожие объекты скорее лежат в одном классе, чем в разных.
В этом случае граница между классами имеет достаточно простую форму,
а классы образуют компактно локализованные области в пространстве объектов.

<h2>Использование модели</h2>

Пусть на пространстве объектов $\mathfrak{I}_i$
задана метрика $\rho(S_i,S_j), S_i, S_j \in \mathfrak{I}_i$,
определяющая сходство объектов.
Для классифицируемого объекта $S$ упорядочим
обучающую выборку по убыванию значения $\rho(S, S_i)$:
\[
    \rho(S,S_s^{(1)}) \leqslant \rho(S,S_s^{(2)}) \leqslant \dots \leqslant \rho(S,S_s^{(q)})
\],
где через $S_s^{(i)}$ обозначается $i$-й сосед объекта $S$ из выборки $\tilde S$,
$q$ &mdash; мощность обучающей выборки: $|\tilde S| = q$.
В наиболее общем виде алгоритм ближайших соседей есть
\[
    A(S) = \arg\max\limits_{j \in \{1,\dots,l\}} \Gamma_j(S), \;
    \Gamma_j = \sum_{i=1}^q [K_s^{(i)} = K_j] \omega (i,S),
\]
где $K_s^{(i)}$ &mdash; известный ответ на объекте $S_s^{(i)}$,
$l$ &mdash; число классов,
$\omega(i,S)$ &mdash; весовая функция,
равная в случае $k$ ближайших соседей $\omega(i,S) = [i \leqslant k]$.

Мы решаем задачу классификации,
используя алгебраический подход,
в рамках которого алгоритмический оператор $B \in \mathfrak{M}^0$ действует
из пространства начальных информаций $\mathfrak{I}_i$
в пространство оценок $\mathfrak{I}_e$.
В этом случае оператор kNN
возвращает оценки принадлежностей классифицируемого объекта классам
и при классификации на $l$ классов имеет вид
\[
    B(S) = (e_1, \dots, e_l), \; e_j = \frac{\Gamma_j(S)}{\sum_{m=0}^l e_m} =
    \frac{\sum_{i=1}^q [K_s^{(i)} = K_j][i \leqslant k]}{\sum_{m=0}^l e_m} =
    \frac{\sum_{i=1}^k [K_s^{(i)} = K_j]}{\sum_{m=0}^l e_m}.
\]
В рассматриваемой задаче пространство $\mathfrak{I}_i$ есть $\mathbb{R}^2$,
поэтому в качестве $\rho(S_i,S_j)$ выбрана
евклидова метрика $\rho(S_i,S_j) = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$,
где $S_i = (x_i,y_i), S_j = (x_j,y_j)$.


<h2>Параметры модели </h2>

<table>
<caption></caption>
<tr>
<th>Название</th>
<th>Обозначение</th>
<th>Область значений</th>
<th>Тип</th>
</tr>
<tr>
<td>Обучающая выборка</td>
<td>$\widetilde{S} = (x,y)$ </td>
<td>$\mathbb{R}^2$</td>
<td>Настраиваемый (процедурно-получаемый)</td>
</tr>
<tr>
<td>Число соседей</td>
<td>$k$</td>
<td>$\mathbb{N}$  </td>
<td>Настраиваемый (численно-оптимизируемый)</td>
</tr>
<tr>
<td>Метрика</td>
<td>$\rho$ </td>
<td>$\mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}_+$</td>
<td>Фиксированный</td>
</tr>
<tr>
<td>Весовая функция </td>
<td>$\omega$  </td>
<td>$\mathbb{Z}_+ \times \mathbb{R}^2 \to \mathbb{R}_+$</td>
<td>Фиксированный</td>
</tr>
</table>


<h2>Настройка (выбор оператора из модели)</h2>

Настроить модель оператора &mdash;
значит присвоить значение каждому из его параметров.
В рассматриваемой модели настраиваемым параметром
является число соседей $k$.
Поиск оптимального значения параметра $k$ происходит
в результате минимизации функционала потерь
$Q(B,\widetilde S) = \Vert B(\widetilde S) - C^{-1}(r(\widetilde S)) \Vert^2$,
который имеет следующий вид:
\[
    Q(k,\widetilde S) = \sum_{i=1}^q {\Vert e_i - K_i \Vert^2_{\mathbb{R}^l}} =
    \sum_{i=1}^q \sum_{j=1}^l {(e_{ij} - K_{ij})}^2 \to \min_k,
\]
где
$e_{ij} = \frac{\sum_{t\neq i} [r(S_t) = K_j][t \leqslant k]}{\sum_{m=1}^l e_{im}}$ &mdash;
оценка принадлежности $i$-ого объекта $K_j$-ому классу,
$K_i$ &mdash; $l$-мерный вектор из 0 и 1: $K_{ij} = [r(S_i) = K_j]$,
то есть координата равна 1, тогда и только тогда,
когда $i$-ый объект принадлежит $j$-ому классу.
При минимизации данного функционала метод оптимизации
будет стремиться увеличить оценку за правильный класс
и уменьшить оценку за неверный класс,
чтобы минимизировать норму разности.

<h3>Настройка оператора</h3>

Так как минимизация производится по целочисленному параметру $k$,
принимающему значения из отрезка $1,\dots,q$,
где $q$ &mdash; длина выборки,
то используется метод полного перебора.
Начальная инициализация $k = 1$:
подсчитывается функционал потерь $Q(1,\widetilde S)$.
Далее $k$ увеличивается итерационно с шагом $1$ до тех пор,
пока $Q(i,\widetilde S) \leqslant Q(i-1, \widetilde S)$.
Если функционал $Q$ не уменьшается на протяжении 5 итераций,
параметр $k$, найденный ранее, считается оптимальным.
	
<h2>Исследование модели алгоритмических операторов</h2>

<h3>Пример задачи, для которой в модели есть корректный алгоритм</h3>

Если классы делятся на локализованные области, для каждой из которых объекты этой области существенно ближе друг для друга, чем объекты других областей,
 то с помощью метода $k$ ближайших соседей можно получить корректный алгоритм.

Пример:

<img border="0" src="knn_good.png" alt="kNN works" width="410" height="410">

<h3>Пример задачи, для которой в модели нет корректного оператора</h3>

Существует множество случаев, когда $kNN$ не работает, например, когда один из классов гораздо более разрежен, чем другой.

Пример:

<img border="0" src="knn_bad.png" alt="kNN works" width="410" height="410">


<h1>Модель алгоритмов: "Наивный" байесовский классификатор</h1>
<h2>Вербальное описание модели</h2>

Метод Байеса это простой классификатор, основанный на вероятностной модели, имеющей сильное предположение независимости компонент вектора признаков. Обычно это допущение не соответствует действительности и потому одно из названий метода - Naıve Bayes (Наивный Байес).
По объектам обучающей выборки восстанавливается распределение путем максимизации правдоподобия этой выборки. 

Вводится понятие штрафа за неправильный ответ алгоритма, величина которого оптимизируется. 

Оценка принадлежности нового объекта $S$ к классу $r$ оценивается как вероятность пары $(S,r)$ в восстановленном распределении с учетом штрафа за отнесение к данному классу.

<h2>Формальное описание модели</h2>

С учетом независимости признаков совместное распределение вероятностей в пространстве описаний объектов и их классов задается следующим образом:
\[p(S,r) = P(r)p(S(x)|r)p(S(y)|r)\],
где S &mdash; признаковое описание объекта, r &mdash; метка класса.

$P(r)$ представляет собой дискретное распределение, а распределения $p(S(x)|r), p(S(y)|r)$ выбираются из некоторого параметрического семейства распределений, в нашем случае они представляют собой одномерные нормальные распределения с параметрами мат. ожидание и дисперсия.

Соответственно задача восстановления совместного распределения по обучающей выборке состоит в максимизации правдоподобия следующего вида:

\[P(\tilde{S}) = \prod_i {p(S_i,r_i)} = \prod_i{P(r_i)p(S_i(x)|r_i)p(S_i(y)|r_i)} \rightarrow \max_{\{\mu,\sigma,P\}}\].

Штрафы за неправильный ответ алгоритма вводятся следующим образом: $\lambda_{r, q}$ &mdash; величина штрафа за ответ алгоритма $q$, если <<правильный>> ответ $r$. Далее предполагается, что $\lambda_{r, r} = 0$, а $\lambda_{r, q} \equiv \lambda_r \forall q \neq r$. Значения $\lambda_r $ оптимизируются.

В итоге, оценки принадлежности к классам для нового объекта $S$ выглядят следующим образом:
	\begin{equation}
	\begin{split}
		&B(S) = ( \Gamma_1(S), \cdots, \Gamma_l(S))^T; \\
		&\Gamma_j(S) = \ln {\lambda_j \cdot \tilde{P_j} } + \ln {\tilde{p}_{j_x}( S( x ) )} + \ln { \tilde{p}_{j_y} ( S(y) )},
	\end{split}
	\end{equation}
	
<h2>Использование модели</h2>
Для данной модели пространство начальных информаций совпадает с пространством описания объектов, а пространством оценок является весь интервал $(-\infty,+\infty)$. Для объекта S и заданных параметров модели оценки вычисляются по следующим формулам: 

\begin{equation}
	\begin{split}
		&B(S) = ( \Gamma_1(S), \cdots, \Gamma_l(S))^T; \\
		&\Gamma_j(S) = \ln {\lambda_j \cdot \tilde{P_j} } + \ln {\tilde{p}_{j_x}( S( x ) )} + \ln { \tilde{p}_{j_y} ( S(y) )},
	\end{split}
	\end{equation}
	
где 
<ul>
<li>$\Gamma_j$ &mdash; $j$-ая компонента вектора ответа оператора $B$. Вспомогательная переменная.
<li>$\lambda_j$ &mdash; величина штрафа при <<неправильной классификации>> объекста из класса $j$. Параметр оператора.
<li>$\tilde{P}_j$ &mdash; оценка априорной вероятности Класса $j$. Параметр оператора.
<li>$\tilde{p}_{j_x}$ &mdash; оценка функции плотности координаты $x$ объектов в классе $j$ $p(x | r=j)$. Параметр оператора.
<li>$S( x )$ &mdash; координата $x$ объекта $S$. Вспомогательная переменная.
<li>$\tilde{p}_{j_y}$ &mdash; оценка функции плотности координаты $y$ объектов в классе $j$ $p(y | r=j)$. Параметр оператора.
<li>$S( y )$ &mdash; координата $y$ объекта $S$. Вспомогательная переменная.
</ul>

<h2>Параметры модели</h2>
<table>
<caption></caption>
<tr>
<th>Название</th>
<th>Обозначение</th>
<th>Область допустимых значений</th>
<th>Тип параметра по отношению к оптимизации</th>
</tr>
<tr>
<td>Величина штрафа</td>
<td>$\lambda_j$</td>
<td>$\mathbb{R}$</td>
<td>Оптимизируемый</td>
</tr>
<tr>
<td>Оценка априорной вероятности</td>
<td>$\tilde{P}_j$</td>
<td>$\mathfrak{I_f} \rightarrow [0, 1]$</td>
<td>Вычисляемый</td>
</tr>
<tr>
<td>Оценка функции плотности $x$</td>
<td>$\tilde{p}_{j_x}$</td>
<td>$ \mathbb{R} \rightarrow \mathbb{R}_{+} $</td>
<td>Вычисляемый</td>
</tr>
<tr>
<td>Оценка функции плотности $y$</td>
<td>$\tilde{p}_{j_y}$</td>
<td>$ \mathbb{R} \rightarrow \mathbb{R}_{+} $</td>
<td>Вычисляемый</td>
</tr>
<tr>
<td>Оценка мат.ожидания $x$</td>
<td>$\mu_{j_x}$</td>
<td>$\mathbb{R}$</td>
<td>Вычисляемый</td>
</tr>
<tr>
<td>Оценка мат.ожидания $y$</td>
<td>$\mu_{j_y}$</td>
<td>$\mathbb{R}$</td>
<td>Вычисляемый</td>
</tr>
<tr>
<td>Оценка дисперсии $x$</td>
<td>$\sigma_{j_x}$</td>
<td>$\mathbb{R}$</td>
<td>Вычисляемый</td>
</tr>
<tr>
<td>Оценка дисперсии $y$</td>
<td>$\sigma_{j_y}$</td>
<td>$\mathbb{R}$</td>
<td>Вычисляемый</td>
</tr>
</table>

<h2>Настройка модели</h2>
<h3>Вычисляемые параметры</h3>
Все вычисляемые параметры находятся путем максимизации правдоподобия обучающей выборки. Получаемые формулы:
\begin{equation}
	\begin{split}
		&\tilde{p}_{j_x}(S(x)) = \frac{1}{\sqrt{2\pi\sigma_{j_x}^2}}\exp\left(  {-\frac{(S(x) - \mu_{j_x})^2}{2\sigma_{j_x}^2}}\right)  ; \\
		&\tilde{p}_{j_y}(S(x)) = \frac{1}{\sqrt{2\pi\sigma_{j_y}^2}}\exp\left( {-\frac{(S(y) - \mu_{j_y})^2}{2\sigma_{j_y}^2}}\right) ; \\
		&\tilde{P}_j = \frac{\sum_{i=1,\dots, q}[r_i=j]}{\sum_{i=1,\dots, q} 1} ;
	\end{split}
	\end{equation}
где
<ul>
<li>$\mu_{j_x}$ &mdash; оценка мат.ожидания для координаты $x$(выборочное мат.ожидание) для Класса $ j $. 
<li>$\sigma_{j_x}$ &mdash; оценка дисперсии для координаты $x$(выборочная дисперсия) для Класса $ j $. 
<li>$\mu_{j_y}$ &mdash; оценка мат.ожидания для координаты $y$(выборочное мат.ожидание) для Класса $ j $. 
<li>$\sigma_{j_y}$ &mdash; оценка дисперсии для координаты $y$(выборочная дисперсия) для Класса $ j $. 
</ul>

<h3>Оптимизируемые параметры</h3>
Как было сказано выше, единственным оптимизируемым параметром является величина штрафа каждого класса $\lambda_j$. Поиск оптимального значения параметра $\lambda_j$ происходит в результате минимизации функционала $ Q $. Выпишем явный вид $ Q $ как функции от $(\lambda_1, \cdots, \lambda_l)$.
	\begin{equation}
	\begin{split}
		Q(\lambda_1, \cdots, \lambda_l, \tilde{S}) &= \frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\Gamma_j(S_i) - c_i(j))^2} } = \\ 
		&= \frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\ln {\lambda_j \cdot \tilde{P_j} } + \ln {\tilde{p}_{j_x}( S_i( x ) )} + \ln { \tilde{p}_{j_y} ( S_i(y) ))} - c_i(j))^2} } = \\
		&= \frac{1}{q} \sum_{i = 1}^q {\sum_{j = 1}^l {(\ln {\lambda_j} - M_i)^2} } \longrightarrow \underset{\lambda_1, \cdots, \lambda_l}{\min}, 
	\end{split}
	\end{equation}
	
	где $ M_i $ &mdash; некоторые константы, не зависящие от $ \lambda_j $. Заметим, что каждую $ \lambda_j $ можно рассматривать отдельно, таким образом получаем $ l $ задач оптимизации вида:
	\begin{equation}
		Q(\lambda_j, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {(\ln {\lambda_j} - M_i)^2} \longrightarrow \underset{\lambda_j}{\min}
	\end{equation}
	Можно сделать еще одно переобозначение: $ \Lambda_j = \ln {\lambda_j} $, и получить следующую оптимизационную задачу:
	\begin{equation}
		Q(\Lambda_j, \tilde{S}) = \frac{1}{q} \sum_{i = 1}^q {(\Lambda_j - M_i)^2} \longrightarrow \underset{\Lambda_j}{\min}
	\end{equation}
	Полученная оптимизационная задача относится к классу квадратичных задач, ее можно решать методом наименьших квадратов. Тогда решение выписывается следующим образом:
	\begin{equation}
	\begin{split}
		&\lambda_j = \exp{\frac{\sum_{i = 1}^q {M_i}}{q}}, \\
		&M_i = c_i(j) - \ln{\tilde{P}_j} - \ln{\tilde{p}_{j_x}(S_i(x))} - \ln{\tilde{p}_{j_y}(S_i(y))}
	\end{split}
	\end{equation} 
	
<h2>Исследование модели алгоритмических операторов</h2>
<h3>Пример задачи, для которой в модели есть корректный оператор</h3>
Пример задачи, для которой алгоритм корректно работает:

<img border="0" src="nb_good.png" alt="nb works" width="410" height="410">


<h3>Пример задачи, для которой в модели нет корректного оператора</h3>
Пример задачи, для которой алгоритм корректно работает:

<img border="0" src="nb_bad.png" alt="nbb works" width="410" height="410">




<h1>Корректирующая операция. Монотонная линейная КО</h1>
<h2>Вербальное описание</h2>
Семейство монотонных линейных КО &mdash; это семейство линейных преобразований над пространством оценок,
которая для данного набора оценок алгоритмических операторов возвращает значение из пространства оценок. 
Такая КО представляет собой линейную комбинацию оценок алгоритмических операторов с неотрицательными весами. 
<h2>Формальное описание</h2>
Корректирующие операции данного вида выглядят следующим образом:
$$F(B_1, \dots, B_P) = a_1 B_1 + \dots + a_p B_p, $$
где $a_1, \dots, a_p \ge 0, B_1, \dots, B_p $ - оценки соответствующих операторов

<h2>Использование модели</h2>
КО принимает на вход оценки $p$ алгоритмических операторов: $\vec{\Gamma}^1, \dots, \vec{\Gamma}^p$. Параметрами являются $p$ неотрицательных весов: $a_1, \dots, a_p$. 
Результатом корректирующей операции является оценка, подсчитываемая по следующей формуле:

<br>
<br>
<center>
	$\vec{\Gamma} = \alpha_1\vec{\Gamma}^1 + \ldots + \alpha_p\vec{\Gamma}^p$, &nbsp; где $\{\alpha_1, \dots, \alpha_p\ \in \mathbb{R}_+\}.$
</center>	
<br>

Результат работы корректирующего оператора полностью опеределен оценками алгоритмических моделей и своими параметрами и принадлежит пространству оценок.



<h2>Перечисление параметров модели операторов</h2>

В таблице приведены параметры модели:
<br>
<br>

<center>

<table>
<caption></caption>
<tr>
<th>Название параметра</th>
<th>Обозначение параметра</th>
<th>Область допустимых значений</th>
<th>Тип параметра</th>
</tr>
<tr>
<td><center>Арность оператора</center></td>
<td><center>$p$</center></td>
<td><center>$\mathbb{N}$</center></td>
<td>Оптимизируемый</td>
</tr>
<tr>
<td><center>Веса оценок</center></td>
<td><center>$\vec{a}$</center></td>
<td><center>$\mathbb{R}_+^p$</center></td>
<td>Оптимизируемый</td>
</tr>
</table>
</center>
<br>
<h2>Настройка параметров</h2>
Необходимо настроить арность оператора и вектор весов. 

Арность оператора будем увеличивать на единицу, добавляя по одному распознающему оператору, 
пока не будет выполнен критерий останова &mdash;  достижение локального минимума функционала качества, 
то есть последний добавленный оператор не уменьшил функционал.

При фиксированной арности требуется минимизировать следующий функционал:
$$Q(\vec{\alpha}, S) = \frac{1}{ql} \sum_{i = 1}^{q} \|  \alpha_1\vec{\Gamma}^1 + \ldots + \alpha_p\vec{\Gamma}^p - C^{-1}(y_i) \| ^2.$$

Это задача условной непрерывной оптимизации по непрерывным параметрам. 
Для оптимизации вектора весов применим метод покоординатного спуска с убывающим шагом. 
Изначально положим все координаты вектора весов равными $0.1$, шаг равен $1$. 
Далее выбирается координата, увеличение которой на величину шага максимально уменьшит функционал качества. 
Ее значение увеличивается на величину шага. 
Если же такое увеличение любой из координат не уменьшает функционал, то величина шага уменьшается вдвое. 
Итерации продолжаются, пока величина шага больше $10^{-3}$.



<h1>Семейство корректирующих операций. Комитет старшинства с индивидуальными классами.</h1>

<h2>Вербальное описание идеи</h2>
Идея комитета старшинства с индивидуальными классами заключается в том, что алгоритмические операции 
упорядочиваются списком. После этого, по порядку списка мы перебираем результаты алгоритмических операторов, пока не выполниться критерий останова.
Если критерий не выполнился, и мы дошли до конца, то выбирается результат последнего алгоритмического оператора.

<h2>Описание семейства</h2>
Корректирующие операторы из семейства комитета старшинаства основываеются на упорядочивание 
алгоритмических операторов и последовательном выборе результата согласно порядку следования.
Этот способ работы корректирующей операции вводится в литературе под разными названиями: комитет с логикой
старшинства, решающий список правил, машина покрывающих множеств. Последнее название относится скорее к
корректирующией операции комитета старшинства с наборами классов.


<h2>Использование корректирующей операции</h2>

Пусть настраиваются суперпозиция алгоритмических операторов $B_1,\dots,B_p$ с 
векторами оценок $\Gamma_1, \dots, \Gamma_p$.  Корректирующая операция в общем случае выглядит так:

\begin{equation}
	\begin{split}
		 &F(\vec {\Gamma_1},\dots,\vec {\Gamma_p}) = (F_1(\Gamma_{1_1},\dots,\Gamma_{p_1}), \cdots, F_l(\Gamma_{1_1},\dots,\Gamma_{p_1}))^T\\
	\end{split}
	\end{equation}

Тогда каждому аргументу $\Gamma_r, 1 \leq r < p$ 
назначается класс $z_r$. Перебираем аргументы последовательно. Если на очередном шаге $r$ в векторе 
$\Gamma_r$ доминирует компонента $z_r$, то перебор прекращается и возвращается $\Gamma_r$. 
Иначе переходим к следующей оценке. Если достигли последнего аргумента $p$, то 
возвращаем $\Gamma_p$. Эта корректирующая операция относится к классу выбирающих, 
также ее называют решающим списком.

<br>
<h2>Перечисление параметров комитета старшинства с индивидуальными классами</h2>
<br>
<br>

<center>

<table>
<caption></caption>
<tr>
<th>Название параметра</th>
<th>Обозначение параметра</th>
<th>Область допустимых значений</th>
<th>Тип параметра</th>
</tr>
<tr>
<td><center>Классы комитета</center></td>
<td><center>$Z = \{z_1, \dots, z_{p-1}\}$</center></td>
<td><center>$\{1,\dots,l\}^{p-1}$</center></td>
<td>Численно оптимизируемый</td>
</tr>
</table>
</center>
<br>

<h2>Настройка параметров</h2>
Так как размер строящейся суперпозиции небольшой, то оптимизации классов 
для комитета старшинства проводится с помощью полного перебора всех возможных классов, 
соотвествующих каждому алгоритмическому оператору. Выбирается та совокупность классов 

<br>
<br>

<center>
$z_1,\dots, z_{p-1}$ 
</center>
<br>
для которых достигается минимум функционала корректирующей операции на обучающей выборке.
{% endblock report %}

<!-- vim: set ft=htmldjango si sw=2 : -->
